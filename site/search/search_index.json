{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLM API Router","text":"<p> \u7edf\u4e00\u7684\u5927\u8bed\u8a00\u6a21\u578b API \u8def\u7531\u5e93 </p> <p> </p>"},{"location":"#_1","title":"\u6982\u8ff0","text":"<p><code>llm-api-router</code> \u662f\u4e00\u4e2a Python \u5e93\uff0c\u4e3a\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5546\uff08\u5982 OpenAI\u3001Anthropic\u3001Google Gemini \u7b49\uff09\u63d0\u4f9b\u7edf\u4e00\u3001\u4e00\u81f4\u4e14\u7c7b\u578b\u5b89\u5168\u7684\u63a5\u53e3\u3002</p>"},{"location":"#_2","title":"\u6838\u5fc3\u7279\u6027","text":"<ul> <li>\ud83d\udd04 \u7edf\u4e00\u63a5\u53e3 - \u7c7b\u4f3c OpenAI SDK \u7684 <code>client.chat.completions.create</code> \u98ce\u683c</li> <li>\ud83c\udf10 \u591a\u5382\u5546\u652f\u6301 - OpenAI\u3001Anthropic\u3001Gemini\u3001DeepSeek\u3001\u667a\u8c31\u3001\u963f\u91cc\u4e91\u7b49</li> <li>\u26a1 \u96f6\u4ee3\u7801\u5207\u6362 - \u4ec5\u9700\u4fee\u6539\u914d\u7f6e\u5373\u53ef\u5207\u6362\u5e95\u5c42\u6a21\u578b\u63d0\u4f9b\u5546</li> <li>\ud83c\udf0a \u6d41\u5f0f\u652f\u6301 - \u7edf\u4e00\u7684 SSE \u6d41\u5f0f\u54cd\u5e94\u5904\u7406</li> <li>\ud83d\udd27 \u5f02\u6b65\u652f\u6301 - \u539f\u751f\u652f\u6301 <code>asyncio</code> \u548c <code>await</code></li> <li>\ud83d\udcca \u53ef\u89c2\u6d4b\u6027 - \u5185\u7f6e\u65e5\u5fd7\u3001\u6307\u6807\u6536\u96c6\u3001\u7f13\u5b58\u7b49\u529f\u80fd</li> </ul>"},{"location":"#_3","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_4","title":"\u5b89\u88c5","text":"<pre><code>pip install llm-api-router\n</code></pre>"},{"location":"#_5","title":"\u57fa\u7840\u7528\u6cd5","text":"<pre><code>from llm_api_router import Client, ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-...\",\n    default_model=\"gpt-3.5-turbo\"\n)\n\nwith Client(config) as client:\n    response = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n    print(response.choices[0].message.content)\n</code></pre>"},{"location":"#_6","title":"\u5207\u6362\u63d0\u4f9b\u5546","text":"<p>\u53ea\u9700\u66f4\u6539\u914d\u7f6e\uff0c\u4ee3\u7801\u65e0\u9700\u4fee\u6539\uff1a</p> <pre><code># \u4f7f\u7528 Anthropic\nconfig = ProviderConfig(\n    provider_type=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    default_model=\"claude-3-haiku-20240307\"\n)\n\n# \u4f7f\u7528\u672c\u5730 Ollama\nconfig = ProviderConfig(\n    provider_type=\"ollama\",\n    api_key=\"not-required\",\n    base_url=\"http://localhost:11434\",\n    default_model=\"llama3.2\"\n)\n</code></pre>"},{"location":"#_7","title":"\u652f\u6301\u7684\u63d0\u4f9b\u5546","text":"\u63d0\u4f9b\u5546 Chat Embeddings Function Calling OpenAI \u2705 \u2705 \u2705 Anthropic \u2705 - \u2705 Google Gemini \u2705 \u2705 - DeepSeek \u2705 - - \u667a\u8c31 AI \u2705 \u2705 - \u963f\u91cc\u4e91 \u2705 \u2705 - Ollama \u2705 - - OpenRouter \u2705 - - xAI \u2705 - -"},{"location":"#_8","title":"\u9ad8\u7ea7\u7279\u6027","text":"<ul> <li>\u54cd\u5e94\u7f13\u5b58 - \u51cf\u5c11\u91cd\u590d API \u8c03\u7528</li> <li>\u914d\u7f6e\u7ba1\u7406 - \u8be6\u7ec6\u914d\u7f6e\u53c2\u8003</li> <li>\u63d0\u4f9b\u5546\u652f\u6301 - \u5404\u63d0\u4f9b\u5546\u8be6\u60c5</li> </ul>"},{"location":"#_9","title":"\u4e0b\u4e00\u6b65","text":"<ul> <li>\ud83d\udcd6 \u67e5\u770b \u5b89\u88c5\u6307\u5357 \u4e86\u89e3\u8be6\u7ec6\u5b89\u88c5\u6b65\u9aa4</li> <li>\ud83d\ude80 \u9605\u8bfb \u5feb\u901f\u5f00\u59cb \u5f00\u59cb\u4f7f\u7528</li> <li>\ud83d\udcda \u6d4f\u89c8 API \u53c2\u8003 \u4e86\u89e3\u5b8c\u6574 API</li> </ul>"},{"location":"cli/","title":"CLI \u547d\u4ee4\u53c2\u8003","text":"<p>LLM API Router \u63d0\u4f9b\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u65b9\u4fbf\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u65e5\u5e38\u4f7f\u7528\u3002</p>"},{"location":"cli/#_1","title":"\u5b89\u88c5","text":"<pre><code>pip install llm-api-router[cli]\n</code></pre>"},{"location":"cli/#_2","title":"\u547d\u4ee4\u6982\u89c8","text":"<pre><code>llm-router --help\n</code></pre> \u547d\u4ee4 \u8bf4\u660e <code>test</code> \u6d4b\u8bd5\u63d0\u4f9b\u5546\u8fde\u63a5 <code>validate</code> \u9a8c\u8bc1\u914d\u7f6e\u6587\u4ef6 <code>benchmark</code> \u8fd0\u884c\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 <code>models</code> \u5217\u51fa\u53ef\u7528\u6a21\u578b <code>chat</code> \u4ea4\u4e92\u5f0f\u804a\u5929"},{"location":"cli/#test-","title":"test - \u6d4b\u8bd5\u8fde\u63a5","text":"<p>\u6d4b\u8bd5\u4e0e LLM \u63d0\u4f9b\u5546\u7684\u8fde\u63a5\uff1a</p> <pre><code># \u57fa\u672c\u4f7f\u7528\nllm-router test openai --api-key sk-xxx\n\n# \u6307\u5b9a\u6a21\u578b\nllm-router test openai --api-key sk-xxx --model gpt-4o\n\n# \u6d4b\u8bd5 Ollama\uff08\u672c\u5730\uff09\nllm-router test ollama --base-url http://localhost:11434\n\n# JSON \u8f93\u51fa\nllm-router test openai --api-key sk-xxx --output json\n</code></pre>"},{"location":"cli/#_3","title":"\u53c2\u6570","text":"\u53c2\u6570 \u8bf4\u660e <code>PROVIDER</code> \u63d0\u4f9b\u5546\u540d\u79f0\uff08\u5fc5\u9700\uff09 <code>--api-key, -k</code> API \u5bc6\u94a5 <code>--base-url, -u</code> \u81ea\u5b9a\u4e49 API \u5730\u5740 <code>--model, -m</code> \u6d4b\u8bd5\u6a21\u578b <code>--output, -o</code> \u8f93\u51fa\u683c\u5f0f\uff1atable/json"},{"location":"cli/#validate-","title":"validate - \u9a8c\u8bc1\u914d\u7f6e","text":"<p>\u9a8c\u8bc1\u914d\u7f6e\u6587\u4ef6\u683c\u5f0f\uff1a</p> <pre><code># \u9a8c\u8bc1 JSON \u914d\u7f6e\nllm-router validate config.json\n\n# \u9a8c\u8bc1 YAML \u914d\u7f6e\nllm-router validate config.yaml\n\n# \u8be6\u7ec6\u8f93\u51fa\nllm-router validate config.json --verbose\n</code></pre>"},{"location":"cli/#_4","title":"\u53c2\u6570","text":"\u53c2\u6570 \u8bf4\u660e <code>CONFIG_PATH</code> \u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08\u5fc5\u9700\uff09 <code>--verbose, -v</code> \u663e\u793a\u8be6\u7ec6\u4fe1\u606f"},{"location":"cli/#_5","title":"\u914d\u7f6e\u6587\u4ef6\u683c\u5f0f","text":"<pre><code>{\n  \"provider_type\": \"openai\",\n  \"api_key\": \"sk-xxx\",\n  \"default_model\": \"gpt-3.5-turbo\",\n  \"timeout\": 30,\n  \"max_retries\": 3\n}\n</code></pre>"},{"location":"cli/#benchmark-","title":"benchmark - \u6027\u80fd\u6d4b\u8bd5","text":"<p>\u8fd0\u884c\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff1a</p> <pre><code># \u57fa\u672c\u57fa\u51c6\u6d4b\u8bd5\nllm-router benchmark openai --api-key sk-xxx\n\n# \u6307\u5b9a\u8bf7\u6c42\u6570\u91cf\nllm-router benchmark openai --api-key sk-xxx --requests 20\n\n# \u5e76\u53d1\u6d4b\u8bd5\nllm-router benchmark openai --api-key sk-xxx --concurrency 5\n\n# \u6307\u5b9a\u63d0\u793a\u8bcd\nllm-router benchmark openai --api-key sk-xxx --prompt \"Hello, how are you?\"\n</code></pre>"},{"location":"cli/#_6","title":"\u53c2\u6570","text":"\u53c2\u6570 \u8bf4\u660e \u9ed8\u8ba4\u503c <code>PROVIDER</code> \u63d0\u4f9b\u5546\u540d\u79f0\uff08\u5fc5\u9700\uff09 - <code>--api-key, -k</code> API \u5bc6\u94a5 - <code>--model, -m</code> \u6d4b\u8bd5\u6a21\u578b \u63d0\u4f9b\u5546\u9ed8\u8ba4 <code>--requests, -n</code> \u8bf7\u6c42\u6570\u91cf 10 <code>--concurrency, -c</code> \u5e76\u53d1\u6570 1 <code>--prompt, -p</code> \u6d4b\u8bd5\u63d0\u793a\u8bcd \"Say hello\" <code>--output, -o</code> \u8f93\u51fa\u683c\u5f0f table"},{"location":"cli/#_7","title":"\u8f93\u51fa\u793a\u4f8b","text":"<pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 \u6307\u6807            \u2503 \u503c         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 \u603b\u8bf7\u6c42\u6570        \u2502 10         \u2502\n\u2502 \u6210\u529f\u8bf7\u6c42        \u2502 10         \u2502\n\u2502 \u5931\u8d25\u8bf7\u6c42        \u2502 0          \u2502\n\u2502 \u5e73\u5747\u5ef6\u8fdf        \u2502 523.4ms    \u2502\n\u2502 P50 \u5ef6\u8fdf        \u2502 498.2ms    \u2502\n\u2502 P95 \u5ef6\u8fdf        \u2502 712.8ms    \u2502\n\u2502 P99 \u5ef6\u8fdf        \u2502 782.1ms    \u2502\n\u2502 \u541e\u5410\u91cf          \u2502 1.9 req/s  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/#models-","title":"models - \u5217\u51fa\u6a21\u578b","text":"<p>\u5217\u51fa\u63d0\u4f9b\u5546\u652f\u6301\u7684\u6a21\u578b\uff1a</p> <pre><code># \u5217\u51fa OpenAI \u6a21\u578b\nllm-router models openai\n\n# \u5217\u51fa Anthropic \u6a21\u578b\nllm-router models anthropic\n\n# JSON \u683c\u5f0f\u8f93\u51fa\nllm-router models openai --output json\n</code></pre>"},{"location":"cli/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u8bf4\u660e <code>PROVIDER</code> \u63d0\u4f9b\u5546\u540d\u79f0\uff08\u5fc5\u9700\uff09 <code>--output, -o</code> \u8f93\u51fa\u683c\u5f0f\uff1atable/json"},{"location":"cli/#chat-","title":"chat - \u4ea4\u4e92\u5f0f\u804a\u5929","text":"<p>\u542f\u52a8\u4ea4\u4e92\u5f0f\u804a\u5929\u4f1a\u8bdd\uff1a</p> <pre><code># \u57fa\u672c\u804a\u5929\nllm-router chat openai --api-key sk-xxx\n\n# \u6307\u5b9a\u6a21\u578b\nllm-router chat openai --api-key sk-xxx --model gpt-4o\n\n# \u8bbe\u7f6e\u7cfb\u7edf\u63d0\u793a\nllm-router chat openai --api-key sk-xxx --system \"\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u7a0b\u5e8f\u5458\"\n\n# \u4e0e\u672c\u5730 Ollama \u804a\u5929\nllm-router chat ollama --base-url http://localhost:11434 --model llama3.2\n</code></pre>"},{"location":"cli/#_9","title":"\u53c2\u6570","text":"\u53c2\u6570 \u8bf4\u660e <code>PROVIDER</code> \u63d0\u4f9b\u5546\u540d\u79f0\uff08\u5fc5\u9700\uff09 <code>--api-key, -k</code> API \u5bc6\u94a5 <code>--base-url, -u</code> \u81ea\u5b9a\u4e49 API \u5730\u5740 <code>--model, -m</code> \u4f7f\u7528\u7684\u6a21\u578b <code>--system, -s</code> \u7cfb\u7edf\u63d0\u793a\u8bcd <code>--no-stream</code> \u7981\u7528\u6d41\u5f0f\u8f93\u51fa"},{"location":"cli/#_10","title":"\u804a\u5929\u547d\u4ee4","text":"<p>\u5728\u804a\u5929\u4e2d\u53ef\u4f7f\u7528\u7684\u547d\u4ee4\uff1a</p> \u547d\u4ee4 \u8bf4\u660e <code>/help</code> \u663e\u793a\u5e2e\u52a9 <code>/clear</code> \u6e05\u7a7a\u5bf9\u8bdd\u5386\u53f2 <code>/history</code> \u663e\u793a\u5bf9\u8bdd\u5386\u53f2 <code>/exit</code> \u9000\u51fa\u804a\u5929"},{"location":"cli/#_11","title":"\u5168\u5c40\u9009\u9879","text":"<pre><code># \u663e\u793a\u7248\u672c\nllm-router --version\nllm-router -v\n\n# \u663e\u793a\u5e2e\u52a9\nllm-router --help\n</code></pre>"},{"location":"cli/#_12","title":"\u73af\u5883\u53d8\u91cf","text":"<p>CLI \u652f\u6301\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u914d\u7f6e API \u5bc6\u94a5\uff1a</p> <pre><code>export OPENAI_API_KEY=\"sk-xxx\"\nexport ANTHROPIC_API_KEY=\"sk-ant-xxx\"\n\n# \u7136\u540e\u53ef\u4ee5\u7701\u7565 --api-key \u53c2\u6570\nllm-router test openai\nllm-router chat anthropic\n</code></pre>"},{"location":"cli/#_13","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"cli/#_14","title":"\u5feb\u901f\u6d4b\u8bd5\u6240\u6709\u63d0\u4f9b\u5546","text":"<pre><code>#!/bin/bash\nproviders=(\"openai\" \"anthropic\" \"gemini\")\n\nfor provider in \"${providers[@]}\"; do\n    echo \"Testing $provider...\"\n    llm-router test \"$provider\" --output json\ndone\n</code></pre>"},{"location":"cli/#_15","title":"\u6279\u91cf\u57fa\u51c6\u6d4b\u8bd5","text":"<pre><code>#!/bin/bash\n# \u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\nfor model in \"gpt-3.5-turbo\" \"gpt-4o\" \"gpt-4o-mini\"; do\n    echo \"Benchmarking $model...\"\n    llm-router benchmark openai --model \"$model\" --requests 5\ndone\n</code></pre>"},{"location":"connection_pool_optimization/","title":"HTTP Connection Pool Optimization","text":"<p>This document describes the HTTP connection pool optimization features in llm-api-router, including configuration options and best practices.</p>"},{"location":"connection_pool_optimization/#overview","title":"Overview","text":"<p>The llm-api-router uses httpx as its HTTP client library, which provides excellent support for connection pooling and reuse. By properly configuring the connection pool, you can significantly improve the performance and resource efficiency of your LLM API interactions.</p>"},{"location":"connection_pool_optimization/#benefits-of-connection-pool-optimization","title":"Benefits of Connection Pool Optimization","text":"<ol> <li>Reduced Latency: Reusing existing connections eliminates TCP handshake overhead</li> <li>Better Throughput: More efficient handling of concurrent requests</li> <li>Resource Efficiency: Controlled resource usage through connection limits</li> <li>Improved Reliability: Proper timeout configuration prevents hanging requests</li> </ol>"},{"location":"connection_pool_optimization/#configuration-classes","title":"Configuration Classes","text":""},{"location":"connection_pool_optimization/#connectionpoolconfig","title":"ConnectionPoolConfig","text":"<p>Controls the HTTP connection pool behavior:</p> <pre><code>from llm_api_router import ConnectionPoolConfig\n\npool_config = ConnectionPoolConfig(\n    max_connections=100,           # Maximum total connections\n    max_keepalive_connections=20,  # Maximum idle connections to keep alive\n    keepalive_expiry=300.0,        # Seconds to keep idle connections alive\n    stream_buffer_size=65536       # Buffer size for streaming responses (bytes)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>max_connections</code> (int, default=100): Maximum number of connections that can exist in the pool at once. This limits concurrent requests.</li> <li><code>max_keepalive_connections</code> (int, default=20): Maximum number of idle connections to keep alive for reuse. Connections beyond this limit are closed immediately after use.</li> <li><code>keepalive_expiry</code> (float, default=300.0): Time in seconds that an idle connection is kept alive before being closed.</li> <li><code>stream_buffer_size</code> (int, default=65536): Size of the buffer used for streaming responses in bytes (64KB default).</li> </ul>"},{"location":"connection_pool_optimization/#timeoutconfig","title":"TimeoutConfig","text":"<p>Provides fine-grained control over various timeout aspects:</p> <pre><code>from llm_api_router import TimeoutConfig\n\ntimeout_config = TimeoutConfig(\n    connect=10.0,  # Connection establishment timeout\n    read=60.0,     # Read timeout for response data\n    write=10.0,    # Write timeout for request data\n    pool=10.0      # Timeout for acquiring a connection from the pool\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>connect</code> (float, default=10.0): Maximum time in seconds to wait for a connection to be established.</li> <li><code>read</code> (float, default=60.0): Maximum time in seconds to wait between consecutive read operations.</li> <li><code>write</code> (float, default=10.0): Maximum time in seconds to wait for write operations.</li> <li><code>pool</code> (float, default=10.0): Maximum time in seconds to wait for acquiring a connection from the pool.</li> </ul>"},{"location":"connection_pool_optimization/#usage","title":"Usage","text":""},{"location":"connection_pool_optimization/#basic-configuration","title":"Basic Configuration","text":"<p>Using default settings (recommended for most use cases):</p> <pre><code>from llm_api_router import Client, ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    default_model=\"gpt-3.5-turbo\"\n)\n\nclient = Client(config)\n</code></pre>"},{"location":"connection_pool_optimization/#custom-connection-pool-configuration","title":"Custom Connection Pool Configuration","text":"<pre><code>from llm_api_router import Client, ProviderConfig, ConnectionPoolConfig\n\npool_config = ConnectionPoolConfig(\n    max_connections=50,\n    max_keepalive_connections=10,\n    keepalive_expiry=120.0\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    connection_pool_config=pool_config\n)\n\nclient = Client(config)\n</code></pre>"},{"location":"connection_pool_optimization/#custom-timeout-configuration","title":"Custom Timeout Configuration","text":"<pre><code>from llm_api_router import Client, ProviderConfig, TimeoutConfig\n\ntimeout_config = TimeoutConfig(\n    connect=5.0,\n    read=30.0,\n    write=5.0,\n    pool=5.0\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    timeout_config=timeout_config\n)\n\nclient = Client(config)\n</code></pre>"},{"location":"connection_pool_optimization/#combined-configuration","title":"Combined Configuration","text":"<pre><code>from llm_api_router import (\n    Client, ProviderConfig,\n    ConnectionPoolConfig, TimeoutConfig\n)\n\ntimeout_config = TimeoutConfig(\n    connect=5.0,\n    read=60.0,\n    write=5.0,\n    pool=5.0\n)\n\npool_config = ConnectionPoolConfig(\n    max_connections=100,\n    max_keepalive_connections=30,\n    keepalive_expiry=600.0\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    timeout_config=timeout_config,\n    connection_pool_config=pool_config\n)\n\nclient = Client(config)\n</code></pre>"},{"location":"connection_pool_optimization/#configuration-scenarios","title":"Configuration Scenarios","text":""},{"location":"connection_pool_optimization/#high-concurrency-workloads","title":"High Concurrency Workloads","text":"<p>When handling many concurrent requests:</p> <pre><code>pool_config = ConnectionPoolConfig(\n    max_connections=200,        # Allow more concurrent connections\n    max_keepalive_connections=50,  # Keep more connections alive\n    keepalive_expiry=600.0,     # Keep connections alive longer\n    stream_buffer_size=131072   # Larger buffer for streaming (128KB)\n)\n</code></pre>"},{"location":"connection_pool_optimization/#low-latency-requirements","title":"Low Latency Requirements","text":"<p>When minimizing latency is critical:</p> <pre><code>timeout_config = TimeoutConfig(\n    connect=5.0,   # Quick connection timeout\n    read=30.0,     # Moderate read timeout\n    write=5.0,     # Quick write timeout\n    pool=5.0       # Fast pool acquisition\n)\n\npool_config = ConnectionPoolConfig(\n    max_connections=50,\n    max_keepalive_connections=20,\n    keepalive_expiry=120.0  # Shorter expiry to avoid stale connections\n)\n</code></pre>"},{"location":"connection_pool_optimization/#resource-constrained-environments","title":"Resource-Constrained Environments","text":"<p>When running with limited resources:</p> <pre><code>pool_config = ConnectionPoolConfig(\n    max_connections=20,         # Lower connection limit\n    max_keepalive_connections=5,   # Fewer persistent connections\n    keepalive_expiry=60.0,      # Shorter expiry\n    stream_buffer_size=32768    # Smaller buffer (32KB)\n)\n</code></pre>"},{"location":"connection_pool_optimization/#long-running-streaming-applications","title":"Long-Running Streaming Applications","text":"<p>When dealing with long streaming responses:</p> <pre><code>timeout_config = TimeoutConfig(\n    connect=10.0,\n    read=300.0,    # Longer read timeout for streaming\n    write=10.0,\n    pool=10.0\n)\n\npool_config = ConnectionPoolConfig(\n    max_connections=50,\n    max_keepalive_connections=20,\n    keepalive_expiry=600.0,\n    stream_buffer_size=131072  # Larger buffer for streaming\n)\n</code></pre>"},{"location":"connection_pool_optimization/#best-practices","title":"Best Practices","text":""},{"location":"connection_pool_optimization/#1-connection-pool-sizing","title":"1. Connection Pool Sizing","text":"<ul> <li>max_connections: Set based on your expected peak concurrent requests. A good starting point is 100.</li> <li>max_keepalive_connections: Keep this at 10-30% of max_connections for most workloads.</li> <li>Rule of thumb: <code>max_keepalive_connections</code> \u2264 <code>max_connections</code></li> </ul>"},{"location":"connection_pool_optimization/#2-keepalive-expiry","title":"2. Keepalive Expiry","text":"<ul> <li>Default 300s (5 minutes) works well for most cases</li> <li>Increase to 600s-3600s for applications with consistent traffic patterns</li> <li>Decrease to 60s-120s for sporadic usage to free up resources</li> </ul>"},{"location":"connection_pool_optimization/#3-timeout-configuration","title":"3. Timeout Configuration","text":"<ul> <li>connect: Should be short (5-10s) to fail fast on network issues</li> <li>read: Should account for typical API response times plus buffer (30-120s)</li> <li>write: Usually short (5-10s) as request bodies are typically small</li> <li>pool: Should be short (5-10s) to detect pool exhaustion quickly</li> </ul>"},{"location":"connection_pool_optimization/#4-buffer-sizing","title":"4. Buffer Sizing","text":"<ul> <li>Default 64KB is suitable for most use cases</li> <li>Increase to 128KB-256KB for high-throughput streaming applications</li> <li>Decrease to 32KB for memory-constrained environments</li> </ul>"},{"location":"connection_pool_optimization/#5-monitoring-and-tuning","title":"5. Monitoring and Tuning","text":"<p>Monitor these metrics to tune your configuration: - Request latency (P50, P95, P99) - Connection pool exhaustion events - Timeout errors - Memory usage</p>"},{"location":"connection_pool_optimization/#backward-compatibility","title":"Backward Compatibility","text":"<p>The simple <code>timeout</code> parameter is still supported for backward compatibility:</p> <pre><code>config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    timeout=30.0  # Simple timeout (seconds)\n)\n</code></pre> <p>When both <code>timeout</code> and <code>timeout_config</code> are provided, <code>timeout_config</code> takes precedence.</p>"},{"location":"connection_pool_optimization/#async-client","title":"Async Client","text":"<p>All configuration options work identically with AsyncClient:</p> <pre><code>from llm_api_router import AsyncClient, ProviderConfig, ConnectionPoolConfig\n\npool_config = ConnectionPoolConfig(\n    max_connections=100,\n    max_keepalive_connections=30\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    connection_pool_config=pool_config\n)\n\nasync with AsyncClient(config) as client:\n    # Use async client\n    response = await client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n</code></pre>"},{"location":"connection_pool_optimization/#performance-impact","title":"Performance Impact","text":"<p>Proper connection pool configuration can provide significant performance improvements:</p> <ul> <li>Connection Reuse: Up to 50% reduction in request latency by eliminating TCP handshake</li> <li>Concurrent Throughput: 2-5x improvement in handling concurrent requests with proper sizing</li> <li>Resource Efficiency: 30-50% reduction in memory usage with appropriate keepalive settings</li> </ul>"},{"location":"connection_pool_optimization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"connection_pool_optimization/#pool-exhaustion","title":"Pool Exhaustion","text":"<p>If you see timeout errors when acquiring connections: - Increase <code>max_connections</code> - Check if requests are being properly closed/released - Verify that <code>keepalive_expiry</code> isn't too short</p>"},{"location":"connection_pool_optimization/#memory-issues","title":"Memory Issues","text":"<p>If memory usage is too high: - Decrease <code>max_keepalive_connections</code> - Reduce <code>keepalive_expiry</code> - Lower <code>stream_buffer_size</code></p>"},{"location":"connection_pool_optimization/#connection-timeouts","title":"Connection Timeouts","text":"<p>If you experience frequent connection timeouts: - Increase <code>timeout_config.connect</code> - Check network stability - Verify API endpoint availability</p>"},{"location":"connection_pool_optimization/#read-timeouts","title":"Read Timeouts","text":"<p>If you experience read timeouts during responses: - Increase <code>timeout_config.read</code> - For streaming, ensure buffer size is adequate - Check API response times</p>"},{"location":"connection_pool_optimization/#examples","title":"Examples","text":"<p>See <code>examples/connection_pool_optimization.py</code> for complete working examples demonstrating various configuration scenarios.</p>"},{"location":"connection_pool_optimization/#references","title":"References","text":"<ul> <li>httpx Documentation</li> <li>Connection Pooling Best Practices</li> <li>Timeout Configuration Guide</li> </ul>"},{"location":"contributing/","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u611f\u8c22\u4f60\u6709\u5174\u8da3\u4e3a LLM API Router \u505a\u51fa\u8d21\u732e\uff01\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u4e86\u89e3\u5982\u4f55\u53c2\u4e0e\u9879\u76ee\u5f00\u53d1\u3002</p>"},{"location":"contributing/#_2","title":"\u884c\u4e3a\u51c6\u5219","text":"<ul> <li>\u5c0a\u91cd\u6240\u6709\u53c2\u4e0e\u8005</li> <li>\u5efa\u8bbe\u6027\u5730\u8ba8\u8bba\u95ee\u9898</li> <li>\u4e13\u6ce8\u4e8e\u6539\u8fdb\u9879\u76ee</li> </ul>"},{"location":"contributing/#_3","title":"\u5982\u4f55\u8d21\u732e","text":""},{"location":"contributing/#bug","title":"\u62a5\u544a Bug","text":"<ol> <li>\u68c0\u67e5 Issue \u5217\u8868 \u786e\u8ba4\u95ee\u9898\u672a\u88ab\u62a5\u544a</li> <li>\u521b\u5efa\u65b0 Issue\uff0c\u5305\u542b\uff1a</li> <li>\u6e05\u6670\u7684\u6807\u9898\u548c\u63cf\u8ff0</li> <li>\u590d\u73b0\u6b65\u9aa4</li> <li>\u9884\u671f\u884c\u4e3a vs \u5b9e\u9645\u884c\u4e3a</li> <li>\u73af\u5883\u4fe1\u606f\uff08Python \u7248\u672c\u3001\u64cd\u4f5c\u7cfb\u7edf\u7b49\uff09</li> <li>\u76f8\u5173\u4ee3\u7801\u548c\u9519\u8bef\u4fe1\u606f</li> </ol>"},{"location":"contributing/#_4","title":"\u63d0\u51fa\u529f\u80fd\u5efa\u8bae","text":"<ol> <li>\u641c\u7d22\u73b0\u6709 Issue \u786e\u8ba4\u5efa\u8bae\u672a\u88ab\u63d0\u51fa</li> <li>\u521b\u5efa Feature Request Issue\uff0c\u8bf4\u660e\uff1a</li> <li>\u529f\u80fd\u63cf\u8ff0</li> <li>\u4f7f\u7528\u573a\u666f</li> <li>\u53ef\u80fd\u7684\u5b9e\u73b0\u65b9\u5f0f</li> </ol>"},{"location":"contributing/#_5","title":"\u63d0\u4ea4\u4ee3\u7801","text":""},{"location":"contributing/#1-fork","title":"1. Fork \u4ed3\u5e93","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/llm-api-router.git\ncd llm-api-router\n</code></pre>"},{"location":"contributing/#2","title":"2. \u521b\u5efa\u5f00\u53d1\u73af\u5883","text":"<pre><code># \u4f7f\u7528 uv\uff08\u63a8\u8350\uff09\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n\n# \u6216\u4f7f\u7528 pip\npython -m venv venv\nsource venv/bin/activate\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3","title":"3. \u521b\u5efa\u7279\u6027\u5206\u652f","text":"<pre><code>git checkout -b feature/your-feature-name\n# \u6216\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributing/#4","title":"4. \u7f16\u5199\u4ee3\u7801","text":"<p>\u9075\u5faa\u9879\u76ee\u4ee3\u7801\u98ce\u683c\uff1a</p> <ul> <li>\u4f7f\u7528 Python 3.10+ \u7279\u6027</li> <li>\u6dfb\u52a0\u7c7b\u578b\u6ce8\u89e3</li> <li>\u7f16\u5199 docstring</li> <li>\u9075\u5faa PEP 8</li> </ul>"},{"location":"contributing/#5","title":"5. \u7f16\u5199\u6d4b\u8bd5","text":"<pre><code># \u8fd0\u884c\u6d4b\u8bd5\npytest tests/\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\npytest tests/unit/test_your_feature.py -v\n\n# \u8fd0\u884c\u6d4b\u8bd5\u5e76\u67e5\u770b\u8986\u76d6\u7387\npytest tests/ --cov=src/llm_api_router --cov-report=html\n</code></pre>"},{"location":"contributing/#6","title":"6. \u683c\u5f0f\u5316\u4ee3\u7801","text":"<pre><code># \u683c\u5f0f\u5316\u4ee3\u7801\nblack src/ tests/\n\n# \u68c0\u67e5\u5bfc\u5165\u6392\u5e8f\nisort src/ tests/\n\n# \u7c7b\u578b\u68c0\u67e5\nmypy src/\n</code></pre>"},{"location":"contributing/#7","title":"7. \u63d0\u4ea4\u66f4\u6539","text":"<pre><code>git add .\ngit commit -m \"feat: add your feature description\"\n</code></pre> <p>\u63d0\u4ea4\u4fe1\u606f\u683c\u5f0f\uff1a</p> <ul> <li><code>feat:</code> \u65b0\u529f\u80fd</li> <li><code>fix:</code> \u4fee\u590d bug</li> <li><code>docs:</code> \u6587\u6863\u66f4\u65b0</li> <li><code>test:</code> \u6d4b\u8bd5\u76f8\u5173</li> <li><code>refactor:</code> \u4ee3\u7801\u91cd\u6784</li> <li><code>style:</code> \u4ee3\u7801\u98ce\u683c</li> <li><code>chore:</code> \u5176\u4ed6\u66f4\u6539</li> </ul>"},{"location":"contributing/#8-pr","title":"8. \u63a8\u9001\u5e76\u521b\u5efa PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>\u5728 GitHub \u4e0a\u521b\u5efa Pull Request\u3002</p>"},{"location":"contributing/#_6","title":"\u4ee3\u7801\u89c4\u8303","text":""},{"location":"contributing/#_7","title":"\u7c7b\u578b\u6ce8\u89e3","text":"<pre><code>def create_completion(\n    self,\n    messages: list[dict[str, str]],\n    model: str | None = None,\n    temperature: float = 1.0,\n) -&gt; ChatCompletion:\n    \"\"\"\u521b\u5efa\u804a\u5929\u5b8c\u6210\u3002\n\n    Args:\n        messages: \u6d88\u606f\u5217\u8868\n        model: \u6a21\u578b\u540d\u79f0\n        temperature: \u6e29\u5ea6\u53c2\u6570\n\n    Returns:\n        \u804a\u5929\u5b8c\u6210\u54cd\u5e94\n    \"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#_8","title":"\u6587\u6863\u5b57\u7b26\u4e32","text":"<p>\u4f7f\u7528 Google \u98ce\u683c\u7684 docstring\uff1a</p> <pre><code>def function(param1: str, param2: int) -&gt; bool:\n    \"\"\"\u7b80\u77ed\u63cf\u8ff0\u3002\n\n    \u8be6\u7ec6\u63cf\u8ff0\uff08\u5982\u9700\u8981\uff09\u3002\n\n    Args:\n        param1: \u53c2\u65701\u63cf\u8ff0\n        param2: \u53c2\u65702\u63cf\u8ff0\n\n    Returns:\n        \u8fd4\u56de\u503c\u63cf\u8ff0\n\n    Raises:\n        ValueError: \u9519\u8bef\u63cf\u8ff0\n    \"\"\"\n</code></pre>"},{"location":"contributing/#_9","title":"\u6d4b\u8bd5\u89c4\u8303","text":"<pre><code>import pytest\nfrom llm_api_router import Client, ProviderConfig\n\n\nclass TestYourFeature:\n    \"\"\"\u6d4b\u8bd5\u4f60\u7684\u529f\u80fd\"\"\"\n\n    def test_basic_usage(self):\n        \"\"\"\u6d4b\u8bd5\u57fa\u672c\u7528\u6cd5\"\"\"\n        # Arrange\n        config = ProviderConfig(...)\n\n        # Act\n        result = some_function()\n\n        # Assert\n        assert result == expected\n\n    @pytest.mark.asyncio\n    async def test_async_usage(self):\n        \"\"\"\u6d4b\u8bd5\u5f02\u6b65\u7528\u6cd5\"\"\"\n        ...\n</code></pre>"},{"location":"contributing/#_10","title":"\u9879\u76ee\u7ed3\u6784","text":"<pre><code>llm-api-router/\n\u251c\u2500\u2500 src/llm_api_router/     # \u6e90\u4ee3\u7801\n\u2502   \u251c\u2500\u2500 __init__.py         # \u5305\u5165\u53e3\n\u2502   \u251c\u2500\u2500 client.py           # \u5ba2\u6237\u7aef\u5b9e\u73b0\n\u2502   \u251c\u2500\u2500 providers/          # \u63d0\u4f9b\u5546\u5b9e\u73b0\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tests/                  # \u6d4b\u8bd5\u4ee3\u7801\n\u2502   \u251c\u2500\u2500 unit/               # \u5355\u5143\u6d4b\u8bd5\n\u2502   \u2514\u2500\u2500 integration/        # \u96c6\u6210\u6d4b\u8bd5\n\u251c\u2500\u2500 docs/                   # \u6587\u6863\n\u251c\u2500\u2500 examples/               # \u793a\u4f8b\u4ee3\u7801\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contributing/#_11","title":"\u53d1\u5e03\u6d41\u7a0b","text":"<p>\u7ef4\u62a4\u8005\u4f1a\u5904\u7406\u53d1\u5e03\uff1a</p> <ol> <li>\u66f4\u65b0\u7248\u672c\u53f7</li> <li>\u66f4\u65b0 CHANGELOG</li> <li>\u521b\u5efa Release Tag</li> <li>\u53d1\u5e03\u5230 PyPI</li> </ol>"},{"location":"contributing/#_12","title":"\u83b7\u53d6\u5e2e\u52a9","text":"<ul> <li>\u67e5\u770b \u6587\u6863</li> <li>\u5728 GitHub Discussion \u8ba8\u8bba</li> <li>\u63d0\u4ea4 Issue \u5bfb\u6c42\u5e2e\u52a9</li> </ul> <p>\u518d\u6b21\u611f\u8c22\u4f60\u7684\u8d21\u732e\uff01\ud83c\udf89</p>"},{"location":"conversation-management/","title":"Conversation Management","text":"<p>LLM API Router \u63d0\u4f9b\u667a\u80fd\u7684\u5bf9\u8bdd\u5386\u53f2\u7ba1\u7406\u529f\u80fd\uff0c\u5305\u62ec\u81ea\u52a8 token \u8ba1\u6570\u548c\u591a\u79cd\u622a\u65ad\u7b56\u7565\u3002</p>"},{"location":"conversation-management/#_1","title":"\u5feb\u901f\u5f00\u59cb","text":"<pre><code>from llm_api_router.conversation import create_conversation\n\n# \u521b\u5efa\u5bf9\u8bdd\u7ba1\u7406\u5668\nconversation = create_conversation(\n    max_tokens=4096,\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002\"\n)\n\n# \u6dfb\u52a0\u6d88\u606f\nconversation.add_user_message(\"\u4f60\u597d\uff01\")\nconversation.add_assistant_message(\"\u4f60\u597d\uff01\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7684\u5417\uff1f\")\n\n# \u83b7\u53d6 API \u683c\u5f0f\u7684\u6d88\u606f\nmessages = conversation.get_messages_for_api()\n</code></pre>"},{"location":"conversation-management/#_2","title":"\u914d\u7f6e","text":"<pre><code>from llm_api_router.conversation import ConversationConfig, ConversationManager\n\nconfig = ConversationConfig(\n    max_tokens=4096,          # \u6700\u5927 token \u6570\n    max_messages=100,         # \u6700\u5927\u6d88\u606f\u6570\n    preserve_system=True,     # \u4fdd\u7559\u7cfb\u7edf\u6d88\u606f\n    model=\"gpt-3.5-turbo\",   # \u7528\u4e8e token \u8ba1\u6570\u7684\u6a21\u578b\n    strategy=\"sliding_window\" # \u622a\u65ad\u7b56\u7565\n)\n\nmanager = ConversationManager(config)\n</code></pre>"},{"location":"conversation-management/#_3","title":"\u6d88\u606f\u7ba1\u7406","text":""},{"location":"conversation-management/#_4","title":"\u6dfb\u52a0\u6d88\u606f","text":"<pre><code># \u8bbe\u7f6e\u7cfb\u7edf\u63d0\u793a\nconversation.set_system_prompt(\"\u4f60\u662f\u4e00\u4e2a\u7f16\u7a0b\u52a9\u624b\u3002\")\n\n# \u6dfb\u52a0\u7528\u6237\u6d88\u606f\nconversation.add_user_message(\"\u5e2e\u6211\u5199\u4e00\u4e2a\u51fd\u6570\")\n\n# \u6dfb\u52a0\u52a9\u624b\u6d88\u606f\nconversation.add_assistant_message(\"\u597d\u7684\uff0c\u8bf7\u544a\u8bc9\u6211\u9700\u8981\u4ec0\u4e48\u529f\u80fd\u3002\")\n\n# \u6dfb\u52a0\u4efb\u610f\u89d2\u8272\u7684\u6d88\u606f\nconversation.add_message(\"tool\", \"\u51fd\u6570\u6267\u884c\u7ed3\u679c\")\n</code></pre>"},{"location":"conversation-management/#_5","title":"\u6807\u8bb0\u91cd\u8981\u6d88\u606f","text":"<p>\u91cd\u8981\u6d88\u606f\u5728\u622a\u65ad\u65f6\u4f1a\u88ab\u4f18\u5148\u4fdd\u7559\uff08\u4f7f\u7528 importance \u7b56\u7565\u65f6\uff09\uff1a</p> <pre><code>conversation.add_user_message(\"\u8bb0\u4f4f\u6211\u53eb Alice\", important=True)\nconversation.add_assistant_message(\"\u597d\u7684\uff0cAlice\uff01\", important=True)\n</code></pre>"},{"location":"conversation-management/#_6","title":"\u5176\u4ed6\u64cd\u4f5c","text":"<pre><code># \u83b7\u53d6\u6d88\u606f\u5217\u8868\uff08\u526f\u672c\uff09\nmessages = conversation.messages\n\n# \u83b7\u53d6 API \u683c\u5f0f\u6d88\u606f\uff08\u4e0d\u542b\u5185\u90e8\u5143\u6570\u636e\uff09\napi_messages = conversation.get_messages_for_api()\n\n# \u5f39\u51fa\u6700\u540e\u4e00\u6761\u6d88\u606f\nlast = conversation.pop_last()\n\n# \u6e05\u7a7a\u5bf9\u8bdd\uff08\u4fdd\u7559\u7cfb\u7edf\u6d88\u606f\uff09\nconversation.clear(preserve_system=True)\n\n# \u5b8c\u5168\u6e05\u7a7a\nconversation.clear(preserve_system=False)\n</code></pre>"},{"location":"conversation-management/#_7","title":"\u622a\u65ad\u7b56\u7565","text":""},{"location":"conversation-management/#sliding-window","title":"Sliding Window\uff08\u6ed1\u52a8\u7a97\u53e3\uff09","text":"<p>\u9ed8\u8ba4\u7b56\u7565\u3002\u79fb\u9664\u6700\u65e7\u7684\u6d88\u606f\uff0c\u4fdd\u7559\u6700\u65b0\u7684\u3002</p> <pre><code>config = ConversationConfig(\n    strategy=\"sliding_window\",\n    max_tokens=4096\n)\n</code></pre> <p>\u5de5\u4f5c\u539f\u7406\uff1a 1. \u8ba1\u7b97\u7cfb\u7edf\u6d88\u606f\u5360\u7528\u7684 token 2. \u4ece\u6700\u65b0\u6d88\u606f\u5f00\u59cb\uff0c\u9010\u4e2a\u6dfb\u52a0\u76f4\u5230\u8fbe\u5230\u9650\u5236 3. \u4e22\u5f03\u65e0\u6cd5\u5bb9\u7eb3\u7684\u65e7\u6d88\u606f</p>"},{"location":"conversation-management/#keep-recentn","title":"Keep Recent\uff08\u4fdd\u7559\u6700\u8fd1N\u6761\uff09","text":"<p>\u4fdd\u7559\u56fa\u5b9a\u6570\u91cf\u7684\u6700\u8fd1\u6d88\u606f\u3002</p> <pre><code>from llm_api_router.conversation import KeepRecentStrategy\n\nstrategy = KeepRecentStrategy(keep_count=10)\nmanager = ConversationManager(config, strategy=strategy)\n</code></pre>"},{"location":"conversation-management/#importance-based","title":"Importance Based\uff08\u57fa\u4e8e\u91cd\u8981\u6027\uff09","text":"<p>\u4f18\u5148\u4fdd\u7559\u6807\u8bb0\u4e3a\u91cd\u8981\u7684\u6d88\u606f\u3002</p> <pre><code>from llm_api_router.conversation import ImportanceBasedStrategy\n\nstrategy = ImportanceBasedStrategy()\nmanager = ConversationManager(config, strategy=strategy)\n\n# \u6807\u8bb0\u91cd\u8981\u6d88\u606f\nmanager.add_user_message(\"\u5173\u952e\u4fe1\u606f\", important=True)\n</code></pre>"},{"location":"conversation-management/#token","title":"Token \u8ba1\u6570","text":""},{"location":"conversation-management/#tokencounter","title":"\u4f7f\u7528 TokenCounter","text":"<pre><code>from llm_api_router.conversation import TokenCounter\n\ncounter = TokenCounter(model=\"gpt-3.5-turbo\")\n\n# \u8ba1\u7b97\u6587\u672c token\ntokens = counter.count_tokens(\"Hello, world!\")\n\n# \u8ba1\u7b97\u6d88\u606f token\uff08\u542b\u5f00\u9500\uff09\nmsg_tokens = counter.count_message_tokens({\n    \"role\": \"user\",\n    \"content\": \"Hello\"\n})\n\n# \u8ba1\u7b97\u591a\u6761\u6d88\u606f\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi\"},\n    {\"role\": \"assistant\", \"content\": \"Hello!\"}\n]\ntotal = counter.count_messages_tokens(messages)\n</code></pre>"},{"location":"conversation-management/#token_1","title":"Token \u8ba1\u6570\u8bf4\u660e","text":"<ul> <li>\u5982\u679c\u5b89\u88c5\u4e86 <code>tiktoken</code>\uff0c\u4f7f\u7528\u7cbe\u786e\u8ba1\u6570</li> <li>\u5426\u5219\u4f7f\u7528\u4f30\u7b97\uff08\u82f1\u6587\u7ea6 4 \u5b57\u7b26/token\uff0c\u4e2d\u6587\u7ea6 1.5 \u5b57\u7b26/token\uff09</li> </ul> <p>\u5b89\u88c5 tiktoken\uff1a <pre><code>pip install tiktoken\n</code></pre></p>"},{"location":"conversation-management/#_8","title":"\u5bf9\u8bdd\u5206\u53c9","text":"<p>\u521b\u5efa\u5bf9\u8bdd\u7684\u72ec\u7acb\u526f\u672c\uff1a</p> <pre><code># \u539f\u59cb\u5bf9\u8bdd\nconversation.add_user_message(\"\u5e2e\u6211\u5199 Python\")\n\n# \u521b\u5efa\u5206\u53c9\nfork1 = conversation.fork()\nfork2 = conversation.fork()\n\n# \u72ec\u7acb\u53d1\u5c55\nfork1.add_user_message(\"\u5217\u8868\u63a8\u5bfc\u5f0f\")\nfork2.add_user_message(\"\u88c5\u9970\u5668\")\n</code></pre>"},{"location":"conversation-management/#_9","title":"\u7edf\u8ba1\u4fe1\u606f","text":"<pre><code>stats = conversation.get_stats()\nprint(stats)\n# {\n#   \"message_count\": 10,\n#   \"token_count\": 850,\n#   \"max_tokens\": 4096,\n#   \"max_messages\": 100,\n#   \"roles\": {\"system\": 1, \"user\": 5, \"assistant\": 4},\n#   \"strategy\": \"sliding_window\"\n# }\n</code></pre>"},{"location":"conversation-management/#client","title":"\u4e0e Client \u96c6\u6210","text":"<pre><code>from llm_api_router import Client, ProviderConfig\nfrom llm_api_router.conversation import create_conversation\n\n# \u521b\u5efa\u5ba2\u6237\u7aef\nconfig = ProviderConfig(provider_type=\"openai\", api_key=\"...\")\nclient = Client(config)\n\n# \u521b\u5efa\u5bf9\u8bdd\nconversation = create_conversation(\n    max_tokens=4096,\n    model=\"gpt-4\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002\"\n)\n\n# \u5bf9\u8bdd\u5faa\u73af\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'quit':\n        break\n\n    # \u6dfb\u52a0\u7528\u6237\u6d88\u606f\n    conversation.add_user_message(user_input)\n\n    # \u83b7\u53d6 API \u6d88\u606f\n    messages = conversation.get_messages_for_api()\n\n    # \u8c03\u7528 API\n    response = client.chat.completions.create(\n        messages=messages,\n        model=\"gpt-4\"\n    )\n\n    # \u83b7\u53d6\u5e76\u4fdd\u5b58\u52a9\u624b\u54cd\u5e94\n    assistant_message = response.choices[0].message.content\n    conversation.add_assistant_message(assistant_message)\n\n    print(f\"Assistant: {assistant_message}\")\n</code></pre>"},{"location":"conversation-management/#_10","title":"\u81ea\u5b9a\u4e49\u7b56\u7565","text":"<p>\u5b9e\u73b0\u81ea\u5df1\u7684\u622a\u65ad\u7b56\u7565\uff1a</p> <pre><code>from llm_api_router.conversation import TruncationStrategy, TokenCounter\n\nclass MyStrategy(TruncationStrategy):\n    def truncate(\n        self,\n        messages: list,\n        max_tokens: int,\n        token_counter: TokenCounter,\n        preserve_system: bool = True\n    ) -&gt; list:\n        # \u81ea\u5b9a\u4e49\u622a\u65ad\u903b\u8f91\n        return messages[-5:]  # \u7b80\u5355\u793a\u4f8b\uff1a\u4fdd\u7559\u6700\u540e 5 \u6761\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u7b56\u7565\nmanager = ConversationManager(config, strategy=MyStrategy())\n</code></pre>"},{"location":"conversation-management/#_11","title":"\u7ebf\u7a0b\u5b89\u5168","text":"<p><code>ConversationManager</code> \u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u53ef\u4ee5\u5728\u591a\u7ebf\u7a0b\u73af\u5883\u4e2d\u4f7f\u7528\u3002</p>"},{"location":"conversation-management/#_12","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u9009\u62e9\u5408\u9002\u7684 max_tokens</li> <li>\u7559\u51fa\u8db3\u591f\u7a7a\u95f4\u7ed9\u6a21\u578b\u54cd\u5e94</li> <li> <p>\u4e00\u822c\u8bbe\u7f6e\u4e3a\u6a21\u578b\u4e0a\u4e0b\u6587\u7684 60-80%</p> </li> <li> <p>\u4f7f\u7528\u91cd\u8981\u6027\u6807\u8bb0</p> </li> <li>\u6807\u8bb0\u5173\u952e\u4fe1\u606f\uff08\u5982\u7528\u6237\u504f\u597d\u3001\u91cd\u8981\u4e0a\u4e0b\u6587\uff09</li> <li> <p>\u914d\u5408 importance \u7b56\u7565\u4f7f\u7528</p> </li> <li> <p>\u5b9a\u671f\u68c0\u67e5\u7edf\u8ba1</p> </li> <li>\u76d1\u63a7 token \u4f7f\u7528\u60c5\u51b5</li> <li> <p>\u8c03\u6574\u622a\u65ad\u7b56\u7565</p> </li> <li> <p>\u5229\u7528\u5206\u53c9\u529f\u80fd</p> </li> <li>\u5b9e\u73b0\u591a\u8def\u5f84\u5bf9\u8bdd</li> <li>\u6d4b\u8bd5\u4e0d\u540c\u7684\u5bf9\u8bdd\u5206\u652f</li> </ol>"},{"location":"error-handling/","title":"Error Handling and Retry Mechanism","text":""},{"location":"error-handling/#overview","title":"Overview","text":"<p>LLM API Router implements a robust error handling and retry mechanism to handle transient failures gracefully. This document describes the error handling features and how to use them.</p>"},{"location":"error-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>All exceptions in LLM API Router inherit from <code>LLMRouterError</code>, which provides a consistent interface for error handling:</p> <pre><code>from llm_api_router.exceptions import (\n    LLMRouterError,           # Base exception\n    AuthenticationError,      # HTTP 401\n    PermissionError,         # HTTP 403\n    NotFoundError,           # HTTP 404\n    BadRequestError,         # HTTP 400\n    RateLimitError,          # HTTP 429\n    ProviderError,           # HTTP 5xx or other errors\n    TimeoutError,            # Request timeout\n    NetworkError,            # Network/connection errors\n    StreamError,             # Stream parsing errors\n    RetryExhaustedError,     # All retries exhausted\n)\n</code></pre>"},{"location":"error-handling/#exception-attributes","title":"Exception Attributes","text":"<p>All exceptions include the following attributes: - <code>message</code>: Error message - <code>provider</code>: Provider name (e.g., \"OpenAI\", \"Anthropic\") - <code>status_code</code>: HTTP status code (if applicable) - <code>details</code>: Additional error details (dictionary)</p> <p>Example: <pre><code>try:\n    response = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept RateLimitError as e:\n    print(f\"Rate limited by {e.provider}: {e.message}\")\n    print(f\"Status code: {e.status_code}\")\n    if \"retry_after\" in e.details:\n        print(f\"Retry after: {e.details['retry_after']} seconds\")\n</code></pre></p>"},{"location":"error-handling/#retry-mechanism","title":"Retry Mechanism","text":""},{"location":"error-handling/#automatic-retry","title":"Automatic Retry","text":"<p>The library automatically retries requests on transient failures: - Network errors (connection failures, timeouts) - Rate limit errors (HTTP 429) - Server errors (HTTP 5xx)</p> <p>Non-retryable errors (fail immediately): - Authentication errors (HTTP 401) - Permission errors (HTTP 403) - Not found errors (HTTP 404) - Bad request errors (HTTP 400)</p>"},{"location":"error-handling/#retryconfig","title":"RetryConfig","text":"<p>Configure retry behavior using <code>RetryConfig</code>:</p> <pre><code>from llm_api_router import Client, ProviderConfig, RetryConfig\n\n# Custom retry configuration\nretry_config = RetryConfig(\n    max_retries=5,              # Maximum number of retry attempts\n    initial_delay=1.0,          # Initial delay in seconds\n    max_delay=60.0,             # Maximum delay in seconds\n    exponential_base=2.0,       # Exponential backoff base\n    retry_on_status_codes=(429, 500, 502, 503, 504)  # Status codes to retry\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    retry_config=retry_config\n)\n\nclient = Client(config)\n</code></pre>"},{"location":"error-handling/#default-retry-behavior","title":"Default Retry Behavior","text":"<p>If no <code>RetryConfig</code> is provided, the library uses these defaults: - max_retries: 3 - initial_delay: 1.0 second - max_delay: 60.0 seconds - exponential_base: 2.0 - retry_on_status_codes: (429, 500, 502, 503, 504)</p>"},{"location":"error-handling/#exponential-backoff","title":"Exponential Backoff","text":"<p>Retry delays follow an exponential backoff pattern: - Attempt 1: 1 second delay - Attempt 2: 2 seconds delay - Attempt 3: 4 seconds delay - Attempt 4: 8 seconds delay - And so on, capped at <code>max_delay</code></p>"},{"location":"error-handling/#disabling-retries","title":"Disabling Retries","text":"<p>To disable retries entirely:</p> <pre><code>retry_config = RetryConfig(max_retries=0)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    retry_config=retry_config\n)\n</code></pre>"},{"location":"error-handling/#timeout-configuration","title":"Timeout Configuration","text":"<p>Configure request timeouts using the <code>timeout</code> parameter:</p> <pre><code>config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    timeout=30.0  # 30 second timeout (default is 60.0)\n)\n\nclient = Client(config)\n</code></pre> <p>The timeout applies to: - HTTP connection establishment - Request sending - Response reception</p>"},{"location":"error-handling/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"error-handling/#1-catch-specific-exceptions","title":"1. Catch Specific Exceptions","text":"<pre><code>from llm_api_router.exceptions import (\n    AuthenticationError,\n    RateLimitError,\n    RetryExhaustedError,\n    LLMRouterError\n)\n\ntry:\n    response = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError as e:\n    # Handle authentication issues\n    print(f\"Invalid API key: {e.message}\")\nexcept RateLimitError as e:\n    # Handle rate limits (though retries are automatic)\n    print(f\"Rate limited: {e.message}\")\nexcept RetryExhaustedError as e:\n    # All retry attempts failed\n    print(f\"Request failed after retries: {e.message}\")\n    print(f\"Original error: {e.details.get('original_error')}\")\nexcept LLMRouterError as e:\n    # Catch-all for any LLM Router errors\n    print(f\"Error: {e.message}\")\n</code></pre>"},{"location":"error-handling/#2-use-context-managers","title":"2. Use Context Managers","text":"<p>Always use context managers to ensure proper cleanup:</p> <pre><code># Synchronous\nwith Client(config) as client:\n    response = client.chat.completions.create(...)\n\n# Asynchronous\nasync with AsyncClient(config) as client:\n    response = await client.chat.completions.create(...)\n</code></pre>"},{"location":"error-handling/#3-handle-stream-errors","title":"3. Handle Stream Errors","text":"<p>When using streaming, handle <code>StreamError</code>:</p> <pre><code>from llm_api_router.exceptions import StreamError\n\ntry:\n    stream = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        stream=True\n    )\n    for chunk in stream:\n        print(chunk.choices[0].delta.content, end=\"\")\nexcept StreamError as e:\n    print(f\"Stream parsing error: {e.message}\")\n</code></pre>"},{"location":"error-handling/#4-log-error-details","title":"4. Log Error Details","text":"<p>Use the error attributes for logging:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    response = client.chat.completions.create(...)\nexcept LLMRouterError as e:\n    logger.error(\n        \"LLM request failed\",\n        extra={\n            \"provider\": e.provider,\n            \"status_code\": e.status_code,\n            \"message\": e.message,\n            \"details\": e.details\n        }\n    )\n</code></pre>"},{"location":"error-handling/#provider-specific-error-handling","title":"Provider-Specific Error Handling","text":"<p>Different providers may return different error formats. The library normalizes these into consistent exceptions:</p>"},{"location":"error-handling/#openai-errors","title":"OpenAI Errors","text":"<pre><code># OpenAI returns: {\"error\": {\"message\": \"...\", \"type\": \"...\"}}\n# Normalized to: AuthenticationError, RateLimitError, etc.\n</code></pre>"},{"location":"error-handling/#anthropic-errors","title":"Anthropic Errors","text":"<pre><code># Anthropic returns: {\"error\": {\"type\": \"...\", \"message\": \"...\"}}\n# Normalized to: AuthenticationError, RateLimitError, etc.\n</code></pre> <p>All providers use the same exception types, making error handling consistent across providers.</p>"},{"location":"error-handling/#testing-error-handling","title":"Testing Error Handling","text":"<p>When testing, you can disable retries for faster tests:</p> <pre><code>import pytest\nfrom llm_api_router import ProviderConfig, RetryConfig\n\n@pytest.fixture\ndef no_retry_config():\n    return ProviderConfig(\n        provider_type=\"openai\",\n        api_key=\"test-key\",\n        retry_config=RetryConfig(max_retries=0)\n    )\n\ndef test_authentication_error(no_retry_config):\n    # Test fails immediately without retries\n    with pytest.raises(AuthenticationError):\n        client = Client(no_retry_config)\n        # ... test code ...\n</code></pre>"},{"location":"error-handling/#summary","title":"Summary","text":"<p>The error handling and retry mechanism in LLM API Router provides:</p> <p>\u2713 Automatic retries for transient failures \u2713 Exponential backoff to avoid overwhelming servers \u2713 Consistent exceptions across all providers \u2713 Configurable behavior for different use cases \u2713 Detailed error information for debugging \u2713 Timeout control for time-sensitive applications  </p> <p>This makes your applications more robust and resilient to temporary failures.</p>"},{"location":"faq/","title":"\u5e38\u89c1\u95ee\u9898 (FAQ)","text":""},{"location":"faq/#_1","title":"\u5b89\u88c5\u95ee\u9898","text":""},{"location":"faq/#q","title":"Q: \u5b89\u88c5\u65f6\u63d0\u793a\u4f9d\u8d56\u51b2\u7a81\u600e\u4e48\u529e\uff1f","text":"<p>\u4f7f\u7528 <code>uv</code> \u6216\u521b\u5efa\u865a\u62df\u73af\u5883\u9694\u79bb\u4f9d\u8d56\uff1a</p> <pre><code># \u4f7f\u7528 uv\nuv venv\nsource .venv/bin/activate\nuv pip install llm-api-router\n\n# \u6216\u4f7f\u7528 pip\npython -m venv venv\nsource venv/bin/activate\npip install llm-api-router\n</code></pre>"},{"location":"faq/#q_1","title":"Q: \u5982\u4f55\u5b89\u88c5\u7279\u5b9a\u529f\u80fd\u7684\u4f9d\u8d56\uff1f","text":"<pre><code># \u5b89\u88c5 CLI \u5de5\u5177\npip install llm-api-router[cli]\n\n# \u5b89\u88c5\u6587\u6863\u5de5\u5177\npip install llm-api-router[docs]\n\n# \u5b89\u88c5\u6240\u6709\u529f\u80fd\npip install llm-api-router[cli,docs]\n\n# \u5f00\u53d1\u8005\u5b89\u88c5\npip install llm-api-router[dev]\n</code></pre>"},{"location":"faq/#api-key","title":"API Key \u95ee\u9898","text":""},{"location":"faq/#q-api-key","title":"Q: \u5982\u4f55\u5b89\u5168\u5b58\u50a8 API Key\uff1f","text":"<p>\u63a8\u8350\u4f7f\u7528\u73af\u5883\u53d8\u91cf\uff1a</p> <pre><code># .env \u6587\u4ef6\nOPENAI_API_KEY=sk-xxx\nANTHROPIC_API_KEY=sk-ant-xxx\n</code></pre> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n)\n</code></pre>"},{"location":"faq/#q-authenticationerror","title":"Q: \u6536\u5230 AuthenticationError \u600e\u4e48\u529e\uff1f","text":"<ol> <li>\u68c0\u67e5 API Key \u662f\u5426\u6b63\u786e</li> <li>\u786e\u8ba4 API Key \u662f\u5426\u6709\u6548\uff08\u672a\u8fc7\u671f\u3001\u672a\u88ab\u64a4\u9500\uff09</li> <li>\u9a8c\u8bc1\u662f\u5426\u6709\u8db3\u591f\u7684\u914d\u989d</li> <li>\u68c0\u67e5\u63d0\u4f9b\u5546\u8d26\u6237\u72b6\u6001</li> </ol> <pre><code>from llm_api_router.exceptions import AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"\u8ba4\u8bc1\u5931\u8d25: {e}\")\n</code></pre>"},{"location":"faq/#_2","title":"\u4f7f\u7528\u95ee\u9898","text":""},{"location":"faq/#q_2","title":"Q: \u5982\u4f55\u5904\u7406\u6d41\u5f0f\u54cd\u5e94\uff1f","text":"<pre><code>stream = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"\u5199\u4e00\u9996\u8bd7\"}],\n    stream=True\n)\n\nfull_response = \"\"\nfor chunk in stream:\n    content = chunk.choices[0].delta.content\n    if content:\n        full_response += content\n        print(content, end=\"\", flush=True)\n</code></pre>"},{"location":"faq/#q_3","title":"Q: \u5982\u4f55\u5b9e\u73b0\u591a\u8f6e\u5bf9\u8bdd\uff1f","text":"<p>\u4f7f\u7528 <code>ConversationManager</code>\uff1a</p> <pre><code>from llm_api_router import ConversationManager\n\nmanager = ConversationManager(system_message=\"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u52a9\u624b\")\n\n# \u7b2c\u4e00\u8f6e\nmanager.add_user_message(\"\u4f60\u597d\")\nresponse = client.chat.completions.create(messages=manager.get_messages())\nmanager.add_assistant_message(response.choices[0].message.content)\n\n# \u7b2c\u4e8c\u8f6e\nmanager.add_user_message(\"\u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")\nresponse = client.chat.completions.create(messages=manager.get_messages())\n</code></pre>"},{"location":"faq/#q-function-calling","title":"Q: \u5982\u4f55\u4f7f\u7528 Function Calling\uff1f","text":"<pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"\u83b7\u53d6\u57ce\u5e02\u5929\u6c14\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\"type\": \"string\", \"description\": \"\u57ce\u5e02\u540d\u79f0\"}\n                },\n                \"required\": [\"city\"]\n            }\n        }\n    }\n]\n\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"\u5317\u4eac\u4eca\u5929\u5929\u6c14\u600e\u4e48\u6837\uff1f\"}],\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"\u8c03\u7528\u51fd\u6570: {tool_call.function.name}\")\n    print(f\"\u53c2\u6570: {tool_call.function.arguments}\")\n</code></pre>"},{"location":"faq/#q_4","title":"Q: \u5982\u4f55\u5207\u6362\u4e0d\u540c\u7684\u6a21\u578b\uff1f","text":"<p>\u5728\u8bf7\u6c42\u65f6\u6307\u5b9a model \u53c2\u6570\uff1a</p> <pre><code># \u4f7f\u7528\u9ed8\u8ba4\u6a21\u578b\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# \u4f7f\u7528\u7279\u5b9a\u6a21\u578b\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"gpt-4o\"  # \u8986\u76d6\u9ed8\u8ba4\u6a21\u578b\n)\n</code></pre>"},{"location":"faq/#_3","title":"\u6027\u80fd\u95ee\u9898","text":""},{"location":"faq/#q_5","title":"Q: \u5982\u4f55\u51cf\u5c11\u5ef6\u8fdf\uff1f","text":"<ol> <li>\u4f7f\u7528\u7f13\u5b58\uff1a\u76f8\u540c\u8bf7\u6c42\u76f4\u63a5\u8fd4\u56de\u7f13\u5b58\u7ed3\u679c</li> <li>\u4f7f\u7528\u6d41\u5f0f\uff1a\u5c3d\u65e9\u5f00\u59cb\u5904\u7406\u54cd\u5e94</li> <li>\u9009\u62e9\u66f4\u5feb\u7684\u6a21\u578b\uff1a\u5982 gpt-3.5-turbo\u3001claude-3-haiku</li> <li>\u51cf\u5c11 token \u6570\uff1a\u7cbe\u7b80 prompt</li> </ol> <pre><code># \u542f\u7528\u7f13\u5b58\nfrom llm_api_router.cache import MemoryCache\n\ncache = MemoryCache(max_size=1000, ttl=3600)\nwith Client(config, cache=cache) as client:\n    # \u76f8\u540c\u8bf7\u6c42\u4f1a\u547d\u4e2d\u7f13\u5b58\n    response = client.chat.completions.create(...)\n</code></pre>"},{"location":"faq/#q_6","title":"Q: \u5982\u4f55\u5904\u7406\u901f\u7387\u9650\u5236\uff1f","text":"<p>\u4f7f\u7528 <code>RateLimiter</code>\uff1a</p> <pre><code>from llm_api_router import RateLimiter\n\nlimiter = RateLimiter(\n    requests_per_minute=60,\n    tokens_per_minute=100000,\n)\n\nasync with limiter.acquire():\n    response = await client.chat.completions.create(...)\n</code></pre>"},{"location":"faq/#q_7","title":"Q: \u5982\u4f55\u63d0\u9ad8\u5e76\u53d1\u6027\u80fd\uff1f","text":"<p>\u4f7f\u7528\u5f02\u6b65\u5ba2\u6237\u7aef\u548c\u8fde\u63a5\u6c60\uff1a</p> <pre><code>import asyncio\nfrom llm_api_router import AsyncClient\n\nasync def process_batch(prompts):\n    async with AsyncClient(config) as client:\n        tasks = [\n            client.chat.completions.create(\n                messages=[{\"role\": \"user\", \"content\": p}]\n            )\n            for p in prompts\n        ]\n        return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"faq/#_4","title":"\u9519\u8bef\u5904\u7406","text":""},{"location":"faq/#q-ratelimiterror","title":"Q: \u6536\u5230 RateLimitError \u600e\u4e48\u529e\uff1f","text":"<pre><code>from llm_api_router.exceptions import RateLimitError\nimport time\n\ntry:\n    response = client.chat.completions.create(...)\nexcept RateLimitError as e:\n    # \u7b49\u5f85\u540e\u91cd\u8bd5\n    wait_time = e.retry_after or 60\n    time.sleep(wait_time)\n    response = client.chat.completions.create(...)\n</code></pre>"},{"location":"faq/#q_8","title":"Q: \u5982\u4f55\u5904\u7406\u8d85\u65f6\uff1f","text":"<pre><code>from llm_api_router.exceptions import TimeoutError\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",\n    timeout=60.0,  # \u589e\u52a0\u8d85\u65f6\u65f6\u95f4\n)\n\ntry:\n    response = client.chat.completions.create(...)\nexcept TimeoutError:\n    print(\"\u8bf7\u6c42\u8d85\u65f6\uff0c\u8bf7\u91cd\u8bd5\")\n</code></pre>"},{"location":"faq/#q_9","title":"Q: \u5982\u4f55\u5b9e\u73b0\u81ea\u52a8\u91cd\u8bd5\uff1f","text":"<p>\u5185\u7f6e\u91cd\u8bd5\u673a\u5236\u4f1a\u81ea\u52a8\u5904\u7406\u4e34\u65f6\u9519\u8bef\uff1a</p> <pre><code>config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",\n    max_retries=5,  # \u6700\u591a\u91cd\u8bd55\u6b21\n)\n</code></pre>"},{"location":"faq/#ollama","title":"Ollama \u672c\u5730\u90e8\u7f72","text":""},{"location":"faq/#q-ollama","title":"Q: \u5982\u4f55\u4f7f\u7528\u672c\u5730 Ollama\uff1f","text":"<pre><code># 1. \u5b89\u88c5 Ollama\nbrew install ollama\n\n# 2. \u542f\u52a8\u670d\u52a1\nollama serve\n\n# 3. \u62c9\u53d6\u6a21\u578b\nollama pull llama3.2\n</code></pre> <pre><code>config = ProviderConfig(\n    provider_type=\"ollama\",\n    api_key=\"\",  # \u4e0d\u9700\u8981\n    base_url=\"http://localhost:11434\",\n    default_model=\"llama3.2\"\n)\n</code></pre>"},{"location":"faq/#q-ollama_1","title":"Q: Ollama \u8fde\u63a5\u5931\u8d25\u600e\u4e48\u529e\uff1f","text":"<ol> <li>\u786e\u8ba4 Ollama \u670d\u52a1\u6b63\u5728\u8fd0\u884c\uff1a<code>ollama serve</code></li> <li>\u68c0\u67e5\u7aef\u53e3\u662f\u5426\u6b63\u786e\uff08\u9ed8\u8ba4 11434\uff09</li> <li>\u786e\u8ba4\u6a21\u578b\u5df2\u4e0b\u8f7d\uff1a<code>ollama list</code></li> </ol>"},{"location":"faq/#cli","title":"CLI \u5de5\u5177","text":""},{"location":"faq/#q-cli","title":"Q: CLI \u547d\u4ee4\u627e\u4e0d\u5230\uff1f","text":"<p>\u786e\u4fdd\u5b89\u88c5\u4e86 CLI \u4f9d\u8d56\uff1a</p> <pre><code>pip install llm-api-router[cli]\n</code></pre> <p>\u6216\u8005\u4f7f\u7528 Python \u6a21\u5757\u65b9\u5f0f\u8fd0\u884c\uff1a</p> <pre><code>python -m llm_api_router.cli --help\n</code></pre>"},{"location":"faq/#q-cli_1","title":"Q: \u5982\u4f55\u4f7f\u7528 CLI \u6d4b\u8bd5\u8fde\u63a5\uff1f","text":"<pre><code># \u6d4b\u8bd5 OpenAI\nllm-router test openai --api-key sk-xxx\n\n# \u6d4b\u8bd5 Anthropic\nllm-router test anthropic --api-key sk-ant-xxx\n\n# \u6d4b\u8bd5\u672c\u5730 Ollama\nllm-router test ollama --base-url http://localhost:11434\n</code></pre>"},{"location":"faq/#_5","title":"\u5176\u4ed6\u95ee\u9898","text":""},{"location":"faq/#q-python","title":"Q: \u652f\u6301\u54ea\u4e9b Python \u7248\u672c\uff1f","text":"<p>Python 3.10 \u53ca\u4ee5\u4e0a\u7248\u672c\u3002</p>"},{"location":"faq/#q-token","title":"Q: \u5982\u4f55\u83b7\u53d6 token \u4f7f\u7528\u7edf\u8ba1\uff1f","text":"<pre><code>response = client.chat.completions.create(...)\nprint(f\"\u8f93\u5165 tokens: {response.usage.prompt_tokens}\")\nprint(f\"\u8f93\u51fa tokens: {response.usage.completion_tokens}\")\nprint(f\"\u603b tokens: {response.usage.total_tokens}\")\n</code></pre>"},{"location":"faq/#q-bug","title":"Q: \u5982\u4f55\u62a5\u544a bug \u6216\u63d0\u51fa\u5efa\u8bae\uff1f","text":"<p>\u8bf7\u5728 GitHub \u4ed3\u5e93\u63d0\u4ea4 Issue\uff1a</p> <ol> <li>\u63cf\u8ff0\u95ee\u9898\u6216\u5efa\u8bae</li> <li>\u63d0\u4f9b\u590d\u73b0\u6b65\u9aa4</li> <li>\u9644\u4e0a\u76f8\u5173\u4ee3\u7801\u548c\u9519\u8bef\u4fe1\u606f</li> <li>\u6ce8\u660e Python \u7248\u672c\u548c\u4f9d\u8d56\u7248\u672c</li> </ol>"},{"location":"faq/#q_10","title":"Q: \u5982\u4f55\u8d21\u732e\u4ee3\u7801\uff1f","text":"<ol> <li>Fork \u4ed3\u5e93</li> <li>\u521b\u5efa\u7279\u6027\u5206\u652f</li> <li>\u7f16\u5199\u4ee3\u7801\u548c\u6d4b\u8bd5</li> <li>\u63d0\u4ea4 Pull Request</li> </ol> <p>\u53c2\u89c1 \u8d21\u732e\u6307\u5357\u3002</p>"},{"location":"load-balancer/","title":"\u8d1f\u8f7d\u5747\u8861\u5668 (Load Balancer)","text":"<p>\u8d1f\u8f7d\u5747\u8861\u5668\u6a21\u5757\u63d0\u4f9b\u591a\u7aef\u70b9\u7ba1\u7406\u3001\u8d1f\u8f7d\u5206\u53d1\u548c\u81ea\u52a8\u6545\u969c\u8f6c\u79fb\u529f\u80fd\uff0c\u786e\u4fdd\u9ad8\u53ef\u7528\u6027\u548c\u6700\u4f18\u8d44\u6e90\u5229\u7528\u3002</p>"},{"location":"load-balancer/#_1","title":"\u529f\u80fd\u7279\u6027","text":"<ul> <li>\u591a\u79cd\u9009\u62e9\u7b56\u7565: \u8f6e\u8be2\u3001\u52a0\u6743\u3001\u6700\u4f4e\u5ef6\u8fdf\u3001\u968f\u673a\u3001\u6545\u969c\u8f6c\u79fb</li> <li>\u5065\u5eb7\u68c0\u67e5: \u81ea\u52a8\u6807\u8bb0\u4e0d\u5065\u5eb7\u7aef\u70b9\u5e76\u5728\u6062\u590d\u540e\u91cd\u65b0\u542f\u7528</li> <li>\u7edf\u8ba1\u8ffd\u8e2a: \u8bf7\u6c42\u8ba1\u6570\u3001\u6210\u529f\u7387\u3001\u5ef6\u8fdf\u76d1\u63a7</li> <li>\u7ebf\u7a0b\u5b89\u5168: \u652f\u6301\u5e76\u53d1\u64cd\u4f5c</li> <li>\u52a8\u6001\u7ba1\u7406: \u8fd0\u884c\u65f6\u6dfb\u52a0/\u79fb\u9664\u7aef\u70b9</li> </ul>"},{"location":"load-balancer/#_2","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"load-balancer/#_3","title":"\u57fa\u7840\u7528\u6cd5","text":"<pre><code>from llm_api_router import LoadBalancer, Endpoint\n\n# \u521b\u5efa\u7aef\u70b9\nendpoints = [\n    Endpoint(name=\"primary\", provider=\"openai\", weight=3),\n    Endpoint(name=\"secondary\", provider=\"anthropic\", weight=1),\n]\n\n# \u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5668\nlb = LoadBalancer(\n    endpoints=endpoints,\n    strategy=\"weighted\",  # \u53ef\u9009: round_robin, weighted, least_latency, random, failover\n)\n\n# \u83b7\u53d6\u7aef\u70b9\u5e76\u53d1\u8d77\u8bf7\u6c42\nendpoint = lb.get_endpoint()\ntry:\n    # \u53d1\u8d77\u5b9e\u9645\u8bf7\u6c42...\n    response = make_request(endpoint)\n    lb.mark_success(endpoint, latency=response.elapsed)\nexcept Exception as e:\n    lb.mark_failure(endpoint)\n</code></pre>"},{"location":"load-balancer/#_4","title":"\u6545\u969c\u8f6c\u79fb\u6a21\u5f0f","text":"<pre><code>from llm_api_router import LoadBalancer, Endpoint, LoadBalancerConfig\n\n# \u4f18\u5148\u7ea7\u8d8a\u4f4e\uff0c\u4f18\u5148\u7ea7\u8d8a\u9ad8\nendpoints = [\n    Endpoint(name=\"primary\", provider=\"openai\", priority=0),\n    Endpoint(name=\"backup1\", provider=\"anthropic\", priority=1),\n    Endpoint(name=\"backup2\", provider=\"gemini\", priority=2),\n]\n\nconfig = LoadBalancerConfig(\n    failure_threshold=3,    # \u8fde\u7eed3\u6b21\u5931\u8d25\u540e\u6807\u8bb0\u4e3a\u4e0d\u5065\u5eb7\n    recovery_time=60.0,     # 60\u79d2\u540e\u91cd\u8bd5\u4e0d\u5065\u5eb7\u7aef\u70b9\n)\n\nlb = LoadBalancer(\n    endpoints=endpoints,\n    strategy=\"failover\",\n    config=config,\n)\n</code></pre>"},{"location":"load-balancer/#_5","title":"\u9009\u62e9\u7b56\u7565","text":""},{"location":"load-balancer/#round-robin","title":"Round Robin (\u8f6e\u8be2)","text":"<p>\u6309\u987a\u5e8f\u5faa\u73af\u9009\u62e9\u7aef\u70b9\u3002</p> <pre><code>lb = LoadBalancer(endpoints=endpoints, strategy=\"round_robin\")\n# \u4f9d\u6b21\u8fd4\u56de: a, b, c, a, b, c, ...\n</code></pre>"},{"location":"load-balancer/#weighted","title":"Weighted (\u52a0\u6743)","text":"<p>\u6839\u636e\u6743\u91cd\u968f\u673a\u9009\u62e9\uff0c\u6743\u91cd\u8d8a\u9ad8\u88ab\u9009\u4e2d\u6982\u7387\u8d8a\u5927\u3002</p> <pre><code>endpoints = [\n    Endpoint(name=\"high\", weight=10),  # \u88ab\u9009\u4e2d\u6982\u7387\u7ea6 10/11\n    Endpoint(name=\"low\", weight=1),    # \u88ab\u9009\u4e2d\u6982\u7387\u7ea6 1/11\n]\nlb = LoadBalancer(endpoints=endpoints, strategy=\"weighted\")\n</code></pre>"},{"location":"load-balancer/#least-latency","title":"Least Latency (\u6700\u4f4e\u5ef6\u8fdf)","text":"<p>\u9009\u62e9\u5e73\u5747\u54cd\u5e94\u5ef6\u8fdf\u6700\u4f4e\u7684\u7aef\u70b9\u3002</p> <pre><code>lb = LoadBalancer(endpoints=endpoints, strategy=\"least_latency\")\n\n# \u8bb0\u5f55\u5ef6\u8fdf\u4ee5\u542f\u7528\u7b56\u7565\nlb.mark_success(endpoint, latency=0.5)  # \u8bb0\u5f55\u5ef6\u8fdf\n</code></pre>"},{"location":"load-balancer/#random","title":"Random (\u968f\u673a)","text":"<p>\u5b8c\u5168\u968f\u673a\u9009\u62e9\u3002</p> <pre><code>lb = LoadBalancer(endpoints=endpoints, strategy=\"random\")\n</code></pre>"},{"location":"load-balancer/#failover","title":"Failover (\u6545\u969c\u8f6c\u79fb)","text":"<p>\u6309\u4f18\u5148\u7ea7\u9009\u62e9\uff0c\u603b\u662f\u9009\u62e9\u4f18\u5148\u7ea7\u6700\u9ad8\uff08priority \u6700\u4f4e\uff09\u7684\u5065\u5eb7\u7aef\u70b9\u3002</p> <pre><code>endpoints = [\n    Endpoint(name=\"primary\", priority=0),   # \u9996\u9009\n    Endpoint(name=\"secondary\", priority=1), # \u5907\u7528\n]\nlb = LoadBalancer(endpoints=endpoints, strategy=\"failover\")\n</code></pre>"},{"location":"load-balancer/#_6","title":"\u5065\u5eb7\u7ba1\u7406","text":""},{"location":"load-balancer/#_7","title":"\u6807\u8bb0\u6210\u529f/\u5931\u8d25","text":"<pre><code>endpoint = lb.get_endpoint()\n\ntry:\n    result = call_api(endpoint)\n    lb.mark_success(endpoint, latency=result.elapsed)\nexcept Exception:\n    lb.mark_failure(endpoint)\n</code></pre>"},{"location":"load-balancer/#_8","title":"\u7aef\u70b9\u72b6\u6001","text":"<p>\u7aef\u70b9\u6709\u4e09\u79cd\u72b6\u6001\uff1a - HEALTHY: \u6b63\u5e38\uff0c\u53ef\u7528 - DEGRADED: \u6709\u5931\u8d25\u8bb0\u5f55\u4f46\u672a\u8fbe\u9608\u503c\uff0c\u4ecd\u53ef\u7528 - UNHEALTHY: \u8fde\u7eed\u5931\u8d25\u8fbe\u5230\u9608\u503c\uff0c\u6682\u65f6\u6392\u9664</p>"},{"location":"load-balancer/#_9","title":"\u81ea\u52a8\u6062\u590d","text":"<p>\u4e0d\u5065\u5eb7\u7684\u7aef\u70b9\u5728 <code>recovery_time</code> \u540e\u4f1a\u88ab\u91cd\u65b0\u5c1d\u8bd5\uff1a</p> <pre><code>config = LoadBalancerConfig(\n    failure_threshold=3,   # 3\u6b21\u8fde\u7eed\u5931\u8d25\u540e\u4e0d\u5065\u5eb7\n    recovery_time=30.0,    # 30\u79d2\u540e\u91cd\u8bd5\n)\n</code></pre>"},{"location":"load-balancer/#_10","title":"\u7edf\u8ba1\u4fe1\u606f","text":""},{"location":"load-balancer/#_11","title":"\u83b7\u53d6\u7aef\u70b9\u7edf\u8ba1","text":"<pre><code># \u83b7\u53d6\u5355\u4e2a\u7aef\u70b9\u7edf\u8ba1\nstats = lb.get_stats(\"primary\")\nprint(f\"\u6210\u529f\u7387: {stats['success_rate']:.2%}\")\nprint(f\"\u5e73\u5747\u5ef6\u8fdf: {stats['avg_latency']:.3f}s\")\n\n# \u83b7\u53d6\u6240\u6709\u7aef\u70b9\u7edf\u8ba1\nall_stats = lb.get_stats()\nfor name, stats in all_stats.items():\n    print(f\"{name}: {stats['total_requests']} requests, {stats['status']}\")\n</code></pre>"},{"location":"load-balancer/#_12","title":"\u7edf\u8ba1\u5b57\u6bb5","text":"\u5b57\u6bb5 \u63cf\u8ff0 status \u7aef\u70b9\u72b6\u6001 (healthy/degraded/unhealthy) total_requests \u603b\u8bf7\u6c42\u6570 successful_requests \u6210\u529f\u8bf7\u6c42\u6570 failed_requests \u5931\u8d25\u8bf7\u6c42\u6570 success_rate \u6210\u529f\u7387 consecutive_failures \u5f53\u524d\u8fde\u7eed\u5931\u8d25\u6b21\u6570 avg_latency \u5e73\u5747\u5ef6\u8fdf\uff08\u79d2\uff09"},{"location":"load-balancer/#_13","title":"\u52a8\u6001\u7aef\u70b9\u7ba1\u7406","text":""},{"location":"load-balancer/#_14","title":"\u6dfb\u52a0\u7aef\u70b9","text":"<pre><code>new_endpoint = Endpoint(name=\"new\", provider=\"openai\")\nlb.add_endpoint(new_endpoint)\n</code></pre>"},{"location":"load-balancer/#_15","title":"\u79fb\u9664\u7aef\u70b9","text":"<pre><code>lb.remove_endpoint(\"old\")\n</code></pre>"},{"location":"load-balancer/#_16","title":"\u91cd\u7f6e\u7edf\u8ba1","text":"<pre><code>lb.reset_stats(\"primary\")  # \u91cd\u7f6e\u5355\u4e2a\nlb.reset_stats()           # \u91cd\u7f6e\u6240\u6709\n</code></pre>"},{"location":"load-balancer/#_17","title":"\u5b8c\u6574\u793a\u4f8b","text":"<pre><code>from llm_api_router import (\n    LoadBalancer,\n    Endpoint,\n    LoadBalancerConfig,\n    create_load_balancer,\n)\n\n# \u65b9\u5f0f1: \u4f7f\u7528\u7c7b\nendpoints = [\n    Endpoint(\n        name=\"openai-1\",\n        provider=\"openai\",\n        url=\"https://api.openai.com/v1\",\n        weight=2,\n    ),\n    Endpoint(\n        name=\"openai-2\",\n        provider=\"openai\",\n        url=\"https://api-backup.openai.com/v1\",\n        weight=1,\n    ),\n]\n\nconfig = LoadBalancerConfig(\n    failure_threshold=3,\n    recovery_time=60.0,\n    latency_window=20,\n)\n\nlb = LoadBalancer(\n    endpoints=endpoints,\n    strategy=\"weighted\",\n    config=config,\n)\n\n# \u65b9\u5f0f2: \u4f7f\u7528\u4fbf\u6377\u51fd\u6570\nlb = create_load_balancer(\n    endpoints=[\n        {\"name\": \"primary\", \"provider\": \"openai\", \"weight\": 3},\n        {\"name\": \"backup\", \"provider\": \"anthropic\", \"weight\": 1},\n    ],\n    strategy=\"weighted\",\n    failure_threshold=3,\n    recovery_time=60.0,\n)\n\n# \u4f7f\u7528\u8d1f\u8f7d\u5747\u8861\u5668\ndef call_with_failover():\n    tried = []\n\n    while True:\n        endpoint = lb.get_endpoint(exclude=tried)\n        if not endpoint:\n            raise Exception(\"All endpoints failed\")\n\n        tried.append(endpoint.name)\n\n        try:\n            result = call_api(endpoint)\n            lb.mark_success(endpoint, latency=result.elapsed)\n            return result\n        except Exception:\n            lb.mark_failure(endpoint)\n            continue\n</code></pre>"},{"location":"load-balancer/#api","title":"API \u53c2\u8003","text":""},{"location":"load-balancer/#endpoint","title":"Endpoint","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 name str \u5fc5\u586b \u7aef\u70b9\u552f\u4e00\u6807\u8bc6 url str \"\" \u57fa\u7840 URL provider str \"\" \u63d0\u4f9b\u5546\u540d\u79f0 api_key str \"\" API \u5bc6\u94a5 weight int 1 \u6743\u91cd\uff08\u7528\u4e8e\u52a0\u6743\u7b56\u7565\uff09 priority int 0 \u4f18\u5148\u7ea7\uff08\u7528\u4e8e\u6545\u969c\u8f6c\u79fb\uff09 metadata dict {} \u989d\u5916\u5143\u6570\u636e"},{"location":"load-balancer/#loadbalancerconfig","title":"LoadBalancerConfig","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 failure_threshold int 3 \u4e0d\u5065\u5eb7\u9608\u503c recovery_time float 30.0 \u6062\u590d\u7b49\u5f85\u65f6\u95f4\uff08\u79d2\uff09 latency_window int 10 \u5ef6\u8fdf\u6837\u672c\u7a97\u53e3\u5927\u5c0f degraded_threshold int 1 \u964d\u7ea7\u9608\u503c"},{"location":"load-balancer/#loadbalancer","title":"LoadBalancer \u65b9\u6cd5","text":"\u65b9\u6cd5 \u63cf\u8ff0 get_endpoint(exclude=None) \u83b7\u53d6\u4e0b\u4e00\u4e2a\u7aef\u70b9 mark_success(endpoint, latency=None) \u6807\u8bb0\u8bf7\u6c42\u6210\u529f mark_failure(endpoint) \u6807\u8bb0\u8bf7\u6c42\u5931\u8d25 get_stats(endpoint_name=None) \u83b7\u53d6\u7edf\u8ba1\u4fe1\u606f get_healthy_endpoints() \u83b7\u53d6\u5065\u5eb7\u7aef\u70b9\u5217\u8868 add_endpoint(endpoint) \u6dfb\u52a0\u65b0\u7aef\u70b9 remove_endpoint(name) \u79fb\u9664\u7aef\u70b9 reset_stats(name=None) \u91cd\u7f6e\u7edf\u8ba1 set_strategy(strategy) \u5207\u6362\u7b56\u7565"},{"location":"logging/","title":"Logging System Documentation","text":"<p>The LLM API Router includes a comprehensive logging system that provides structured logging, sensitive data filtering, request tracking, and flexible configuration.</p>"},{"location":"logging/#features","title":"Features","text":"<ul> <li>Structured Logging: Support for both text and JSON formats</li> <li>Sensitive Data Filtering: Automatically masks API keys, tokens, and other sensitive information</li> <li>Request ID Tracking: Unique IDs for correlating logs across operations</li> <li>Configurable Levels: Support for DEBUG, INFO, WARNING, ERROR, and CRITICAL levels</li> <li>Rich Context: Log provider names, models, latency, token usage, and more</li> <li>Retry Tracking: Automatic logging of retry attempts with exponential backoff</li> </ul>"},{"location":"logging/#quick-start","title":"Quick Start","text":""},{"location":"logging/#basic-usage","title":"Basic Usage","text":"<pre><code>from llm_api_router import Client, ProviderConfig\n\n# Logging is enabled by default with INFO level\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    default_model=\"gpt-3.5-turbo\"\n)\n\nclient = Client(config)\n# Logs will automatically be generated for requests, responses, and errors\n</code></pre>"},{"location":"logging/#custom-logging-configuration","title":"Custom Logging Configuration","text":"<pre><code>from llm_api_router import ProviderConfig, LogConfig\n\n# Configure logging\nlog_config = LogConfig(\n    level=\"DEBUG\",           # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL\n    format=\"json\",          # Format: \"text\" or \"json\"\n    enable_request_id=True, # Enable unique request ID tracking\n    filter_sensitive=True,  # Filter sensitive data (API keys, tokens)\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    default_model=\"gpt-3.5-turbo\",\n    log_config=log_config\n)\n\nclient = Client(config)\n</code></pre>"},{"location":"logging/#configuration-options","title":"Configuration Options","text":""},{"location":"logging/#logconfig-parameters","title":"LogConfig Parameters","text":"Parameter Type Default Description <code>level</code> str \"INFO\" Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) <code>format</code> str \"text\" Output format (\"text\" or \"json\") <code>enable_request_id</code> bool True Generate unique request IDs <code>filter_sensitive</code> bool True Filter sensitive data from logs <code>log_requests</code> bool True Log API requests <code>log_responses</code> bool True Log API responses <code>log_errors</code> bool True Log errors <code>sensitive_patterns</code> list [default patterns] Regex patterns for sensitive data"},{"location":"logging/#default-sensitive-patterns","title":"Default Sensitive Patterns","text":"<p>The following patterns are filtered by default:</p> <ul> <li>Bearer tokens: <code>Bearer \\S+</code></li> <li>API keys: <code>api[_-]?key['\\\"]?\\s*[:=]\\s*['\\\"]?[\\w-]+</code></li> <li>OpenAI-style keys: <code>sk-[\\w-]+</code></li> <li>Common fields: <code>authorization</code>, <code>api_key</code>, <code>token</code>, <code>secret</code></li> </ul>"},{"location":"logging/#log-formats","title":"Log Formats","text":""},{"location":"logging/#text-format","title":"Text Format","text":"<p>Human-readable format with timestamps:</p> <pre><code>2026-01-29 16:46:05 - llm_api_router.OpenAIProvider - INFO - [req-123] Sending chat completion request\n2026-01-29 16:46:05 - llm_api_router.OpenAIProvider - INFO - [req-123] Chat completion successful\n</code></pre>"},{"location":"logging/#json-format","title":"JSON Format","text":"<p>Structured format for machine parsing:</p> <pre><code>{\n  \"timestamp\": \"2026-01-29T16:46:05.468899+00:00\",\n  \"level\": \"INFO\",\n  \"logger\": \"llm_api_router.OpenAIProvider\",\n  \"message\": \"Sending chat completion request\",\n  \"request_id\": \"req-123\",\n  \"provider\": \"OpenAI\",\n  \"model\": \"gpt-3.5-turbo\",\n  \"message_count\": 2\n}\n\n{\n  \"timestamp\": \"2026-01-29T16:46:05.469804+00:00\",\n  \"level\": \"INFO\",\n  \"logger\": \"llm_api_router.OpenAIProvider\",\n  \"message\": \"Chat completion successful\",\n  \"request_id\": \"req-123\",\n  \"provider\": \"OpenAI\",\n  \"latency_ms\": 245,\n  \"tokens\": 156\n}\n</code></pre>"},{"location":"logging/#logged-information","title":"Logged Information","text":""},{"location":"logging/#request-logs","title":"Request Logs","text":"<p>For each API request, the following information is logged:</p> <ul> <li>Request ID (unique identifier)</li> <li>Provider name (e.g., \"OpenAI\", \"Anthropic\")</li> <li>Model name</li> <li>Number of messages</li> <li>Stream mode indicator</li> </ul> <p>Example: <pre><code>logger.info(\n    \"Sending chat completion request\",\n    extra={\n        \"request_id\": \"req-abc123\",\n        \"provider\": \"OpenAI\",\n        \"model\": \"gpt-3.5-turbo\",\n        \"message_count\": 2\n    }\n)\n</code></pre></p>"},{"location":"logging/#response-logs","title":"Response Logs","text":"<p>For successful responses, the following is logged:</p> <ul> <li>Request ID (for correlation)</li> <li>Provider name</li> <li>Latency in milliseconds</li> <li>Token usage (prompt, completion, total)</li> </ul> <p>Example: <pre><code>logger.info(\n    \"Chat completion successful\",\n    extra={\n        \"request_id\": \"req-abc123\",\n        \"provider\": \"OpenAI\",\n        \"latency_ms\": 245,\n        \"tokens\": 156\n    }\n)\n</code></pre></p>"},{"location":"logging/#error-logs","title":"Error Logs","text":"<p>For errors, comprehensive information is logged:</p> <ul> <li>Request ID</li> <li>Provider name</li> <li>HTTP status code</li> <li>Error message</li> <li>Exception traceback (for DEBUG level)</li> </ul> <p>Example: <pre><code>logger.error(\n    \"API error: Rate limit exceeded\",\n    extra={\n        \"request_id\": \"req-abc123\",\n        \"provider\": \"OpenAI\",\n        \"status_code\": 429\n    }\n)\n</code></pre></p>"},{"location":"logging/#retry-logs","title":"Retry Logs","text":"<p>When retries occur, the following is logged:</p> <ul> <li>Retry attempt number</li> <li>Delay before retry</li> <li>Error type</li> <li>Request ID</li> </ul> <p>Example: <pre><code>logger.warning(\n    \"Retry attempt 1/3 after 1.0s: RateLimitError\",\n    extra={\n        \"attempt\": 1,\n        \"delay\": 1.0,\n        \"request_id\": \"req-abc123\",\n        \"status_code\": 429\n    }\n)\n</code></pre></p>"},{"location":"logging/#advanced-usage","title":"Advanced Usage","text":""},{"location":"logging/#manual-logger-setup","title":"Manual Logger Setup","text":"<pre><code>from llm_api_router import setup_logging, LogConfig\n\n# Setup logger with custom configuration\nlogger = setup_logging(LogConfig(\n    level=\"DEBUG\",\n    format=\"json\"\n))\n\nlogger.info(\"Application started\")\n</code></pre>"},{"location":"logging/#get-named-logger","title":"Get Named Logger","text":"<pre><code>from llm_api_router import get_logger\n\n# Get a named logger for a specific module\nlogger = get_logger(\"my_module\")\nlogger.info(\"Custom module log\")\n</code></pre>"},{"location":"logging/#request-id-generation","title":"Request ID Generation","text":"<pre><code>from llm_api_router.logging_config import generate_request_id\n\n# Generate unique request IDs for tracking\nrequest_id = generate_request_id()\nlogger.info(\"Processing request\", extra={\"request_id\": request_id})\n</code></pre>"},{"location":"logging/#custom-sensitive-patterns","title":"Custom Sensitive Patterns","text":"<pre><code>log_config = LogConfig(\n    filter_sensitive=True,\n    sensitive_patterns=[\n        r\"password['\\\"]?\\s*[:=]\\s*['\\\"]?[\\w-]+\",  # Passwords\n        r\"token['\\\"]?\\s*[:=]\\s*['\\\"]?[\\w-]+\",     # Tokens\n        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Emails\n    ]\n)\n</code></pre>"},{"location":"logging/#best-practices","title":"Best Practices","text":""},{"location":"logging/#1-use-appropriate-log-levels","title":"1. Use Appropriate Log Levels","text":"<ul> <li>DEBUG: Detailed diagnostic information</li> <li>INFO: Normal operational messages (requests, responses)</li> <li>WARNING: Unexpected situations (retries, degraded performance)</li> <li>ERROR: Errors that need attention (API failures)</li> <li>CRITICAL: Critical failures (system-wide issues)</li> </ul>"},{"location":"logging/#2-enable-sensitive-data-filtering-in-production","title":"2. Enable Sensitive Data Filtering in Production","text":"<p>Always enable sensitive data filtering in production:</p> <pre><code>log_config = LogConfig(filter_sensitive=True)\n</code></pre>"},{"location":"logging/#3-use-json-format-for-log-aggregation","title":"3. Use JSON Format for Log Aggregation","text":"<p>For centralized logging systems (ELK, Splunk, CloudWatch), use JSON format:</p> <pre><code>log_config = LogConfig(format=\"json\")\n</code></pre>"},{"location":"logging/#4-enable-request-id-tracking","title":"4. Enable Request ID Tracking","text":"<p>Request IDs help correlate logs across multiple operations:</p> <pre><code>log_config = LogConfig(enable_request_id=True)\n</code></pre>"},{"location":"logging/#5-set-appropriate-log-levels","title":"5. Set Appropriate Log Levels","text":"<ul> <li>Development: <code>DEBUG</code> or <code>INFO</code></li> <li>Staging: <code>INFO</code></li> <li>Production: <code>INFO</code> or <code>WARNING</code></li> </ul>"},{"location":"logging/#6-monitor-log-volume","title":"6. Monitor Log Volume","text":"<p>Be mindful of log volume in production. Consider: - Setting level to <code>WARNING</code> for high-traffic applications - Using structured logging for efficient parsing - Setting up log rotation and retention policies</p>"},{"location":"logging/#integration-with-log-aggregation-systems","title":"Integration with Log Aggregation Systems","text":""},{"location":"logging/#elasticsearchlogstashkibana-elk","title":"Elasticsearch/Logstash/Kibana (ELK)","text":"<pre><code># Use JSON format for easy ingestion\nlog_config = LogConfig(\n    format=\"json\",\n    level=\"INFO\"\n)\n</code></pre>"},{"location":"logging/#aws-cloudwatch","title":"AWS CloudWatch","text":"<pre><code># JSON format works well with CloudWatch Insights\nlog_config = LogConfig(\n    format=\"json\",\n    level=\"INFO\"\n)\n</code></pre>"},{"location":"logging/#splunk","title":"Splunk","text":"<pre><code># Structured logs for Splunk\nlog_config = LogConfig(\n    format=\"json\",\n    level=\"INFO\"\n)\n</code></pre>"},{"location":"logging/#troubleshooting","title":"Troubleshooting","text":""},{"location":"logging/#logs-not-appearing","title":"Logs Not Appearing","text":"<ol> <li>Check log level configuration</li> <li>Ensure logging is initialized before making requests</li> <li>Verify that handlers are attached to the logger</li> </ol>"},{"location":"logging/#sensitive-data-not-filtered","title":"Sensitive Data Not Filtered","text":"<ol> <li>Verify <code>filter_sensitive=True</code> in LogConfig</li> <li>Check if your sensitive pattern is included in default patterns</li> <li>Add custom patterns if needed</li> </ol>"},{"location":"logging/#duplicate-logs","title":"Duplicate Logs","text":"<p>This can happen if logging is initialized multiple times. Use the setup function once:</p> <pre><code># Do this once at application startup\nlogger = setup_logging(log_config)\n</code></pre>"},{"location":"logging/#examples","title":"Examples","text":"<p>See <code>examples/logging_example.py</code> for a comprehensive demonstration of all logging features.</p>"},{"location":"logging/#api-reference","title":"API Reference","text":""},{"location":"logging/#logconfig","title":"LogConfig","text":"<pre><code>@dataclass\nclass LogConfig:\n    level: str = \"INFO\"\n    format: str = \"text\"\n    enable_request_id: bool = True\n    filter_sensitive: bool = True\n    log_requests: bool = True\n    log_responses: bool = True\n    log_errors: bool = True\n    sensitive_patterns: list = field(default_factory=lambda: [...])\n</code></pre>"},{"location":"logging/#setup_logging","title":"setup_logging()","text":"<pre><code>def setup_logging(config: Optional[LogConfig] = None) -&gt; logging.Logger:\n    \"\"\"\n    Setup and configure logging for LLM API Router\n\n    Args:\n        config: Logging configuration, uses defaults if not provided\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n</code></pre>"},{"location":"logging/#get_logger","title":"get_logger()","text":"<pre><code>def get_logger(name: Optional[str] = None) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger instance\n\n    Args:\n        name: Logger name, defaults to \"llm_api_router\"\n\n    Returns:\n        Logger instance\n    \"\"\"\n</code></pre>"},{"location":"logging/#generate_request_id","title":"generate_request_id()","text":"<pre><code>def generate_request_id() -&gt; str:\n    \"\"\"Generate a unique request ID\"\"\"\n</code></pre>"},{"location":"metrics/","title":"Performance Metrics and Monitoring","text":"<p>The LLM API Router includes a comprehensive metrics collection system to help you monitor and optimize your LLM API usage. This document explains how to use the metrics features.</p>"},{"location":"metrics/#overview","title":"Overview","text":"<p>The metrics system automatically collects: - Request latency statistics (min, max, avg, p50, p95, p99) - Token usage (prompt, completion, total) - Success/failure rates and error types - Provider performance comparison - Prometheus-compatible export format</p>"},{"location":"metrics/#quick-start","title":"Quick Start","text":"<p>Metrics collection is enabled by default. No configuration needed:</p> <pre><code>from llm_api_router import Client, ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    default_model=\"gpt-3.5-turbo\",\n)\n\nclient = Client(config)\n\n# Make requests\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Get aggregated metrics\nmetrics = client.get_aggregated_metrics()\nfor m in metrics:\n    print(f\"Provider: {m.provider}, Success Rate: {m.success_rate:.2%}\")\n    print(f\"Avg Latency: {m.avg_latency_ms:.2f}ms\")\n</code></pre>"},{"location":"metrics/#configuration","title":"Configuration","text":""},{"location":"metrics/#enabledisable-metrics","title":"Enable/Disable Metrics","text":"<pre><code># Enable metrics (default)\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    metrics_enabled=True,\n)\n\n# Disable metrics\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    metrics_enabled=False,\n)\n</code></pre>"},{"location":"metrics/#custom-metrics-collector","title":"Custom Metrics Collector","text":"<p>Use a shared metrics collector across multiple clients:</p> <pre><code>from llm_api_router.metrics import MetricsCollector\n\n# Create a shared collector\ncollector = MetricsCollector(enabled=True)\n\n# Use it with multiple clients\nopenai_config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"openai-key\",\n    metrics_collector=collector,\n)\n\nanthropic_config = ProviderConfig(\n    provider_type=\"anthropic\",\n    api_key=\"anthropic-key\",\n    metrics_collector=collector,\n)\n\nopenai_client = Client(openai_config)\nanthropic_client = Client(anthropic_config)\n\n# Both clients will record to the same collector\n# This allows cross-provider comparison\n</code></pre>"},{"location":"metrics/#accessing-metrics","title":"Accessing Metrics","text":""},{"location":"metrics/#raw-metrics","title":"Raw Metrics","text":"<p>Get individual request metrics:</p> <pre><code>raw_metrics = client.get_metrics()\n\nfor metric in raw_metrics:\n    print(f\"Request ID: {metric.request_id}\")\n    print(f\"Provider: {metric.provider}\")\n    print(f\"Model: {metric.model}\")\n    print(f\"Latency: {metric.latency_ms}ms\")\n    print(f\"Success: {metric.success}\")\n    print(f\"Tokens: {metric.total_tokens}\")\n</code></pre>"},{"location":"metrics/#aggregated-metrics","title":"Aggregated Metrics","text":"<p>Get aggregated statistics by provider/model:</p> <pre><code>aggregated = client.get_aggregated_metrics()\n\nfor agg in aggregated:\n    print(f\"\\nProvider: {agg.provider} - Model: {agg.model}\")\n    print(f\"Total Requests: {agg.total_requests}\")\n    print(f\"Success Rate: {agg.success_rate:.2%}\")\n\n    # Latency statistics\n    if agg.min_latency_ms is not None:\n        print(f\"Latency - Min: {agg.min_latency_ms:.2f}ms\")\n        print(f\"Latency - Max: {agg.max_latency_ms:.2f}ms\")\n        print(f\"Latency - Avg: {agg.avg_latency_ms:.2f}ms\")\n        print(f\"Latency - P50: {agg.p50_latency_ms:.2f}ms\")\n        print(f\"Latency - P95: {agg.p95_latency_ms:.2f}ms\")\n        print(f\"Latency - P99: {agg.p99_latency_ms:.2f}ms\")\n\n    # Token usage\n    print(f\"Total Tokens: {agg.total_tokens}\")\n    if agg.avg_prompt_tokens is not None:\n        print(f\"Avg Prompt Tokens: {agg.avg_prompt_tokens:.1f}\")\n        print(f\"Avg Completion Tokens: {agg.avg_completion_tokens:.1f}\")\n\n    # Error breakdown\n    if agg.error_counts:\n        print(\"Errors:\")\n        for error_type, count in agg.error_counts.items():\n            print(f\"  {error_type}: {count}\")\n</code></pre>"},{"location":"metrics/#provider-comparison","title":"Provider Comparison","text":"<p>Compare performance across providers:</p> <pre><code>comparison = client.compare_providers()\n\n# Results are sorted by success rate (desc) and latency (asc)\nfor comp in comparison:\n    print(f\"{comp['provider']} ({comp['model']})\")\n    print(f\"  Success Rate: {comp['success_rate']:.2%}\")\n    if comp['avg_latency_ms'] is not None:\n        print(f\"  Avg Latency: {comp['avg_latency_ms']:.2f}ms\")\n        print(f\"  P95 Latency: {comp['p95_latency_ms']:.2f}ms\")\n    print(f\"  Total Tokens: {comp['total_tokens']}\")\n</code></pre>"},{"location":"metrics/#filtered-metrics","title":"Filtered Metrics","text":"<p>Get metrics for specific providers or models:</p> <pre><code># Filter by provider\nopenai_metrics = client.get_metrics(provider=\"openai\")\n\n# Filter by model\ngpt4_metrics = client.get_metrics(model=\"gpt-4\")\n\n# Filter by both\nspecific_metrics = client.get_metrics(provider=\"openai\", model=\"gpt-4\")\n</code></pre>"},{"location":"metrics/#prometheus-export","title":"Prometheus Export","text":"<p>Export metrics in Prometheus text format:</p> <pre><code>prometheus_text = client.export_metrics_prometheus()\nprint(prometheus_text)\n\n# Write to file for Prometheus to scrape\nwith open(\"/var/metrics/llm_router.prom\", \"w\") as f:\n    f.write(prometheus_text)\n</code></pre> <p>The exported metrics include:</p> <ul> <li><code>llm_router_requests_total</code> - Total requests (counter)</li> <li><code>llm_router_requests_success</code> - Successful requests (counter)</li> <li><code>llm_router_requests_failed</code> - Failed requests (counter)</li> <li><code>llm_router_success_rate</code> - Success rate 0-1 (gauge)</li> <li><code>llm_router_latency_ms</code> - Latency quantiles (summary)</li> <li><code>llm_router_tokens_total</code> - Token usage (counter)</li> </ul> <p>All metrics are labeled with: - <code>provider</code> - The LLM provider name - <code>model</code> - The model name</p>"},{"location":"metrics/#prometheus-configuration","title":"Prometheus Configuration","text":"<p>Configure Prometheus to scrape your metrics endpoint:</p> <pre><code>scrape_configs:\n  - job_name: 'llm_api_router'\n    static_configs:\n      - targets: ['localhost:9090']\n    metrics_path: '/metrics'\n    scrape_interval: 15s\n</code></pre>"},{"location":"metrics/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>A pre-built Grafana dashboard is available at <code>examples/grafana_dashboard.json</code>.</p>"},{"location":"metrics/#import-dashboard","title":"Import Dashboard","text":"<ol> <li>Open Grafana</li> <li>Click \"+\" \u2192 \"Import\"</li> <li>Upload <code>examples/grafana_dashboard.json</code></li> <li>Select your Prometheus data source</li> <li>Click \"Import\"</li> </ol>"},{"location":"metrics/#dashboard-panels","title":"Dashboard Panels","text":"<p>The dashboard includes:</p> <ul> <li>Request Rate - Requests per second over time</li> <li>Success Rate - Current success rate gauge</li> <li>Response Latency - P50, P95, P99 latency percentiles</li> <li>Token Usage Rate - Token consumption over time</li> <li>Total Requests/Success/Failed - Cumulative statistics</li> <li>Total Tokens Used - Cumulative token usage</li> </ul>"},{"location":"metrics/#advanced-usage","title":"Advanced Usage","text":""},{"location":"metrics/#direct-metrics-collector-access","title":"Direct Metrics Collector Access","text":"<p>Access the underlying metrics collector:</p> <pre><code>collector = client.get_metrics_collector()\n\n# Get summary\nsummary = collector.get_summary()\nprint(f\"Total Requests: {summary['total_requests']}\")\nprint(f\"Success Rate: {summary['success_rate']:.2%}\")\nprint(f\"Providers: {summary['providers']}\")\n\n# Reset metrics\ncollector.reset()\n</code></pre>"},{"location":"metrics/#custom-recording","title":"Custom Recording","text":"<p>Manually record metrics (advanced use case):</p> <pre><code>from llm_api_router.metrics import get_metrics_collector\n\ncollector = get_metrics_collector()\n\ncollector.record_request(\n    provider=\"custom\",\n    model=\"custom-model\",\n    latency_ms=125.5,\n    success=True,\n    status_code=200,\n    prompt_tokens=100,\n    completion_tokens=50,\n    total_tokens=150,\n)\n</code></pre>"},{"location":"metrics/#metrics-data-structures","title":"Metrics Data Structures","text":""},{"location":"metrics/#requestmetrics","title":"RequestMetrics","text":"<p>Individual request metrics:</p> <pre><code>@dataclass\nclass RequestMetrics:\n    provider: str\n    model: Optional[str]\n    timestamp: datetime\n    latency_ms: float\n    success: bool\n    status_code: Optional[int]\n    error_type: Optional[str]\n    prompt_tokens: Optional[int]\n    completion_tokens: Optional[int]\n    total_tokens: Optional[int]\n    stream: bool\n    request_id: Optional[str]\n</code></pre>"},{"location":"metrics/#aggregatedmetrics","title":"AggregatedMetrics","text":"<p>Aggregated statistics:</p> <pre><code>@dataclass\nclass AggregatedMetrics:\n    provider: str\n    model: Optional[str]\n    total_requests: int\n    successful_requests: int\n    failed_requests: int\n    success_rate: float\n    min_latency_ms: Optional[float]\n    max_latency_ms: Optional[float]\n    avg_latency_ms: Optional[float]\n    p50_latency_ms: Optional[float]\n    p95_latency_ms: Optional[float]\n    p99_latency_ms: Optional[float]\n    total_prompt_tokens: int\n    total_completion_tokens: int\n    total_tokens: int\n    avg_prompt_tokens: Optional[float]\n    avg_completion_tokens: Optional[float]\n    error_counts: Dict[str, int]\n    first_request_time: Optional[datetime]\n    last_request_time: Optional[datetime]\n</code></pre>"},{"location":"metrics/#performance-considerations","title":"Performance Considerations","text":""},{"location":"metrics/#memory-usage","title":"Memory Usage","text":"<p>Metrics are stored in memory. For long-running applications with many requests:</p> <pre><code># Reset metrics periodically\ncollector = client.get_metrics_collector()\nif collector:\n    collector.reset()\n</code></pre> <p>Or disable metrics if not needed:</p> <pre><code>config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-api-key\",\n    metrics_enabled=False,\n)\n</code></pre>"},{"location":"metrics/#thread-safety","title":"Thread Safety","text":"<p>The metrics collector is thread-safe and can be used in multi-threaded environments without additional synchronization.</p>"},{"location":"metrics/#examples","title":"Examples","text":"<p>See <code>examples/metrics_example.py</code> for complete working examples:</p> <pre><code>python examples/metrics_example.py\n</code></pre>"},{"location":"metrics/#best-practices","title":"Best Practices","text":"<ol> <li>Enable metrics in development and staging to understand performance characteristics</li> <li>Use shared collectors when comparing multiple providers</li> <li>Export to Prometheus for production monitoring and alerting</li> <li>Set up Grafana dashboards for visual monitoring</li> <li>Reset metrics periodically in long-running applications to manage memory</li> <li>Monitor success rates and latencies to detect issues early</li> <li>Compare providers to optimize cost and performance</li> </ol>"},{"location":"metrics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metrics/#no-metrics-collected","title":"No metrics collected","text":"<p>Check if metrics are enabled:</p> <pre><code>collector = client.get_metrics_collector()\nif collector:\n    print(f\"Metrics enabled: {collector.enabled}\")\nelse:\n    print(\"Metrics disabled\")\n</code></pre>"},{"location":"metrics/#metrics-not-updating","title":"Metrics not updating","text":"<p>Ensure requests are actually completing:</p> <pre><code>try:\n    response = client.chat.completions.create(messages=messages)\n    print(\"Request successful\")\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n\n# Check metrics\nmetrics = client.get_metrics()\nprint(f\"Total metrics: {len(metrics)}\")\n</code></pre>"},{"location":"metrics/#high-memory-usage","title":"High memory usage","text":"<p>Reset metrics periodically:</p> <pre><code>import time\n\ncollector = client.get_metrics_collector()\n\nwhile True:\n    # Your application logic\n    make_requests()\n\n    # Reset every hour\n    time.sleep(3600)\n    if collector:\n        collector.reset()\n</code></pre>"},{"location":"metrics/#related-documentation","title":"Related Documentation","text":"<ul> <li>Error Handling - Understanding error types in metrics</li> <li>Logging - Complementary logging system</li> <li>Examples - Working code examples</li> </ul>"},{"location":"prompt-templates/","title":"Prompt Templates","text":"<p>LLM API Router \u63d0\u4f9b\u4e86\u7075\u6d3b\u7684 Prompt \u6a21\u677f\u7cfb\u7edf\uff0c\u5e2e\u52a9\u4f60\u521b\u5efa\u7ed3\u6784\u5316\u3001\u53ef\u590d\u7528\u7684\u63d0\u793a\u8bcd\u3002</p>"},{"location":"prompt-templates/#_1","title":"\u5feb\u901f\u5f00\u59cb","text":"<pre><code>from llm_api_router.templates import render_template, render_messages\n\n# \u4f7f\u7528\u5185\u7f6e\u6a21\u677f\nresult = render_template(\"summarize\", text=\"\u8981\u603b\u7ed3\u7684\u957f\u6587\u672c...\")\n\n# \u751f\u6210\u6d88\u606f\u5217\u8868\uff08\u7528\u4e8e LLM API\uff09\nmessages = render_messages(\"translate\", \n    source_language=\"\u4e2d\u6587\",\n    target_language=\"\u82f1\u6587\",\n    text=\"\u4f60\u597d\uff0c\u4e16\u754c\uff01\"\n)\n</code></pre>"},{"location":"prompt-templates/#_2","title":"\u6a21\u677f\u8bed\u6cd5","text":""},{"location":"prompt-templates/#_3","title":"\u53d8\u91cf\u66ff\u6362","text":"<p>\u4f7f\u7528 <code>{{variable}}</code> \u8bed\u6cd5\u63d2\u5165\u53d8\u91cf\uff1a</p> <pre><code>from llm_api_router.templates import TemplateEngine\n\nengine = TemplateEngine()\nresult = engine.render(\"Hello, {{name}}!\", {\"name\": \"World\"})\n# \u8f93\u51fa: Hello, World!\n</code></pre>"},{"location":"prompt-templates/#_4","title":"\u9ed8\u8ba4\u503c","text":"<p>\u4f7f\u7528 <code>|</code> \u6307\u5b9a\u9ed8\u8ba4\u503c\uff1a</p> <pre><code>result = engine.render(\"Language: {{lang|Python}}\", {})\n# \u8f93\u51fa: Language: Python\n\nresult = engine.render(\"Language: {{lang|Python}}\", {\"lang\": \"Go\"})\n# \u8f93\u51fa: Language: Go\n</code></pre>"},{"location":"prompt-templates/#_5","title":"\u6761\u4ef6\u5757","text":"<p>\u4f7f\u7528 <code>{% if %}...{% endif %}</code> \u8fdb\u884c\u6761\u4ef6\u6e32\u67d3\uff1a</p> <pre><code>template = \"\"\"\n{% if show_details %}\n\u8be6\u7ec6\u4fe1\u606f: {{details}}\n{% endif %}\n\"\"\"\n\nresult = engine.render(template, {\"show_details\": True, \"details\": \"...\"})\n</code></pre> <p>\u652f\u6301 else \u5206\u652f\uff1a</p> <pre><code>template = \"{% if is_admin %}\u7ba1\u7406\u5458{% else %}\u7528\u6237{% endif %}\"\n</code></pre>"},{"location":"prompt-templates/#_6","title":"\u5faa\u73af","text":"<p>\u4f7f\u7528 <code>{% for %}...{% endfor %}</code> \u8fdb\u884c\u5faa\u73af\uff1a</p> <pre><code>template = \"\"\"\n\u9879\u76ee\u5217\u8868:\n{% for item in items %}- {{item}}\n{% endfor %}\n\"\"\"\n\nresult = engine.render(template, {\"items\": [\"A\", \"B\", \"C\"]})\n</code></pre>"},{"location":"prompt-templates/#_7","title":"\u8fc7\u6ee4\u5668","text":"<p>\u4f7f\u7528 <code>|filter:</code> \u8bed\u6cd5\u5e94\u7528\u8fc7\u6ee4\u5668\uff1a</p> <pre><code># \u5185\u7f6e\u8fc7\u6ee4\u5668\nengine.render(\"{{name|upper:}}\", {\"name\": \"hello\"})   # HELLO\nengine.render(\"{{name|lower:}}\", {\"name\": \"HELLO\"})   # hello\nengine.render(\"{{name|title:}}\", {\"name\": \"hello\"})   # Hello\n\n# \u81ea\u5b9a\u4e49\u8fc7\u6ee4\u5668\nengine.register_filter(\"reverse\", lambda s: s[::-1])\nengine.render(\"{{text|reverse:}}\", {\"text\": \"hello\"})  # olleh\n</code></pre>"},{"location":"prompt-templates/#_8","title":"\u521b\u5efa\u6a21\u677f","text":""},{"location":"prompt-templates/#_9","title":"\u57fa\u672c\u6a21\u677f","text":"<pre><code>from llm_api_router.templates import PromptTemplate\n\ntemplate = PromptTemplate(\n    name=\"greeting\",\n    template=\"Hello, {{name}}!\",\n    description=\"\u7b80\u5355\u7684\u95ee\u5019\u6a21\u677f\"\n)\n</code></pre>"},{"location":"prompt-templates/#_10","title":"\u5e26\u7cfb\u7edf\u63d0\u793a\u7684\u6a21\u677f","text":"<pre><code>template = PromptTemplate(\n    name=\"qa\",\n    template=\"\u95ee\u9898: {{question}}\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002\u8bf7\u51c6\u786e\u56de\u7b54\u7528\u6237\u7684\u95ee\u9898\u3002\",\n    description=\"\u95ee\u7b54\u6a21\u677f\"\n)\n</code></pre>"},{"location":"prompt-templates/#_11","title":"\u5e26\u9ed8\u8ba4\u503c\u7684\u6a21\u677f","text":"<pre><code>template = PromptTemplate(\n    name=\"translate\",\n    template=\"\u5c06\u4ee5\u4e0b{{source}}\u7ffb\u8bd1\u6210{{target}}: {{text}}\",\n    default_values={\n        \"source\": \"\u4e2d\u6587\",\n        \"target\": \"\u82f1\u6587\"\n    }\n)\n</code></pre>"},{"location":"prompt-templates/#_12","title":"\u4f7f\u7528\u6ce8\u518c\u8868","text":""},{"location":"prompt-templates/#_13","title":"\u6ce8\u518c\u548c\u83b7\u53d6\u6a21\u677f","text":"<pre><code>from llm_api_router.templates import TemplateRegistry\n\nregistry = TemplateRegistry()\n\n# \u6ce8\u518c\u6a21\u677f\nregistry.register(template)\n\n# \u4ece\u5b57\u5178\u6ce8\u518c\nregistry.register_from_dict({\n    \"name\": \"custom\",\n    \"template\": \"{{content}}\",\n    \"description\": \"\u81ea\u5b9a\u4e49\u6a21\u677f\"\n})\n\n# \u83b7\u53d6\u6a21\u677f\ntemplate = registry.get(\"custom\")\n\n# \u5217\u51fa\u6240\u6709\u6a21\u677f\nnames = registry.list_templates()\n</code></pre>"},{"location":"prompt-templates/#_14","title":"\u6e32\u67d3\u6a21\u677f","text":"<pre><code># \u6e32\u67d3\u4e3a\u5b57\u7b26\u4e32\nresult = registry.render(\"greeting\", name=\"World\")\n\n# \u6e32\u67d3\u4e3a\u6d88\u606f\u5217\u8868\nmessages = registry.render_messages(\"qa\", question=\"\u4ec0\u4e48\u662f AI?\")\n# [\n#   {\"role\": \"system\", \"content\": \"\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b...\"},\n#   {\"role\": \"user\", \"content\": \"\u95ee\u9898: \u4ec0\u4e48\u662f AI?\"}\n# ]\n</code></pre>"},{"location":"prompt-templates/#_15","title":"\u5185\u7f6e\u6a21\u677f","text":"<p>LLM API Router \u63d0\u4f9b\u4e86 10+ \u4e2a\u5e38\u7528\u5185\u7f6e\u6a21\u677f\uff1a</p> \u6a21\u677f\u540d \u63cf\u8ff0 \u4e3b\u8981\u53d8\u91cf <code>summarize</code> \u6587\u672c\u6458\u8981 <code>text</code> <code>translate</code> \u7ffb\u8bd1 <code>text</code>, <code>source_language</code>, <code>target_language</code> <code>qa</code> \u95ee\u7b54 <code>question</code>, <code>context</code>(\u53ef\u9009) <code>code_review</code> \u4ee3\u7801\u5ba1\u67e5 <code>code</code>, <code>language</code> <code>explain_code</code> \u4ee3\u7801\u89e3\u91ca <code>code</code>, <code>language</code> <code>rewrite</code> \u91cd\u5199\u6587\u672c <code>text</code>, <code>style</code> <code>grammar_fix</code> \u8bed\u6cd5\u4fee\u6b63 <code>text</code> <code>extract</code> \u4fe1\u606f\u63d0\u53d6 <code>text</code>, <code>extract_type</code> <code>classify</code> \u6587\u672c\u5206\u7c7b <code>text</code>, <code>categories</code> <code>sentiment</code> \u60c5\u611f\u5206\u6790 <code>text</code>"},{"location":"prompt-templates/#_16","title":"\u4f7f\u7528\u5185\u7f6e\u6a21\u677f","text":"<pre><code>from llm_api_router.templates import render_template, render_messages\n\n# \u6458\u8981\nsummary = render_template(\"summarize\", text=\"\u957f\u6587\u672c...\")\n\n# \u7ffb\u8bd1\ntranslation = render_template(\"translate\", \n    text=\"Hello\",\n    source_language=\"\u82f1\u6587\",\n    target_language=\"\u4e2d\u6587\"\n)\n\n# \u4ee3\u7801\u5ba1\u67e5\nmessages = render_messages(\"code_review\",\n    code=\"def foo(): pass\",\n    language=\"python\"\n)\n</code></pre>"},{"location":"prompt-templates/#_17","title":"\u5168\u5c40\u6ce8\u518c\u8868","text":"<pre><code>from llm_api_router.templates import get_template_registry\n\n# \u83b7\u53d6\u5168\u5c40\u6ce8\u518c\u8868\uff08\u5df2\u5305\u542b\u5185\u7f6e\u6a21\u677f\uff09\nregistry = get_template_registry()\n\n# \u6dfb\u52a0\u81ea\u5b9a\u4e49\u6a21\u677f\u5230\u5168\u5c40\u6ce8\u518c\u8868\nregistry.register(my_template)\n</code></pre>"},{"location":"prompt-templates/#client","title":"\u4e0e Client \u96c6\u6210","text":"<pre><code>from llm_api_router import Client\nfrom llm_api_router.templates import render_messages\n\n# \u521b\u5efa\u5ba2\u6237\u7aef\nclient = Client(config)\n\n# \u4f7f\u7528\u6a21\u677f\u751f\u6210\u6d88\u606f\nmessages = render_messages(\"summarize\", text=\"\u8981\u603b\u7ed3\u7684\u6587\u672c...\")\n\n# \u53d1\u9001\u8bf7\u6c42\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=\"gpt-4\"\n)\n</code></pre>"},{"location":"prompt-templates/#_18","title":"\u4e25\u683c\u6a21\u5f0f","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7f3a\u5c11\u7684\u53d8\u91cf\u4f1a\u4fdd\u7559\u539f\u59cb\u5360\u4f4d\u7b26\u3002\u542f\u7528\u4e25\u683c\u6a21\u5f0f\u4f1a\u629b\u51fa\u9519\u8bef\uff1a</p> <pre><code># \u975e\u4e25\u683c\u6a21\u5f0f\uff08\u9ed8\u8ba4\uff09\nresult = registry.render(\"greeting\", strict=False)  # \u4fdd\u7559 {{name}}\n\n# \u4e25\u683c\u6a21\u5f0f\nresult = registry.render(\"greeting\", strict=True)   # \u629b\u51fa ValueError\n</code></pre>"},{"location":"prompt-templates/#_19","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u4f7f\u7528\u63cf\u8ff0\u6027\u53d8\u91cf\u540d\uff1a\u4f7f\u6a21\u677f\u610f\u56fe\u6e05\u6670</li> <li>\u63d0\u4f9b\u5408\u7406\u7684\u9ed8\u8ba4\u503c\uff1a\u51cf\u5c11\u5fc5\u9700\u53c2\u6570\u6570\u91cf</li> <li>\u7f16\u5199\u7cfb\u7edf\u63d0\u793a\uff1a\u4e3a\u6a21\u677f\u63d0\u4f9b\u4e0a\u4e0b\u6587\u548c\u89d2\u8272\u8bbe\u5b9a</li> <li>\u7ec4\u7ec7\u6a21\u677f\u6ce8\u518c\u8868\uff1a\u6309\u7528\u9014\u5206\u7c7b\u7ba1\u7406\u6a21\u677f</li> <li>\u5229\u7528\u5185\u7f6e\u6a21\u677f\uff1a\u4ece\u5185\u7f6e\u6a21\u677f\u5f00\u59cb\uff0c\u6839\u636e\u9700\u8981\u81ea\u5b9a\u4e49</li> </ol>"},{"location":"rate-limiter/","title":"Rate Limiter","text":"<p>LLM API Router \u63d0\u4f9b\u4e86\u5ba2\u6237\u7aef\u901f\u7387\u9650\u5236\u529f\u80fd\uff0c\u5e2e\u52a9\u4f60\u907f\u514d\u8d85\u8fc7 API \u670d\u52a1\u5546\u7684\u901f\u7387\u9650\u5236\u3002</p>"},{"location":"rate-limiter/#_1","title":"\u914d\u7f6e","text":"<pre><code>from llm_api_router.rate_limiter import RateLimiterConfig, RateLimiter\n\nconfig = RateLimiterConfig(\n    enabled=True,                    # \u542f\u7528\u901f\u7387\u9650\u5236\n    backend=\"token_bucket\",          # \u7b97\u6cd5: \"token_bucket\" \u6216 \"sliding_window\"\n    requests_per_minute=60,          # \u6bcf\u5206\u949f\u6700\u5927\u8bf7\u6c42\u6570\n    burst_size=10,                   # \u6700\u5927\u7a81\u53d1\u8bf7\u6c42\u6570\uff08\u4ec5 token_bucket\uff09\n    wait_timeout=30.0,               # \u7b49\u5f85\u8d85\u65f6\u65f6\u95f4\uff08\u79d2\uff09\n)\n\nlimiter = RateLimiter(config)\n</code></pre>"},{"location":"rate-limiter/#_2","title":"\u7b97\u6cd5\u9009\u62e9","text":""},{"location":"rate-limiter/#token-bucket","title":"Token Bucket\uff08\u4ee4\u724c\u6876\uff09","text":"<p>\u9ed8\u8ba4\u7b97\u6cd5\u3002\u5141\u8bb8\u77ed\u65f6\u95f4\u5185\u7684\u7a81\u53d1\u6d41\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u671f\u5e73\u5747\u901f\u7387\u3002</p> <p>\u4f18\u70b9\uff1a - \u5141\u8bb8\u7a81\u53d1\u6d41\u91cf - \u5e73\u6ed1\u7684\u901f\u7387\u9650\u5236 - \u9002\u5408 API \u8c03\u7528\u573a\u666f</p> <p>\u914d\u7f6e\u793a\u4f8b\uff1a <pre><code>config = RateLimiterConfig(\n    enabled=True,\n    backend=\"token_bucket\",\n    requests_per_minute=60,  # \u5e73\u5747\u6bcf\u79d2 1 \u4e2a\u8bf7\u6c42\n    burst_size=10,           # \u5141\u8bb8\u6700\u591a 10 \u4e2a\u7a81\u53d1\u8bf7\u6c42\n)\n</code></pre></p>"},{"location":"rate-limiter/#sliding-window","title":"Sliding Window\uff08\u6ed1\u52a8\u7a97\u53e3\uff09","text":"<p>\u66f4\u4e25\u683c\u7684\u901f\u7387\u9650\u5236\uff0c\u5728\u56fa\u5b9a\u65f6\u95f4\u7a97\u53e3\u5185\u7cbe\u786e\u8ba1\u6570\u8bf7\u6c42\u3002</p> <p>\u4f18\u70b9\uff1a - \u7cbe\u786e\u7684\u8bf7\u6c42\u8ba1\u6570 - \u66f4\u53ef\u9884\u6d4b\u7684\u884c\u4e3a - \u9002\u5408\u4e25\u683c\u9650\u5236\u573a\u666f</p> <p>\u914d\u7f6e\u793a\u4f8b\uff1a <pre><code>config = RateLimiterConfig(\n    enabled=True,\n    backend=\"sliding_window\",\n    requests_per_minute=60,\n)\n</code></pre></p>"},{"location":"rate-limiter/#_3","title":"\u57fa\u672c\u7528\u6cd5","text":""},{"location":"rate-limiter/#_4","title":"\u7acb\u5373\u83b7\u53d6","text":"<pre><code>allowed, wait_time = limiter.acquire()\nif allowed:\n    # \u6267\u884c API \u8c03\u7528\n    pass\nelse:\n    print(f\"\u9700\u8981\u7b49\u5f85 {wait_time:.2f} \u79d2\")\n</code></pre>"},{"location":"rate-limiter/#_5","title":"\u7b49\u5f85\u83b7\u53d6","text":"<pre><code># \u963b\u585e\u76f4\u5230\u83b7\u53d6\u8bb8\u53ef\uff08\u6216\u8d85\u65f6\uff09\nacquired = limiter.wait_and_acquire(timeout=5.0)\nif acquired:\n    # \u6267\u884c API \u8c03\u7528\n    pass\nelse:\n    print(\"\u8d85\u65f6\")\n</code></pre>"},{"location":"rate-limiter/#_6","title":"\u5f02\u6b65\u7b49\u5f85","text":"<pre><code>acquired = await limiter.wait_and_acquire_async(timeout=5.0)\nif acquired:\n    # \u6267\u884c API \u8c03\u7528\n    pass\n</code></pre>"},{"location":"rate-limiter/#_7","title":"\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668","text":"<pre><code>from llm_api_router.rate_limiter import RateLimitContext\n\n# \u540c\u6b65\nwith RateLimitContext(limiter, key=\"openai\") as acquired:\n    if acquired:\n        # \u6267\u884c API \u8c03\u7528\n        pass\n\n# \u5f02\u6b65\nasync with RateLimitContext(limiter, key=\"openai\") as acquired:\n    if acquired:\n        # \u6267\u884c API \u8c03\u7528\n        pass\n</code></pre>"},{"location":"rate-limiter/#provider","title":"\u591a Provider \u9650\u5236","text":"<p>\u4e3a\u4e0d\u540c Provider \u8bbe\u7f6e\u72ec\u7acb\u7684\u901f\u7387\u9650\u5236\uff1a</p> <pre><code># \u4f7f\u7528\u4e0d\u540c\u7684 key \u6765\u5206\u9694\u4e0d\u540c Provider \u7684\u9650\u5236\nlimiter.acquire(key=\"provider:openai\")\nlimiter.acquire(key=\"provider:anthropic\")\nlimiter.acquire(key=\"provider:gemini\")\n</code></pre>"},{"location":"rate-limiter/#_8","title":"\u83b7\u53d6\u7edf\u8ba1\u4fe1\u606f","text":"<pre><code>stats = limiter.get_stats()\nprint(f\"\u603b\u8bf7\u6c42\u6570: {stats['total_requests']}\")\nprint(f\"\u88ab\u62d2\u7edd\u6570: {stats['rejected_requests']}\")\nprint(f\"\u62d2\u7edd\u7387: {stats['rejection_rate']:.1%}\")\n</code></pre>"},{"location":"rate-limiter/#_9","title":"\u8f85\u52a9\u65b9\u6cd5","text":"<pre><code># \u83b7\u53d6\u5269\u4f59\u8bf7\u6c42\u6570\nremaining = limiter.get_remaining(key=\"default\")\n\n# \u91cd\u7f6e\u901f\u7387\u9650\u5236\nlimiter.reset(key=\"default\")\n</code></pre>"},{"location":"rate-limiter/#_10","title":"\u914d\u7f6e\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>enabled</code> bool False \u662f\u5426\u542f\u7528\u901f\u7387\u9650\u5236 <code>backend</code> str \"token_bucket\" \u7b97\u6cd5\u7c7b\u578b <code>requests_per_minute</code> int 60 \u6bcf\u5206\u949f\u6700\u5927\u8bf7\u6c42\u6570 <code>requests_per_day</code> int None \u6bcf\u5929\u6700\u5927\u8bf7\u6c42\u6570\uff08\u53ef\u9009\uff09 <code>tokens_per_minute</code> int None \u6bcf\u5206\u949f\u6700\u5927 token \u6570\uff08\u53ef\u9009\uff09 <code>burst_size</code> int None \u6700\u5927\u7a81\u53d1\u8bf7\u6c42\u6570 <code>wait_timeout</code> float 30.0 \u7b49\u5f85\u8d85\u65f6\u65f6\u95f4\uff08\u79d2\uff09"},{"location":"rate-limiter/#_11","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5</li> <li>API \u8c03\u7528\u573a\u666f\u63a8\u8350 Token Bucket</li> <li> <p>\u9700\u8981\u4e25\u683c\u9650\u5236\u65f6\u4f7f\u7528 Sliding Window</p> </li> <li> <p>\u8bbe\u7f6e\u5408\u7406\u7684\u8d85\u65f6</p> </li> <li>\u907f\u514d\u8fc7\u957f\u7684\u7b49\u5f85\u65f6\u95f4\u963b\u585e\u5e94\u7528</li> <li> <p>\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u8bbe\u7f6e <code>wait_timeout</code></p> </li> <li> <p>\u4f7f\u7528 Provider \u5206\u79bb</p> </li> <li>\u4e3a\u6bcf\u4e2a Provider \u4f7f\u7528\u72ec\u7acb\u7684 key</li> <li> <p>\u53ef\u4ee5\u6839\u636e\u4e0d\u540c Provider \u7684\u9650\u5236\u8c03\u6574\u914d\u7f6e</p> </li> <li> <p>\u76d1\u63a7\u7edf\u8ba1\u6570\u636e</p> </li> <li>\u5b9a\u671f\u68c0\u67e5 <code>get_stats()</code> \u4e86\u89e3\u4f7f\u7528\u60c5\u51b5</li> <li>\u6839\u636e\u62d2\u7edd\u7387\u8c03\u6574\u914d\u7f6e</li> </ol>"},{"location":"api-reference/client/","title":"API \u53c2\u8003","text":"<p>\u672c\u8282\u63d0\u4f9b LLM API Router \u5b8c\u6574\u7684 API \u6587\u6863\u3002</p>"},{"location":"api-reference/client/#_1","title":"\u6838\u5fc3\u6a21\u5757","text":""},{"location":"api-reference/client/#client","title":"Client","text":""},{"location":"api-reference/client/#llm_api_router.Client","title":"<code>llm_api_router.Client</code>","text":"<p>\u540c\u6b65\u5ba2\u6237\u7aef</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>class Client:\n    \"\"\"\u540c\u6b65\u5ba2\u6237\u7aef\"\"\"\n    def __init__(self, config: ProviderConfig):\n        self.config = config\n        self._http_client = self._create_http_client(config)\n        self._provider = self._get_provider(config)\n        self.chat = Chat(self)\n        self.embeddings = Embeddings(self)\n\n        # Initialize cache manager\n        if config.cache_config:\n            self._cache_manager = CacheManager(config.cache_config)\n        else:\n            self._cache_manager = None\n\n        # Initialize logging only if custom config is provided or not yet configured\n        if config.log_config or not logging.getLogger(\"llm_api_router\").handlers:\n            self._logger = setup_logging(config.log_config)\n\n    def _create_http_client(self, config: ProviderConfig) -&gt; httpx.Client:\n        \"\"\"\u521b\u5efa\u914d\u7f6e\u4f18\u5316\u7684HTTP\u5ba2\u6237\u7aef\"\"\"\n        # \u4f7f\u7528\u7ec6\u7c92\u5ea6\u8d85\u65f6\u914d\u7f6e\u6216\u56de\u9000\u5230\u7b80\u5355\u8d85\u65f6\n        if config.timeout_config:\n            timeout = httpx.Timeout(\n                connect=config.timeout_config.connect,\n                read=config.timeout_config.read,\n                write=config.timeout_config.write,\n                pool=config.timeout_config.pool,\n            )\n        else:\n            timeout = config.timeout\n\n        # \u914d\u7f6e\u8fde\u63a5\u6c60\u9650\u5236\n        pool_config = config.connection_pool_config or ConnectionPoolConfig()\n        limits = httpx.Limits(\n            max_connections=pool_config.max_connections,\n            max_keepalive_connections=pool_config.max_keepalive_connections,\n            keepalive_expiry=pool_config.keepalive_expiry,\n        )\n\n        return httpx.Client(\n            timeout=timeout,\n            limits=limits,\n        )\n\n    def _get_provider(self, config: ProviderConfig):\n        return ProviderFactory.get_provider(config)\n\n    def get_metrics_collector(self):\n        \"\"\"\n        Get the metrics collector instance used by this client\n\n        Returns:\n            MetricsCollector instance or None if metrics are disabled\n        \"\"\"\n        if hasattr(self._provider, '_metrics_collector'):\n            return self._provider._metrics_collector\n        return None\n\n    def get_metrics(self, provider: Optional[str] = None, model: Optional[str] = None):\n        \"\"\"\n        Get raw metrics, optionally filtered by provider and/or model\n\n        Args:\n            provider: Filter by provider (optional)\n            model: Filter by model (optional)\n\n        Returns:\n            List of RequestMetrics or empty list if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.get_metrics(provider, model)\n        return []\n\n    def get_aggregated_metrics(self, provider: Optional[str] = None, model: Optional[str] = None):\n        \"\"\"\n        Get aggregated metrics grouped by provider and model\n\n        Args:\n            provider: Filter by provider (optional)\n            model: Filter by model (optional)\n\n        Returns:\n            List of AggregatedMetrics or empty list if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.get_aggregated_metrics(provider, model)\n        return []\n\n    def export_metrics_prometheus(self) -&gt; str:\n        \"\"\"\n        Export metrics in Prometheus text format\n\n        Returns:\n            Prometheus-formatted metrics string or empty string if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.export_prometheus()\n        return \"\"\n\n    def compare_providers(self):\n        \"\"\"\n        Compare performance across providers\n\n        Returns:\n            List of provider comparison data or empty list if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.compare_providers()\n        return []\n\n    def get_cache_stats(self) -&gt; Dict:\n        \"\"\"\n        Get cache statistics\n\n        Returns:\n            Dictionary containing cache statistics\n        \"\"\"\n        if self._cache_manager:\n            return self._cache_manager.get_stats()\n        return {'enabled': False}\n\n    def clear_cache(self):\n        \"\"\"Clear all cached responses\"\"\"\n        if self._cache_manager:\n            self._cache_manager.clear()\n\n    def _reconstruct_response(self, cached_data: Dict) -&gt; UnifiedResponse:\n        \"\"\"Reconstruct UnifiedResponse from cached dictionary\"\"\"\n        from .types import Usage, Message, Choice\n\n        # Reconstruct Usage\n        usage = Usage(**cached_data['usage'])\n\n        # Reconstruct Choices\n        choices = []\n        for choice_data in cached_data['choices']:\n            message = Message(**choice_data['message'])\n            choice = Choice(\n                index=choice_data['index'],\n                message=message,\n                finish_reason=choice_data['finish_reason']\n            )\n            choices.append(choice)\n\n        return UnifiedResponse(\n            id=cached_data['id'],\n            object=cached_data['object'],\n            created=cached_data['created'],\n            model=cached_data['model'],\n            choices=choices,\n            usage=usage\n        )\n\n    def close(self):\n        self._http_client.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.Client.chat","title":"<code>chat = Chat(self)</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/client/#llm_api_router.Client.embeddings","title":"<code>embeddings = Embeddings(self)</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/client/#llm_api_router.Client.close","title":"<code>close()</code>","text":"\u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def close(self):\n    self._http_client.close()\n</code></pre>"},{"location":"api-reference/client/#asyncclient","title":"AsyncClient","text":""},{"location":"api-reference/client/#llm_api_router.AsyncClient","title":"<code>llm_api_router.AsyncClient</code>","text":"<p>\u5f02\u6b65\u5ba2\u6237\u7aef</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>class AsyncClient:\n    \"\"\"\u5f02\u6b65\u5ba2\u6237\u7aef\"\"\"\n    def __init__(self, config: ProviderConfig):\n        self.config = config\n        self._http_client = self._create_http_client(config)\n        self._provider = self._get_provider(config)\n        self.chat = AsyncChat(self)\n        self.embeddings = AsyncEmbeddings(self)\n\n        # Initialize cache manager\n        if config.cache_config:\n            self._cache_manager = CacheManager(config.cache_config)\n        else:\n            self._cache_manager = None\n\n        # Initialize logging only if custom config is provided or not yet configured\n        if config.log_config or not logging.getLogger(\"llm_api_router\").handlers:\n            self._logger = setup_logging(config.log_config)\n\n    def _create_http_client(self, config: ProviderConfig) -&gt; httpx.AsyncClient:\n        \"\"\"\u521b\u5efa\u914d\u7f6e\u4f18\u5316\u7684\u5f02\u6b65HTTP\u5ba2\u6237\u7aef\"\"\"\n        # \u4f7f\u7528\u7ec6\u7c92\u5ea6\u8d85\u65f6\u914d\u7f6e\u6216\u56de\u9000\u5230\u7b80\u5355\u8d85\u65f6\n        if config.timeout_config:\n            timeout = httpx.Timeout(\n                connect=config.timeout_config.connect,\n                read=config.timeout_config.read,\n                write=config.timeout_config.write,\n                pool=config.timeout_config.pool,\n            )\n        else:\n            timeout = config.timeout\n\n        # \u914d\u7f6e\u8fde\u63a5\u6c60\u9650\u5236\n        pool_config = config.connection_pool_config or ConnectionPoolConfig()\n        limits = httpx.Limits(\n            max_connections=pool_config.max_connections,\n            max_keepalive_connections=pool_config.max_keepalive_connections,\n            keepalive_expiry=pool_config.keepalive_expiry,\n        )\n\n        return httpx.AsyncClient(\n            timeout=timeout,\n            limits=limits,\n        )\n\n    def _get_provider(self, config: ProviderConfig):\n        return ProviderFactory.get_provider(config)\n\n    def get_metrics_collector(self):\n        \"\"\"\n        Get the metrics collector instance used by this client\n\n        Returns:\n            MetricsCollector instance or None if metrics are disabled\n        \"\"\"\n        if hasattr(self._provider, '_metrics_collector'):\n            return self._provider._metrics_collector\n        return None\n\n    def get_metrics(self, provider: Optional[str] = None, model: Optional[str] = None):\n        \"\"\"\n        Get raw metrics, optionally filtered by provider and/or model\n\n        Args:\n            provider: Filter by provider (optional)\n            model: Filter by model (optional)\n\n        Returns:\n            List of RequestMetrics or empty list if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.get_metrics(provider, model)\n        return []\n\n    def get_aggregated_metrics(self, provider: Optional[str] = None, model: Optional[str] = None):\n        \"\"\"\n        Get aggregated metrics grouped by provider and model\n\n        Args:\n            provider: Filter by provider (optional)\n            model: Filter by model (optional)\n\n        Returns:\n            List of AggregatedMetrics or empty list if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.get_aggregated_metrics(provider, model)\n        return []\n\n    def export_metrics_prometheus(self) -&gt; str:\n        \"\"\"\n        Export metrics in Prometheus text format\n\n        Returns:\n            Prometheus-formatted metrics string or empty string if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.export_prometheus()\n        return \"\"\n\n    def compare_providers(self):\n        \"\"\"\n        Compare performance across providers\n\n        Returns:\n            List of provider comparison data or empty list if metrics are disabled\n        \"\"\"\n        collector = self.get_metrics_collector()\n        if collector:\n            return collector.compare_providers()\n        return []\n\n    def get_cache_stats(self) -&gt; Dict:\n        \"\"\"\n        Get cache statistics\n\n        Returns:\n            Dictionary containing cache statistics\n        \"\"\"\n        if self._cache_manager:\n            return self._cache_manager.get_stats()\n        return {'enabled': False}\n\n    def clear_cache(self):\n        \"\"\"Clear all cached responses\"\"\"\n        if self._cache_manager:\n            self._cache_manager.clear()\n\n    def _reconstruct_response(self, cached_data: Dict) -&gt; UnifiedResponse:\n        \"\"\"Reconstruct UnifiedResponse from cached dictionary\"\"\"\n        from .types import Usage, Message, Choice\n\n        # Reconstruct Usage\n        usage = Usage(**cached_data['usage'])\n\n        # Reconstruct Choices\n        choices = []\n        for choice_data in cached_data['choices']:\n            message = Message(**choice_data['message'])\n            choice = Choice(\n                index=choice_data['index'],\n                message=message,\n                finish_reason=choice_data['finish_reason']\n            )\n            choices.append(choice)\n\n        return UnifiedResponse(\n            id=cached_data['id'],\n            object=cached_data['object'],\n            created=cached_data['created'],\n            model=cached_data['model'],\n            choices=choices,\n            usage=usage\n        )\n\n    async def close(self):\n        await self._http_client.aclose()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear all cached responses</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear all cached responses\"\"\"\n    if self._cache_manager:\n        self._cache_manager.clear()\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.compare_providers","title":"<code>compare_providers()</code>","text":"<p>Compare performance across providers</p> <p>\u8fd4\u56de\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 <p>List of provider comparison data or empty list if metrics are disabled</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def compare_providers(self):\n    \"\"\"\n    Compare performance across providers\n\n    Returns:\n        List of provider comparison data or empty list if metrics are disabled\n    \"\"\"\n    collector = self.get_metrics_collector()\n    if collector:\n        return collector.compare_providers()\n    return []\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.export_metrics_prometheus","title":"<code>export_metrics_prometheus()</code>","text":"<p>Export metrics in Prometheus text format</p> <p>\u8fd4\u56de\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 <code>str</code> <p>Prometheus-formatted metrics string or empty string if metrics are disabled</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def export_metrics_prometheus(self) -&gt; str:\n    \"\"\"\n    Export metrics in Prometheus text format\n\n    Returns:\n        Prometheus-formatted metrics string or empty string if metrics are disabled\n    \"\"\"\n    collector = self.get_metrics_collector()\n    if collector:\n        return collector.export_prometheus()\n    return \"\"\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.get_aggregated_metrics","title":"<code>get_aggregated_metrics(provider=None, model=None)</code>","text":"<p>Get aggregated metrics grouped by provider and model</p> <p>\u53c2\u6570\uff1a</p> \u540d\u79f0 \u7c7b\u578b \u63cf\u8ff0 \u9ed8\u8ba4 <code>provider</code> <code>Optional[str]</code> <p>Filter by provider (optional)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by model (optional)</p> <code>None</code> <p>\u8fd4\u56de\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 <p>List of AggregatedMetrics or empty list if metrics are disabled</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def get_aggregated_metrics(self, provider: Optional[str] = None, model: Optional[str] = None):\n    \"\"\"\n    Get aggregated metrics grouped by provider and model\n\n    Args:\n        provider: Filter by provider (optional)\n        model: Filter by model (optional)\n\n    Returns:\n        List of AggregatedMetrics or empty list if metrics are disabled\n    \"\"\"\n    collector = self.get_metrics_collector()\n    if collector:\n        return collector.get_aggregated_metrics(provider, model)\n    return []\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.get_cache_stats","title":"<code>get_cache_stats()</code>","text":"<p>Get cache statistics</p> <p>\u8fd4\u56de\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 <code>Dict</code> <p>Dictionary containing cache statistics</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def get_cache_stats(self) -&gt; Dict:\n    \"\"\"\n    Get cache statistics\n\n    Returns:\n        Dictionary containing cache statistics\n    \"\"\"\n    if self._cache_manager:\n        return self._cache_manager.get_stats()\n    return {'enabled': False}\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.get_metrics","title":"<code>get_metrics(provider=None, model=None)</code>","text":"<p>Get raw metrics, optionally filtered by provider and/or model</p> <p>\u53c2\u6570\uff1a</p> \u540d\u79f0 \u7c7b\u578b \u63cf\u8ff0 \u9ed8\u8ba4 <code>provider</code> <code>Optional[str]</code> <p>Filter by provider (optional)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by model (optional)</p> <code>None</code> <p>\u8fd4\u56de\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 <p>List of RequestMetrics or empty list if metrics are disabled</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def get_metrics(self, provider: Optional[str] = None, model: Optional[str] = None):\n    \"\"\"\n    Get raw metrics, optionally filtered by provider and/or model\n\n    Args:\n        provider: Filter by provider (optional)\n        model: Filter by model (optional)\n\n    Returns:\n        List of RequestMetrics or empty list if metrics are disabled\n    \"\"\"\n    collector = self.get_metrics_collector()\n    if collector:\n        return collector.get_metrics(provider, model)\n    return []\n</code></pre>"},{"location":"api-reference/client/#llm_api_router.AsyncClient.get_metrics_collector","title":"<code>get_metrics_collector()</code>","text":"<p>Get the metrics collector instance used by this client</p> <p>\u8fd4\u56de\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 <p>MetricsCollector instance or None if metrics are disabled</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/client.py</code> <pre><code>def get_metrics_collector(self):\n    \"\"\"\n    Get the metrics collector instance used by this client\n\n    Returns:\n        MetricsCollector instance or None if metrics are disabled\n    \"\"\"\n    if hasattr(self._provider, '_metrics_collector'):\n        return self._provider._metrics_collector\n    return None\n</code></pre>"},{"location":"api-reference/client/#providerconfig","title":"ProviderConfig","text":""},{"location":"api-reference/client/#llm_api_router.ProviderConfig","title":"<code>llm_api_router.ProviderConfig</code>  <code>dataclass</code>","text":"<p>\u63d0\u4f9b\u5546\u914d\u7f6e</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src/llm_api_router/types.py</code> <pre><code>@dataclass\nclass ProviderConfig:\n    \"\"\"\u63d0\u4f9b\u5546\u914d\u7f6e\"\"\"\n    provider_type: str\n    api_key: str\n    base_url: Optional[str] = None\n    default_model: Optional[str] = None\n    extra_headers: Dict[str, str] = field(default_factory=dict)\n    api_version: Optional[str] = None  # \u4e3b\u8981\u7528\u4e8e Azure\n    timeout: float = 60.0  # \u7b80\u5355\u8d85\u65f6\u914d\u7f6e\uff08\u79d2\uff09\uff0c\u7528\u4e8e\u5411\u540e\u517c\u5bb9\n    timeout_config: Optional[TimeoutConfig] = None  # \u7ec6\u7c92\u5ea6\u8d85\u65f6\u914d\u7f6e\uff0c\u4f18\u5148\u4e8etimeout\u4f7f\u7528\n    connection_pool_config: Optional[ConnectionPoolConfig] = None  # \u8fde\u63a5\u6c60\u914d\u7f6e\uff0cNone\u8868\u793a\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\n    retry_config: Optional[RetryConfig] = None  # \u91cd\u8bd5\u914d\u7f6e\uff0cNone\u8868\u793a\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\n    log_config: Optional['LogConfig'] = None  # \u65e5\u5fd7\u914d\u7f6e\uff0cNone\u8868\u793a\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\n    metrics_enabled: bool = True  # \u662f\u5426\u542f\u7528\u6027\u80fd\u6307\u6807\u6536\u96c6\n    metrics_collector: Optional['MetricsCollector'] = None  # \u81ea\u5b9a\u4e49metrics\u6536\u96c6\u5668\uff0cNone\u8868\u793a\u4f7f\u7528\u5168\u5c40\u6536\u96c6\u5668\n    cache_config: Optional['CacheConfig'] = None  # \u7f13\u5b58\u914d\u7f6e\uff0cNone\u8868\u793a\u7981\u7528\u7f13\u5b58\n</code></pre>"},{"location":"api-reference/client/#_2","title":"\u5feb\u901f\u53c2\u8003","text":""},{"location":"api-reference/client/#client_1","title":"Client \u7c7b","text":"<pre><code>class Client:\n    \"\"\"\u540c\u6b65 LLM \u5ba2\u6237\u7aef\"\"\"\n\n    def __init__(\n        self,\n        config: ProviderConfig,\n        cache: Optional[Cache] = None,\n    ) -&gt; None:\n        \"\"\"\n        \u521d\u59cb\u5316\u5ba2\u6237\u7aef\u3002\n\n        Args:\n            config: \u63d0\u4f9b\u5546\u914d\u7f6e\n            cache: \u53ef\u9009\u7684\u7f13\u5b58\u5b9e\u4f8b\n        \"\"\"\n\n    def chat(self) -&gt; ChatCompletions:\n        \"\"\"\u8fd4\u56de\u804a\u5929\u5b8c\u6210\u63a5\u53e3\"\"\"\n\n    def embeddings(self) -&gt; Embeddings:\n        \"\"\"\u8fd4\u56de\u5d4c\u5165\u63a5\u53e3\"\"\"\n\n    def close(self) -&gt; None:\n        \"\"\"\u5173\u95ed\u5ba2\u6237\u7aef\u8fde\u63a5\"\"\"\n\n    def __enter__(self) -&gt; \"Client\":\n        \"\"\"\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u5165\u53e3\"\"\"\n\n    def __exit__(self, *args) -&gt; None:\n        \"\"\"\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u9000\u51fa\"\"\"\n</code></pre>"},{"location":"api-reference/client/#chatcompletions","title":"ChatCompletions \u63a5\u53e3","text":"<pre><code>class ChatCompletions:\n    \"\"\"\u804a\u5929\u5b8c\u6210\u63a5\u53e3\"\"\"\n\n    def create(\n        self,\n        messages: List[Dict[str, str]],\n        model: Optional[str] = None,\n        temperature: float = 1.0,\n        max_tokens: Optional[int] = None,\n        stream: bool = False,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n        \"\"\"\n        \u521b\u5efa\u804a\u5929\u5b8c\u6210\u3002\n\n        Args:\n            messages: \u6d88\u606f\u5217\u8868\n            model: \u6a21\u578b\u540d\u79f0\uff08\u8986\u76d6\u9ed8\u8ba4\uff09\n            temperature: \u6e29\u5ea6\u53c2\u6570 (0-2)\n            max_tokens: \u6700\u5927\u751f\u6210 token \u6570\n            stream: \u662f\u5426\u6d41\u5f0f\u8f93\u51fa\n            tools: \u5de5\u5177\u5b9a\u4e49\u5217\u8868\n            tool_choice: \u5de5\u5177\u9009\u62e9\u7b56\u7565\n            **kwargs: \u5176\u4ed6\u53c2\u6570\n\n        Returns:\n            ChatCompletion \u6216\u6d41\u5f0f\u8fed\u4ee3\u5668\n        \"\"\"\n</code></pre>"},{"location":"api-reference/client/#_3","title":"\u54cd\u5e94\u7c7b\u578b","text":"<pre><code>@dataclass\nclass ChatCompletion:\n    \"\"\"\u804a\u5929\u5b8c\u6210\u54cd\u5e94\"\"\"\n    id: str\n    choices: List[Choice]\n    created: int\n    model: str\n    usage: Usage\n\n@dataclass\nclass Choice:\n    \"\"\"\u9009\u62e9\u9879\"\"\"\n    index: int\n    message: Message\n    finish_reason: str\n\n@dataclass\nclass Message:\n    \"\"\"\u6d88\u606f\"\"\"\n    role: str\n    content: Optional[str]\n    tool_calls: Optional[List[ToolCall]]\n\n@dataclass\nclass Usage:\n    \"\"\"\u4f7f\u7528\u7edf\u8ba1\"\"\"\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n</code></pre>"},{"location":"api-reference/client/#_4","title":"\u6d41\u5f0f\u54cd\u5e94","text":"<pre><code>@dataclass\nclass ChatCompletionChunk:\n    \"\"\"\u6d41\u5f0f\u54cd\u5e94\u5757\"\"\"\n    id: str\n    choices: List[ChunkChoice]\n    created: int\n    model: str\n\n@dataclass\nclass ChunkChoice:\n    \"\"\"\u6d41\u5f0f\u9009\u62e9\u9879\"\"\"\n    index: int\n    delta: Delta\n    finish_reason: Optional[str]\n\n@dataclass\nclass Delta:\n    \"\"\"\u589e\u91cf\u5185\u5bb9\"\"\"\n    role: Optional[str]\n    content: Optional[str]\n    tool_calls: Optional[List[ToolCall]]\n</code></pre>"},{"location":"api-reference/client/#_5","title":"\u9ad8\u7ea7\u529f\u80fd","text":""},{"location":"api-reference/client/#ratelimiter","title":"RateLimiter","text":"<pre><code>class RateLimiter:\n    \"\"\"\u901f\u7387\u9650\u5236\u5668\"\"\"\n\n    def __init__(\n        self,\n        requests_per_minute: int = 60,\n        tokens_per_minute: int = 100000,\n        max_concurrent: int = 10,\n    ) -&gt; None:\n        \"\"\"\u521d\u59cb\u5316\u901f\u7387\u9650\u5236\u5668\"\"\"\n\n    async def acquire(self) -&gt; AsyncContextManager:\n        \"\"\"\u83b7\u53d6\u8bf7\u6c42\u8bb8\u53ef\"\"\"\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"\u83b7\u53d6\u7edf\u8ba1\u4fe1\u606f\"\"\"\n</code></pre>"},{"location":"api-reference/client/#loadbalancer","title":"LoadBalancer","text":"<pre><code>class LoadBalancer:\n    \"\"\"\u8d1f\u8f7d\u5747\u8861\u5668\"\"\"\n\n    def __init__(\n        self,\n        configs: List[ProviderConfig],\n        strategy: str = \"round_robin\",\n        weights: Optional[List[float]] = None,\n    ) -&gt; None:\n        \"\"\"\n        \u521d\u59cb\u5316\u8d1f\u8f7d\u5747\u8861\u5668\u3002\n\n        Args:\n            configs: \u63d0\u4f9b\u5546\u914d\u7f6e\u5217\u8868\n            strategy: \u8d1f\u8f7d\u5747\u8861\u7b56\u7565\n            weights: \u6743\u91cd\u5217\u8868\uff08\u4ec5 weighted \u7b56\u7565\uff09\n        \"\"\"\n\n    def chat(self) -&gt; LoadBalancedChat:\n        \"\"\"\u8fd4\u56de\u8d1f\u8f7d\u5747\u8861\u804a\u5929\u63a5\u53e3\"\"\"\n</code></pre>"},{"location":"api-reference/client/#prompttemplate","title":"PromptTemplate","text":"<pre><code>class PromptTemplate:\n    \"\"\"\u63d0\u793a\u6a21\u677f\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        template: str,\n        variables: List[str],\n        default_values: Optional[Dict[str, str]] = None,\n    ) -&gt; None:\n        \"\"\"\u521d\u59cb\u5316\u6a21\u677f\"\"\"\n\n    def render(self, **kwargs) -&gt; str:\n        \"\"\"\u6e32\u67d3\u6a21\u677f\"\"\"\n\n    def validate(self, **kwargs) -&gt; bool:\n        \"\"\"\u9a8c\u8bc1\u53d8\u91cf\"\"\"\n</code></pre>"},{"location":"api-reference/client/#conversationmanager","title":"ConversationManager","text":"<pre><code>class ConversationManager:\n    \"\"\"\u4f1a\u8bdd\u7ba1\u7406\u5668\"\"\"\n\n    def __init__(\n        self,\n        max_history: int = 50,\n        max_tokens: int = 4000,\n        system_message: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\u521d\u59cb\u5316\u4f1a\u8bdd\u7ba1\u7406\u5668\"\"\"\n\n    def add_user_message(self, content: str) -&gt; None:\n        \"\"\"\u6dfb\u52a0\u7528\u6237\u6d88\u606f\"\"\"\n\n    def add_assistant_message(self, content: str) -&gt; None:\n        \"\"\"\u6dfb\u52a0\u52a9\u624b\u6d88\u606f\"\"\"\n\n    def get_messages(self) -&gt; List[Dict[str, str]]:\n        \"\"\"\u83b7\u53d6\u6d88\u606f\u5217\u8868\"\"\"\n\n    def clear(self) -&gt; None:\n        \"\"\"\u6e05\u7a7a\u5386\u53f2\"\"\"\n</code></pre>"},{"location":"api-reference/client/#_6","title":"\u7f13\u5b58\u6a21\u5757","text":""},{"location":"api-reference/client/#memorycache","title":"MemoryCache","text":"<pre><code>class MemoryCache:\n    \"\"\"\u5185\u5b58\u7f13\u5b58\"\"\"\n\n    def __init__(\n        self,\n        max_size: int = 1000,\n        ttl: int = 3600,\n    ) -&gt; None:\n        \"\"\"\u521d\u59cb\u5316\u5185\u5b58\u7f13\u5b58\"\"\"\n\n    def get(self, key: str) -&gt; Optional[Any]:\n        \"\"\"\u83b7\u53d6\u7f13\u5b58\"\"\"\n\n    def set(self, key: str, value: Any) -&gt; None:\n        \"\"\"\u8bbe\u7f6e\u7f13\u5b58\"\"\"\n\n    def clear(self) -&gt; None:\n        \"\"\"\u6e05\u7a7a\u7f13\u5b58\"\"\"\n</code></pre>"},{"location":"api-reference/client/#diskcache","title":"DiskCache","text":"<pre><code>class DiskCache:\n    \"\"\"\u78c1\u76d8\u7f13\u5b58\"\"\"\n\n    def __init__(\n        self,\n        cache_dir: str = \".llm_cache\",\n        max_size_mb: int = 500,\n        ttl: int = 86400,\n    ) -&gt; None:\n        \"\"\"\u521d\u59cb\u5316\u78c1\u76d8\u7f13\u5b58\"\"\"\n</code></pre>"},{"location":"api-reference/client/#_7","title":"\u5f02\u5e38\u7c7b","text":"<pre><code>class LLMRouterError(Exception):\n    \"\"\"\u57fa\u7840\u5f02\u5e38\u7c7b\"\"\"\n\nclass AuthenticationError(LLMRouterError):\n    \"\"\"\u8ba4\u8bc1\u9519\u8bef\"\"\"\n\nclass RateLimitError(LLMRouterError):\n    \"\"\"\u901f\u7387\u9650\u5236\u9519\u8bef\"\"\"\n\nclass InvalidRequestError(LLMRouterError):\n    \"\"\"\u65e0\u6548\u8bf7\u6c42\u9519\u8bef\"\"\"\n\nclass APIError(LLMRouterError):\n    \"\"\"API \u9519\u8bef\"\"\"\n\nclass TimeoutError(LLMRouterError):\n    \"\"\"\u8d85\u65f6\u9519\u8bef\"\"\"\n\nclass ProviderError(LLMRouterError):\n    \"\"\"\u63d0\u4f9b\u5546\u9519\u8bef\"\"\"\n</code></pre>"},{"location":"api-reference/client/#_8","title":"\u5de5\u5382\u51fd\u6570","text":"<pre><code>def create_client(\n    config: ProviderConfig,\n    cache: Optional[Cache] = None,\n) -&gt; Client:\n    \"\"\"\n    \u521b\u5efa\u5ba2\u6237\u7aef\u7684\u5de5\u5382\u51fd\u6570\u3002\n\n    Args:\n        config: \u63d0\u4f9b\u5546\u914d\u7f6e\n        cache: \u53ef\u9009\u7684\u7f13\u5b58\u5b9e\u4f8b\n\n    Returns:\n        \u914d\u7f6e\u597d\u7684 Client \u5b9e\u4f8b\n    \"\"\"\n</code></pre>"},{"location":"api-reference/client/#_9","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<p>\u5b8c\u6574\u7c7b\u578b\u5b9a\u4e49\u53c2\u89c1 types.py\uff1a</p> <pre><code>from llm_api_router.types import (\n    Message,\n    ChatCompletion,\n    ChatCompletionChunk,\n    Choice,\n    Delta,\n    Usage,\n    ToolCall,\n    Tool,\n    EmbeddingResponse,\n)\n</code></pre>"},{"location":"api-reference/exceptions/","title":"\u5f02\u5e38\u53c2\u8003","text":"<p>LLM API Router \u5b9a\u4e49\u4e86\u4e00\u5957\u6e05\u6670\u7684\u5f02\u5e38\u5c42\u6b21\u7ed3\u6784\uff0c\u65b9\u4fbf\u9519\u8bef\u5904\u7406\u3002</p>"},{"location":"api-reference/exceptions/#_2","title":"\u5f02\u5e38\u5c42\u6b21\u7ed3\u6784","text":"<pre><code>LLMRouterError (\u57fa\u7c7b)\n\u251c\u2500\u2500 AuthenticationError    # \u8ba4\u8bc1\u5931\u8d25\n\u251c\u2500\u2500 RateLimitError         # \u901f\u7387\u9650\u5236\n\u251c\u2500\u2500 InvalidRequestError    # \u65e0\u6548\u8bf7\u6c42\n\u251c\u2500\u2500 APIError              # API \u9519\u8bef\n\u251c\u2500\u2500 TimeoutError          # \u8bf7\u6c42\u8d85\u65f6\n\u251c\u2500\u2500 ProviderError         # \u63d0\u4f9b\u5546\u9519\u8bef\n\u2514\u2500\u2500 ConfigurationError    # \u914d\u7f6e\u9519\u8bef\n</code></pre>"},{"location":"api-reference/exceptions/#_3","title":"\u57fa\u7c7b","text":""},{"location":"api-reference/exceptions/#llmroutererror","title":"LLMRouterError","text":"<p>\u6240\u6709\u5f02\u5e38\u7684\u57fa\u7c7b\uff1a</p> <pre><code>class LLMRouterError(Exception):\n    \"\"\"LLM API Router \u57fa\u7840\u5f02\u5e38\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        status_code: Optional[int] = None,\n    ):\n        self.message = message\n        self.provider = provider\n        self.status_code = status_code\n        super().__init__(message)\n</code></pre>"},{"location":"api-reference/exceptions/#_4","title":"\u5177\u4f53\u5f02\u5e38","text":""},{"location":"api-reference/exceptions/#authenticationerror","title":"AuthenticationError","text":"<p>API \u5bc6\u94a5\u65e0\u6548\u6216\u8fc7\u671f\uff1a</p> <pre><code>class AuthenticationError(LLMRouterError):\n    \"\"\"\u8ba4\u8bc1\u5931\u8d25\u5f02\u5e38\"\"\"\n    pass\n</code></pre> <p>\u5e38\u89c1\u539f\u56e0\uff1a</p> <ul> <li>API Key \u65e0\u6548</li> <li>API Key \u5df2\u8fc7\u671f</li> <li>API Key \u6743\u9650\u4e0d\u8db3</li> </ul> <p>\u5904\u7406\u793a\u4f8b\uff1a</p> <pre><code>from llm_api_router.exceptions import AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"\u8ba4\u8bc1\u5931\u8d25: {e.message}\")\n    print(f\"\u63d0\u4f9b\u5546: {e.provider}\")\n    # \u63d0\u793a\u7528\u6237\u68c0\u67e5 API Key\n</code></pre>"},{"location":"api-reference/exceptions/#ratelimiterror","title":"RateLimitError","text":"<p>\u8bf7\u6c42\u8d85\u8fc7\u901f\u7387\u9650\u5236\uff1a</p> <pre><code>class RateLimitError(LLMRouterError):\n    \"\"\"\u901f\u7387\u9650\u5236\u5f02\u5e38\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        retry_after: Optional[float] = None,\n        **kwargs\n    ):\n        super().__init__(message, **kwargs)\n        self.retry_after = retry_after  # \u5efa\u8bae\u7b49\u5f85\u65f6\u95f4\uff08\u79d2\uff09\n</code></pre> <p>\u5904\u7406\u793a\u4f8b\uff1a</p> <pre><code>from llm_api_router.exceptions import RateLimitError\nimport time\n\ntry:\n    response = client.chat.completions.create(...)\nexcept RateLimitError as e:\n    wait_time = e.retry_after or 60\n    print(f\"\u8fbe\u5230\u901f\u7387\u9650\u5236\uff0c{wait_time} \u79d2\u540e\u91cd\u8bd5\")\n    time.sleep(wait_time)\n    # \u91cd\u8bd5\u8bf7\u6c42\n</code></pre>"},{"location":"api-reference/exceptions/#invalidrequesterror","title":"InvalidRequestError","text":"<p>\u8bf7\u6c42\u53c2\u6570\u65e0\u6548\uff1a</p> <pre><code>class InvalidRequestError(LLMRouterError):\n    \"\"\"\u65e0\u6548\u8bf7\u6c42\u5f02\u5e38\"\"\"\n    pass\n</code></pre> <p>\u5e38\u89c1\u539f\u56e0\uff1a</p> <ul> <li>\u6a21\u578b\u540d\u79f0\u9519\u8bef</li> <li>\u53c2\u6570\u683c\u5f0f\u9519\u8bef</li> <li>\u6d88\u606f\u5185\u5bb9\u4e3a\u7a7a</li> </ul>"},{"location":"api-reference/exceptions/#apierror","title":"APIError","text":"<p>API \u8c03\u7528\u5931\u8d25\uff1a</p> <pre><code>class APIError(LLMRouterError):\n    \"\"\"API \u9519\u8bef\u5f02\u5e38\"\"\"\n    pass\n</code></pre> <p>\u5e38\u89c1\u539f\u56e0\uff1a</p> <ul> <li>\u670d\u52a1\u6682\u65f6\u4e0d\u53ef\u7528</li> <li>\u63d0\u4f9b\u5546\u670d\u52a1\u5668\u9519\u8bef</li> <li>\u7f51\u7edc\u95ee\u9898</li> </ul>"},{"location":"api-reference/exceptions/#timeouterror","title":"TimeoutError","text":"<p>\u8bf7\u6c42\u8d85\u65f6\uff1a</p> <pre><code>class TimeoutError(LLMRouterError):\n    \"\"\"\u8d85\u65f6\u5f02\u5e38\"\"\"\n    pass\n</code></pre> <p>\u5904\u7406\u793a\u4f8b\uff1a</p> <pre><code>from llm_api_router.exceptions import TimeoutError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept TimeoutError as e:\n    print(f\"\u8bf7\u6c42\u8d85\u65f6: {e.message}\")\n    # \u53ef\u4ee5\u589e\u52a0 timeout \u53c2\u6570\u91cd\u8bd5\n</code></pre>"},{"location":"api-reference/exceptions/#providererror","title":"ProviderError","text":"<p>\u63d0\u4f9b\u5546\u7279\u5b9a\u9519\u8bef\uff1a</p> <pre><code>class ProviderError(LLMRouterError):\n    \"\"\"\u63d0\u4f9b\u5546\u9519\u8bef\u5f02\u5e38\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#configurationerror","title":"ConfigurationError","text":"<p>\u914d\u7f6e\u9519\u8bef\uff1a</p> <pre><code>class ConfigurationError(LLMRouterError):\n    \"\"\"\u914d\u7f6e\u9519\u8bef\u5f02\u5e38\"\"\"\n    pass\n</code></pre> <p>\u5e38\u89c1\u539f\u56e0\uff1a</p> <ul> <li>\u7f3a\u5c11\u5fc5\u9700\u7684\u914d\u7f6e\u9879</li> <li>\u914d\u7f6e\u503c\u7c7b\u578b\u9519\u8bef</li> <li>\u4e0d\u652f\u6301\u7684\u63d0\u4f9b\u5546\u7c7b\u578b</li> </ul>"},{"location":"api-reference/exceptions/#_5","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"api-reference/exceptions/#_6","title":"\u6355\u83b7\u7279\u5b9a\u5f02\u5e38","text":"<pre><code>from llm_api_router.exceptions import (\n    LLMRouterError,\n    AuthenticationError,\n    RateLimitError,\n    TimeoutError,\n)\n\ntry:\n    response = client.chat.completions.create(...)\n\nexcept AuthenticationError as e:\n    # \u5904\u7406\u8ba4\u8bc1\u9519\u8bef\n    log.error(f\"API Key \u65e0\u6548: {e}\")\n    raise\n\nexcept RateLimitError as e:\n    # \u5904\u7406\u901f\u7387\u9650\u5236\n    time.sleep(e.retry_after or 60)\n    # \u91cd\u8bd5...\n\nexcept TimeoutError as e:\n    # \u5904\u7406\u8d85\u65f6\n    log.warning(f\"\u8bf7\u6c42\u8d85\u65f6: {e}\")\n    # \u91cd\u8bd5\u6216\u964d\u7ea7...\n\nexcept LLMRouterError as e:\n    # \u5904\u7406\u5176\u4ed6 LLM Router \u9519\u8bef\n    log.error(f\"LLM \u8c03\u7528\u5931\u8d25: {e}\")\n    raise\n</code></pre>"},{"location":"api-reference/exceptions/#_7","title":"\u4e0e\u91cd\u8bd5\u673a\u5236\u7ed3\u5408","text":"<pre><code>from llm_api_router.retry import RetryConfig\n\n# \u914d\u7f6e\u81ea\u52a8\u91cd\u8bd5\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",\n    max_retries=3,  # \u81ea\u52a8\u91cd\u8bd5 3 \u6b21\n)\n</code></pre>"},{"location":"api-reference/exceptions/#_8","title":"\u65e5\u5fd7\u8bb0\u5f55","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    response = client.chat.completions.create(...)\nexcept LLMRouterError as e:\n    logger.exception(\n        \"LLM \u8c03\u7528\u5931\u8d25\",\n        extra={\n            \"provider\": e.provider,\n            \"status_code\": e.status_code,\n        }\n    )\n</code></pre>"},{"location":"api-reference/types/","title":"\u7c7b\u578b\u53c2\u8003","text":"<p>LLM API Router \u4f7f\u7528\u7edf\u4e00\u7684\u7c7b\u578b\u5b9a\u4e49\uff0c\u786e\u4fdd\u8de8\u63d0\u4f9b\u5546\u7684\u4e00\u81f4\u6027\u3002</p>"},{"location":"api-reference/types/#_2","title":"\u6d88\u606f\u7c7b\u578b","text":""},{"location":"api-reference/types/#message","title":"Message","text":"<p>\u8868\u793a\u5bf9\u8bdd\u4e2d\u7684\u5355\u6761\u6d88\u606f\uff1a</p> <pre><code>@dataclass\nclass Message:\n    \"\"\"\u804a\u5929\u6d88\u606f\"\"\"\n    role: str           # \"system\" | \"user\" | \"assistant\" | \"tool\"\n    content: str        # \u6d88\u606f\u5185\u5bb9\n    name: Optional[str] = None  # \u6d88\u606f\u540d\u79f0\uff08\u53ef\u9009\uff09\n    tool_calls: Optional[List[ToolCall]] = None  # \u5de5\u5177\u8c03\u7528\n    tool_call_id: Optional[str] = None  # \u5de5\u5177\u8c03\u7528 ID\n</code></pre>"},{"location":"api-reference/types/#messagerole","title":"MessageRole","text":"<p>\u652f\u6301\u7684\u6d88\u606f\u89d2\u8272\uff1a</p> <ul> <li><code>system</code> - \u7cfb\u7edf\u6307\u4ee4</li> <li><code>user</code> - \u7528\u6237\u8f93\u5165</li> <li><code>assistant</code> - \u52a9\u624b\u54cd\u5e94</li> <li><code>tool</code> - \u5de5\u5177\u54cd\u5e94</li> </ul>"},{"location":"api-reference/types/#_3","title":"\u54cd\u5e94\u7c7b\u578b","text":""},{"location":"api-reference/types/#chatcompletion","title":"ChatCompletion","text":"<p>\u804a\u5929\u5b8c\u6210\u54cd\u5e94\uff1a</p> <pre><code>@dataclass\nclass ChatCompletion:\n    \"\"\"\u804a\u5929\u5b8c\u6210\u54cd\u5e94\"\"\"\n    id: str              # \u54cd\u5e94 ID\n    choices: List[Choice]  # \u9009\u62e9\u5217\u8868\n    created: int         # \u521b\u5efa\u65f6\u95f4\u6233\n    model: str           # \u4f7f\u7528\u7684\u6a21\u578b\n    usage: Usage         # Token \u4f7f\u7528\u7edf\u8ba1\n    system_fingerprint: Optional[str] = None\n</code></pre>"},{"location":"api-reference/types/#choice","title":"Choice","text":"<p>\u54cd\u5e94\u9009\u62e9\u9879\uff1a</p> <pre><code>@dataclass\nclass Choice:\n    \"\"\"\u804a\u5929\u5b8c\u6210\u9009\u62e9\"\"\"\n    index: int           # \u9009\u62e9\u7d22\u5f15\n    message: Message     # \u54cd\u5e94\u6d88\u606f\n    finish_reason: str   # \u5b8c\u6210\u539f\u56e0: \"stop\" | \"length\" | \"tool_calls\"\n</code></pre>"},{"location":"api-reference/types/#usage","title":"Usage","text":"<p>Token \u4f7f\u7528\u7edf\u8ba1\uff1a</p> <pre><code>@dataclass\nclass Usage:\n    \"\"\"Token \u4f7f\u7528\u7edf\u8ba1\"\"\"\n    prompt_tokens: int       # \u8f93\u5165 token \u6570\n    completion_tokens: int   # \u8f93\u51fa token \u6570\n    total_tokens: int        # \u603b token \u6570\n</code></pre>"},{"location":"api-reference/types/#_4","title":"\u6d41\u5f0f\u54cd\u5e94\u7c7b\u578b","text":""},{"location":"api-reference/types/#chatcompletionchunk","title":"ChatCompletionChunk","text":"<p>\u6d41\u5f0f\u54cd\u5e94\u5757\uff1a</p> <pre><code>@dataclass\nclass ChatCompletionChunk:\n    \"\"\"\u6d41\u5f0f\u54cd\u5e94\u5757\"\"\"\n    id: str              # \u54cd\u5e94 ID\n    choices: List[ChunkChoice]  # \u9009\u62e9\u5217\u8868\n    created: int         # \u521b\u5efa\u65f6\u95f4\u6233\n    model: str           # \u4f7f\u7528\u7684\u6a21\u578b\n</code></pre>"},{"location":"api-reference/types/#chunkchoice","title":"ChunkChoice","text":"<p>\u6d41\u5f0f\u9009\u62e9\u9879\uff1a</p> <pre><code>@dataclass\nclass ChunkChoice:\n    \"\"\"\u6d41\u5f0f\u9009\u62e9\"\"\"\n    index: int                     # \u9009\u62e9\u7d22\u5f15\n    delta: Delta                   # \u589e\u91cf\u5185\u5bb9\n    finish_reason: Optional[str]   # \u5b8c\u6210\u539f\u56e0\n</code></pre>"},{"location":"api-reference/types/#delta","title":"Delta","text":"<p>\u589e\u91cf\u5185\u5bb9\uff1a</p> <pre><code>@dataclass\nclass Delta:\n    \"\"\"\u589e\u91cf\u5185\u5bb9\"\"\"\n    role: Optional[str] = None     # \u89d2\u8272\uff08\u901a\u5e38\u4ec5\u9996\u5757\u5305\u542b\uff09\n    content: Optional[str] = None  # \u5185\u5bb9\u7247\u6bb5\n    tool_calls: Optional[List[ToolCall]] = None\n</code></pre>"},{"location":"api-reference/types/#_5","title":"\u5de5\u5177\u7c7b\u578b","text":""},{"location":"api-reference/types/#tool","title":"Tool","text":"<p>\u5de5\u5177\u5b9a\u4e49\uff1a</p> <pre><code>@dataclass\nclass Tool:\n    \"\"\"\u5de5\u5177\u5b9a\u4e49\"\"\"\n    type: str = \"function\"  # \u5de5\u5177\u7c7b\u578b\n    function: FunctionDef   # \u51fd\u6570\u5b9a\u4e49\n</code></pre>"},{"location":"api-reference/types/#functiondef","title":"FunctionDef","text":"<p>\u51fd\u6570\u5b9a\u4e49\uff1a</p> <pre><code>@dataclass\nclass FunctionDef:\n    \"\"\"\u51fd\u6570\u5b9a\u4e49\"\"\"\n    name: str            # \u51fd\u6570\u540d\u79f0\n    description: str     # \u51fd\u6570\u63cf\u8ff0\n    parameters: dict     # JSON Schema \u683c\u5f0f\u7684\u53c2\u6570\u5b9a\u4e49\n</code></pre>"},{"location":"api-reference/types/#toolcall","title":"ToolCall","text":"<p>\u5de5\u5177\u8c03\u7528\uff1a</p> <pre><code>@dataclass\nclass ToolCall:\n    \"\"\"\u5de5\u5177\u8c03\u7528\"\"\"\n    id: str              # \u8c03\u7528 ID\n    type: str            # \u7c7b\u578b\uff08\u901a\u5e38\u4e3a \"function\"\uff09\n    function: FunctionCall  # \u51fd\u6570\u8c03\u7528\u8be6\u60c5\n</code></pre>"},{"location":"api-reference/types/#functioncall","title":"FunctionCall","text":"<p>\u51fd\u6570\u8c03\u7528\u8be6\u60c5\uff1a</p> <pre><code>@dataclass\nclass FunctionCall:\n    \"\"\"\u51fd\u6570\u8c03\u7528\"\"\"\n    name: str            # \u51fd\u6570\u540d\u79f0\n    arguments: str       # JSON \u683c\u5f0f\u7684\u53c2\u6570\u5b57\u7b26\u4e32\n</code></pre>"},{"location":"api-reference/types/#_6","title":"\u5d4c\u5165\u7c7b\u578b","text":""},{"location":"api-reference/types/#embeddingresponse","title":"EmbeddingResponse","text":"<p>\u5d4c\u5165\u54cd\u5e94\uff1a</p> <pre><code>@dataclass\nclass EmbeddingResponse:\n    \"\"\"\u5d4c\u5165\u54cd\u5e94\"\"\"\n    data: List[EmbeddingData]  # \u5d4c\u5165\u6570\u636e\u5217\u8868\n    model: str                 # \u4f7f\u7528\u7684\u6a21\u578b\n    usage: EmbeddingUsage      # \u4f7f\u7528\u7edf\u8ba1\n</code></pre>"},{"location":"api-reference/types/#embeddingdata","title":"EmbeddingData","text":"<p>\u5355\u4e2a\u5d4c\u5165\u6570\u636e\uff1a</p> <pre><code>@dataclass\nclass EmbeddingData:\n    \"\"\"\u5d4c\u5165\u6570\u636e\"\"\"\n    index: int           # \u7d22\u5f15\n    embedding: List[float]  # \u5d4c\u5165\u5411\u91cf\n    object: str = \"embedding\"\n</code></pre>"},{"location":"api-reference/types/#_7","title":"\u914d\u7f6e\u7c7b\u578b","text":""},{"location":"api-reference/types/#providerconfig","title":"ProviderConfig","text":"<p>\u63d0\u4f9b\u5546\u914d\u7f6e\uff1a</p> <pre><code>@dataclass\nclass ProviderConfig:\n    \"\"\"\u63d0\u4f9b\u5546\u914d\u7f6e\"\"\"\n    provider_type: str           # \u63d0\u4f9b\u5546\u7c7b\u578b\n    api_key: str                 # API \u5bc6\u94a5\n    default_model: Optional[str] = None  # \u9ed8\u8ba4\u6a21\u578b\n    base_url: Optional[str] = None      # \u81ea\u5b9a\u4e49 API \u5730\u5740\n    timeout: float = 30.0        # \u8bf7\u6c42\u8d85\u65f6\uff08\u79d2\uff09\n    max_retries: int = 3         # \u6700\u5927\u91cd\u8bd5\u6b21\u6570\n</code></pre>"},{"location":"api-reference/types/#_8","title":"\u7c7b\u578b\u63d0\u793a","text":"<p>LLM API Router \u5b8c\u5168\u652f\u6301\u7c7b\u578b\u63d0\u793a\uff0c\u53ef\u4ee5\u4e0e IDE \u548c\u7c7b\u578b\u68c0\u67e5\u5668\uff08\u5982 mypy\uff09\u914d\u5408\u4f7f\u7528\uff1a</p> <pre><code>from llm_api_router import Client, ProviderConfig\nfrom llm_api_router.types import ChatCompletion, Message\n\ndef chat_with_llm(client: Client, prompt: str) -&gt; str:\n    response: ChatCompletion = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    message: Message = response.choices[0].message\n    return message.content or \"\"\n</code></pre>"},{"location":"getting-started/installation/","title":"\u5b89\u88c5\u6307\u5357","text":""},{"location":"getting-started/installation/#_2","title":"\u7cfb\u7edf\u8981\u6c42","text":"<ul> <li>Python 3.10 \u6216\u66f4\u9ad8\u7248\u672c</li> <li>pip \u6216 uv \u5305\u7ba1\u7406\u5668</li> </ul>"},{"location":"getting-started/installation/#_3","title":"\u57fa\u7840\u5b89\u88c5","text":"<p>\u4f7f\u7528 pip \u5b89\u88c5\uff1a</p> <pre><code>pip install llm-api-router\n</code></pre> <p>\u4f7f\u7528 uv \u5b89\u88c5\uff1a</p> <pre><code>uv pip install llm-api-router\n</code></pre>"},{"location":"getting-started/installation/#_4","title":"\u53ef\u9009\u4f9d\u8d56","text":""},{"location":"getting-started/installation/#cli","title":"CLI \u5de5\u5177","text":"<p>\u5b89\u88c5\u547d\u4ee4\u884c\u5de5\u5177\u4f9d\u8d56\uff1a</p> <pre><code>pip install llm-api-router[cli]\n</code></pre> <p>\u5b89\u88c5\u540e\u53ef\u4f7f\u7528 <code>llm-router</code> \u547d\u4ee4\uff1a</p> <pre><code>llm-router --help\nllm-router test openai --api-key sk-xxx\nllm-router chat anthropic\n</code></pre>"},{"location":"getting-started/installation/#_5","title":"\u6587\u6863\u6784\u5efa","text":"<p>\u5982\u679c\u9700\u8981\u672c\u5730\u6784\u5efa\u6587\u6863\uff1a</p> <pre><code>pip install llm-api-router[docs]\n</code></pre>"},{"location":"getting-started/installation/#_6","title":"\u5f00\u53d1\u73af\u5883","text":"<p>\u5b89\u88c5\u6240\u6709\u5f00\u53d1\u4f9d\u8d56\uff1a</p> <pre><code>pip install llm-api-router[dev]\n</code></pre>"},{"location":"getting-started/installation/#_7","title":"\u5b8c\u6574\u5b89\u88c5","text":"<p>\u5b89\u88c5\u6240\u6709\u53ef\u9009\u4f9d\u8d56\uff1a</p> <pre><code>pip install llm-api-router[all]\n</code></pre>"},{"location":"getting-started/installation/#_8","title":"\u4ece\u6e90\u7801\u5b89\u88c5","text":"<pre><code>git clone https://github.com/Mikko-ww/llm-api-router.git\ncd llm-api-router\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#_9","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<pre><code>&gt;&gt;&gt; import llm_api_router\n&gt;&gt;&gt; print(llm_api_router.__version__)\n0.1.2\n</code></pre> <p>\u6216\u4f7f\u7528 CLI\uff1a</p> <pre><code>llm-router --version\n</code></pre>"},{"location":"getting-started/installation/#_10","title":"\u4f9d\u8d56\u8bf4\u660e","text":"<p>\u6838\u5fc3\u4f9d\u8d56\uff08\u81ea\u52a8\u5b89\u88c5\uff09\uff1a</p> \u5305 \u7528\u9014 <code>httpx</code> HTTP \u5ba2\u6237\u7aef <code>python-dotenv</code> \u73af\u5883\u53d8\u91cf\u7ba1\u7406 <p>\u53ef\u9009\u4f9d\u8d56\uff1a</p> \u5305 \u7528\u9014 \u5b89\u88c5\u65b9\u5f0f <code>typer</code> CLI \u6846\u67b6 <code>[cli]</code> <code>rich</code> \u7ec8\u7aef\u7f8e\u5316 <code>[cli]</code> <code>mkdocs-material</code> \u6587\u6863\u4e3b\u9898 <code>[docs]</code> <code>redis</code> Redis \u7f13\u5b58\u540e\u7aef \u624b\u52a8\u5b89\u88c5 <code>tiktoken</code> Token \u8ba1\u6570 \u624b\u52a8\u5b89\u88c5"},{"location":"getting-started/installation/#_11","title":"\u5e38\u89c1\u95ee\u9898","text":""},{"location":"getting-started/installation/#_12","title":"\u5b89\u88c5\u5931\u8d25\uff1a\u6ca1\u6709\u627e\u5230\u5339\u914d\u7684\u7248\u672c","text":"<p>\u786e\u4fdd Python \u7248\u672c &gt;= 3.10\uff1a</p> <pre><code>python --version\n</code></pre>"},{"location":"getting-started/installation/#importerror-no-module-named-llm_api_router","title":"ImportError: No module named 'llm_api_router'","text":"<p>\u786e\u4fdd\u5728\u6b63\u786e\u7684 Python \u73af\u5883\u4e2d\u5b89\u88c5\uff0c\u6216\u68c0\u67e5\u865a\u62df\u73af\u5883\u662f\u5426\u6fc0\u6d3b\u3002</p>"},{"location":"getting-started/installation/#cli_1","title":"CLI \u547d\u4ee4\u672a\u627e\u5230","text":"<p>\u786e\u4fdd\u5b89\u88c5\u4e86 CLI \u4f9d\u8d56\uff0c\u5e76\u4e14 Python bin \u76ee\u5f55\u5728 PATH \u4e2d\uff1a</p> <pre><code>pip install llm-api-router[cli]\n</code></pre>"},{"location":"getting-started/quickstart/","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u672c\u6307\u5357\u5e2e\u52a9\u4f60\u5728 5 \u5206\u949f\u5185\u5f00\u59cb\u4f7f\u7528 LLM API Router\u3002</p>"},{"location":"getting-started/quickstart/#_2","title":"\u7b2c\u4e00\u6b65\uff1a\u5b89\u88c5","text":"<pre><code>pip install llm-api-router\n</code></pre>"},{"location":"getting-started/quickstart/#api-key","title":"\u7b2c\u4e8c\u6b65\uff1a\u83b7\u53d6 API Key","text":"<p>\u4ece\u4f60\u7684 LLM \u63d0\u4f9b\u5546\u83b7\u53d6 API Key\uff1a</p> <ul> <li>OpenAI: platform.openai.com/api-keys</li> <li>Anthropic: console.anthropic.com</li> <li>Google Gemini: aistudio.google.com</li> </ul>"},{"location":"getting-started/quickstart/#_3","title":"\u7b2c\u4e09\u6b65\uff1a\u53d1\u9001\u7b2c\u4e00\u4e2a\u8bf7\u6c42","text":"<pre><code>from llm_api_router import Client, ProviderConfig\n\n# \u914d\u7f6e\u63d0\u4f9b\u5546\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-your-api-key\",  # \u66ff\u6362\u4e3a\u4f60\u7684 API Key\n    default_model=\"gpt-3.5-turbo\"\n)\n\n# \u521b\u5efa\u5ba2\u6237\u7aef\u5e76\u53d1\u9001\u8bf7\u6c42\nwith Client(config) as client:\n    response = client.chat.completions.create(\n        messages=[\n            {\"role\": \"user\", \"content\": \"\u7528\u4e00\u53e5\u8bdd\u4ecb\u7ecd\u81ea\u5df1\"}\n        ]\n    )\n    print(response.choices[0].message.content)\n</code></pre>"},{"location":"getting-started/quickstart/#_4","title":"\u4f7f\u7528\u73af\u5883\u53d8\u91cf","text":"<p>\u63a8\u8350\u5c06 API Key \u5b58\u50a8\u5728\u73af\u5883\u53d8\u91cf\u4e2d\uff1a</p> <pre><code>export OPENAI_API_KEY=\"sk-your-api-key\"\n</code></pre> <pre><code>import os\nfrom llm_api_router import Client, ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    default_model=\"gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#_5","title":"\u6d41\u5f0f\u54cd\u5e94","text":"<p>\u83b7\u53d6\u5b9e\u65f6\u6d41\u5f0f\u8f93\u51fa\uff1a</p> <pre><code>with Client(config) as client:\n    stream = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"\u5199\u4e00\u9996\u5173\u4e8e\u6625\u5929\u7684\u8bd7\"}],\n        stream=True\n    )\n\n    for chunk in stream:\n        content = chunk.choices[0].delta.content\n        if content:\n            print(content, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started/quickstart/#_6","title":"\u5f02\u6b65\u8c03\u7528","text":"<p>\u4f7f\u7528 <code>AsyncClient</code> \u8fdb\u884c\u5f02\u6b65\u64cd\u4f5c\uff1a</p> <pre><code>import asyncio\nfrom llm_api_router import AsyncClient, ProviderConfig\n\nasync def main():\n    config = ProviderConfig(\n        provider_type=\"openai\",\n        api_key=\"sk-xxx\",\n        default_model=\"gpt-3.5-turbo\"\n    )\n\n    async with AsyncClient(config) as client:\n        response = await client.chat.completions.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n        )\n        print(response.choices[0].message.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#_7","title":"\u5207\u6362\u63d0\u4f9b\u5546","text":"<p>\u53ea\u9700\u66f4\u6539\u914d\u7f6e\u5373\u53ef\u5207\u6362\u5230\u4e0d\u540c\u7684\u63d0\u4f9b\u5546\uff1a</p> OpenAIAnthropicGoogle Gemini\u672c\u5730 Ollama <pre><code>config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",\n    default_model=\"gpt-4o\"\n)\n</code></pre> <pre><code>config = ProviderConfig(\n    provider_type=\"anthropic\",\n    api_key=\"sk-ant-xxx\",\n    default_model=\"claude-3-5-sonnet-20241022\"\n)\n</code></pre> <pre><code>config = ProviderConfig(\n    provider_type=\"gemini\",\n    api_key=\"AIza-xxx\",\n    default_model=\"gemini-1.5-flash\"\n)\n</code></pre> <pre><code>config = ProviderConfig(\n    provider_type=\"ollama\",\n    api_key=\"not-required\",\n    base_url=\"http://localhost:11434\",\n    default_model=\"llama3.2\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#cli","title":"\u4f7f\u7528 CLI \u5feb\u901f\u6d4b\u8bd5","text":"<p>\u5b89\u88c5 CLI \u5de5\u5177\u540e\uff0c\u53ef\u4ee5\u5feb\u901f\u6d4b\u8bd5\u8fde\u63a5\uff1a</p> <pre><code># \u5b89\u88c5 CLI\npip install llm-api-router[cli]\n\n# \u6d4b\u8bd5\u8fde\u63a5\nllm-router test openai --api-key sk-xxx\n\n# \u4ea4\u4e92\u5f0f\u804a\u5929\nllm-router chat openai --api-key sk-xxx\n</code></pre>"},{"location":"getting-started/quickstart/#_8","title":"\u4e0b\u4e00\u6b65","text":"<ul> <li>\u67e5\u770b \u63d0\u4f9b\u5546\u914d\u7f6e \u4e86\u89e3\u5404\u63d0\u4f9b\u5546\u8be6\u60c5</li> <li>\u5b66\u4e60 \u9ad8\u7ea7\u914d\u7f6e \u81ea\u5b9a\u4e49\u5ba2\u6237\u7aef\u884c\u4e3a</li> <li>\u63a2\u7d22 API \u53c2\u8003 \u4e86\u89e3\u5b8c\u6574\u529f\u80fd</li> </ul>"},{"location":"user-guide/caching/","title":"\u7f13\u5b58\u914d\u7f6e","text":"<p>LLM API Router \u63d0\u4f9b\u5185\u7f6e\u7684\u7f13\u5b58\u652f\u6301\uff0c\u5e2e\u52a9\u51cf\u5c11\u91cd\u590d\u8bf7\u6c42\u548c\u964d\u4f4e\u6210\u672c\u3002</p>"},{"location":"user-guide/caching/#_2","title":"\u5feb\u901f\u5f00\u59cb","text":"<pre><code>from llm_api_router import Client, ProviderConfig\nfrom llm_api_router.cache import MemoryCache\n\n# \u521b\u5efa\u5185\u5b58\u7f13\u5b58\ncache = MemoryCache(max_size=1000, ttl=3600)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",\n    default_model=\"gpt-3.5-turbo\"\n)\n\nwith Client(config, cache=cache) as client:\n    # \u7b2c\u4e00\u6b21\u8bf7\u6c42\uff1a\u8c03\u7528 API\n    response1 = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n\n    # \u7b2c\u4e8c\u6b21\u76f8\u540c\u8bf7\u6c42\uff1a\u4f7f\u7528\u7f13\u5b58\n    response2 = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n</code></pre>"},{"location":"user-guide/caching/#_3","title":"\u7f13\u5b58\u7c7b\u578b","text":""},{"location":"user-guide/caching/#memorycache","title":"MemoryCache","text":"<p>\u5185\u5b58\u7f13\u5b58\u9002\u7528\u4e8e\u5355\u8fdb\u7a0b\u5e94\u7528\uff1a</p> <pre><code>from llm_api_router.cache import MemoryCache\n\ncache = MemoryCache(\n    max_size=1000,     # \u6700\u5927\u7f13\u5b58\u6761\u76ee\u6570\n    ttl=3600,          # \u751f\u5b58\u65f6\u95f4\uff08\u79d2\uff09\uff0c\u8d85\u65f6\u540e\u81ea\u52a8\u5931\u6548\n)\n</code></pre> <p>\u7279\u70b9\uff1a</p> <ul> <li>\u901f\u5ea6\u5feb\uff0c\u65e0 I/O \u5f00\u9500</li> <li>\u8fdb\u7a0b\u91cd\u542f\u540e\u7f13\u5b58\u4e22\u5931</li> <li>\u9002\u5408\u5f00\u53d1\u548c\u6d4b\u8bd5\u73af\u5883</li> </ul>"},{"location":"user-guide/caching/#diskcache","title":"DiskCache","text":"<p>\u78c1\u76d8\u7f13\u5b58\u9002\u7528\u4e8e\u9700\u8981\u6301\u4e45\u5316\u7684\u573a\u666f\uff1a</p> <pre><code>from llm_api_router.cache import DiskCache\n\ncache = DiskCache(\n    cache_dir=\".llm_cache\",  # \u7f13\u5b58\u76ee\u5f55\n    max_size_mb=500,         # \u6700\u5927\u7f13\u5b58\u5927\u5c0f\uff08MB\uff09\n    ttl=86400,               # \u751f\u5b58\u65f6\u95f4\uff08\u79d2\uff09\n)\n</code></pre> <p>\u7279\u70b9\uff1a</p> <ul> <li>\u7f13\u5b58\u6301\u4e45\u5316\u5230\u78c1\u76d8</li> <li>\u8fdb\u7a0b\u91cd\u542f\u540e\u7f13\u5b58\u4fdd\u7559</li> <li>\u9002\u5408\u751f\u4ea7\u73af\u5883</li> </ul>"},{"location":"user-guide/caching/#_4","title":"\u7f13\u5b58\u952e\u751f\u6210","text":"<p>\u7f13\u5b58\u952e\u57fa\u4e8e\u4ee5\u4e0b\u56e0\u7d20\u81ea\u52a8\u751f\u6210\uff1a</p> <ul> <li>Provider \u7c7b\u578b</li> <li>\u6a21\u578b\u540d\u79f0</li> <li>\u6d88\u606f\u5185\u5bb9</li> <li>Temperature \u7b49\u53c2\u6570</li> </ul> <p>\u76f8\u540c\u7684\u8bf7\u6c42\u53c2\u6570\u4f1a\u751f\u6210\u76f8\u540c\u7684\u7f13\u5b58\u952e\u3002</p>"},{"location":"user-guide/caching/#_5","title":"\u7981\u7528\u7f13\u5b58","text":"<p>\u5bf9\u4e8e\u7279\u5b9a\u8bf7\u6c42\uff0c\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u7981\u7528\u7f13\u5b58\uff1a</p> <pre><code># \u4f7f\u7528 bypass_cache \u53c2\u6570\uff08\u5982\u679c\u652f\u6301\uff09\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    # \u67d0\u4e9b\u5b9e\u73b0\u53ef\u80fd\u652f\u6301\u6b64\u53c2\u6570\n)\n</code></pre>"},{"location":"user-guide/caching/#_6","title":"\u6e05\u9664\u7f13\u5b58","text":"<pre><code># \u6e05\u9664\u6240\u6709\u7f13\u5b58\ncache.clear()\n\n# \u83b7\u53d6\u7f13\u5b58\u7edf\u8ba1\nstats = cache.stats()\nprint(f\"\u547d\u4e2d\u6b21\u6570: {stats['hits']}\")\nprint(f\"\u672a\u547d\u4e2d\u6b21\u6570: {stats['misses']}\")\nprint(f\"\u547d\u4e2d\u7387: {stats['hit_rate']:.2%}\")\n</code></pre>"},{"location":"user-guide/caching/#_7","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u5f00\u53d1\u73af\u5883\uff1a\u4f7f\u7528 <code>MemoryCache</code>\uff0c\u5feb\u901f\u8fed\u4ee3</li> <li>\u751f\u4ea7\u73af\u5883\uff1a\u4f7f\u7528 <code>DiskCache</code>\uff0c\u907f\u514d\u91cd\u590d API \u8c03\u7528</li> <li>\u8bbe\u7f6e\u5408\u7406 TTL\uff1a\u6839\u636e\u6570\u636e\u65f6\u6548\u6027\u8c03\u6574</li> <li>\u76d1\u63a7\u547d\u4e2d\u7387\uff1a\u8fc7\u4f4e\u7684\u547d\u4e2d\u7387\u53ef\u80fd\u9700\u8981\u8c03\u6574\u7b56\u7565</li> </ol>"},{"location":"user-guide/caching/#_8","title":"\u7f13\u5b58\u4e0e\u6d41\u5f0f\u54cd\u5e94","text":"<p>\u26a0\ufe0f \u6d41\u5f0f\u54cd\u5e94\u9ed8\u8ba4\u4e0d\u4f1a\u88ab\u7f13\u5b58\uff0c\u56e0\u4e3a\u6d41\u5f0f\u8f93\u51fa\u662f\u589e\u91cf\u7684\u3002</p> <p>\u5982\u9700\u7f13\u5b58\u6d41\u5f0f\u54cd\u5e94\u7684\u5b8c\u6574\u5185\u5bb9\uff0c\u8bf7\u5728\u5e94\u7528\u5c42\u81ea\u884c\u5b9e\u73b0\u3002</p>"},{"location":"user-guide/configuration/","title":"\u914d\u7f6e\u53c2\u8003","text":"<p>\u672c\u6587\u6863\u8be6\u7ec6\u4ecb\u7ecd LLM API Router \u7684\u6240\u6709\u914d\u7f6e\u9009\u9879\u3002</p>"},{"location":"user-guide/configuration/#providerconfig","title":"ProviderConfig","text":"<p><code>ProviderConfig</code> \u662f\u914d\u7f6e LLM \u63d0\u4f9b\u5546\u7684\u6838\u5fc3\u7c7b\u3002</p> <pre><code>from llm_api_router import ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type: str,           # \u5fc5\u586b\n    api_key: str,                 # \u5fc5\u586b\n    default_model: str = None,    # \u53ef\u9009\n    base_url: str = None,         # \u53ef\u9009\n    timeout: float = 30.0,        # \u53ef\u9009\n    max_retries: int = 3,         # \u53ef\u9009\n)\n</code></pre>"},{"location":"user-guide/configuration/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>provider_type</code> <code>str</code> - \u63d0\u4f9b\u5546\u7c7b\u578b\uff0c\u5982 <code>\"openai\"</code>, <code>\"anthropic\"</code> <code>api_key</code> <code>str</code> - API \u5bc6\u94a5 <code>default_model</code> <code>str</code> <code>None</code> \u9ed8\u8ba4\u4f7f\u7528\u7684\u6a21\u578b <code>base_url</code> <code>str</code> <code>None</code> \u81ea\u5b9a\u4e49 API \u5730\u5740 <code>timeout</code> <code>float</code> <code>30.0</code> \u8bf7\u6c42\u8d85\u65f6\u65f6\u95f4\uff08\u79d2\uff09 <code>max_retries</code> <code>int</code> <code>3</code> \u6700\u5927\u91cd\u8bd5\u6b21\u6570"},{"location":"user-guide/configuration/#client","title":"Client \u914d\u7f6e","text":""},{"location":"user-guide/configuration/#_3","title":"\u540c\u6b65\u5ba2\u6237\u7aef","text":"<pre><code>from llm_api_router import Client, ProviderConfig\n\nconfig = ProviderConfig(...)\n\n# \u4f7f\u7528\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff08\u63a8\u8350\uff09\nwith Client(config) as client:\n    response = client.chat.completions.create(...)\n\n# \u624b\u52a8\u7ba1\u7406\nclient = Client(config)\ntry:\n    response = client.chat.completions.create(...)\nfinally:\n    client.close()\n</code></pre>"},{"location":"user-guide/configuration/#_4","title":"\u5f02\u6b65\u5ba2\u6237\u7aef","text":"<pre><code>from llm_api_router import AsyncClient, ProviderConfig\n\nconfig = ProviderConfig(...)\n\n# \u4f7f\u7528\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff08\u63a8\u8350\uff09\nasync with AsyncClient(config) as client:\n    response = await client.chat.completions.create(...)\n</code></pre>"},{"location":"user-guide/configuration/#_5","title":"\u91cd\u8bd5\u914d\u7f6e","text":"<p>\u5185\u7f6e\u6307\u6570\u9000\u907f\u91cd\u8bd5\u673a\u5236\uff1a</p> <pre><code>from llm_api_router import Client, ProviderConfig, RetryConfig\n\n# \u81ea\u5b9a\u4e49\u91cd\u8bd5\u914d\u7f6e\nretry_config = RetryConfig(\n    max_retries=5,              # \u6700\u5927\u91cd\u8bd5\u6b21\u6570\n    initial_delay=1.0,          # \u521d\u59cb\u5ef6\u8fdf\uff08\u79d2\uff09\n    max_delay=60.0,             # \u6700\u5927\u5ef6\u8fdf\uff08\u79d2\uff09\n    exponential_base=2.0,       # \u6307\u6570\u57fa\u6570\n    jitter=0.1,                 # \u6296\u52a8\u56e0\u5b50\n    retryable_errors=[          # \u53ef\u91cd\u8bd5\u7684\u9519\u8bef\u7c7b\u578b\n        \"rate_limit_error\",\n        \"timeout_error\", \n        \"server_error\",\n    ],\n)\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",\n    default_model=\"gpt-4o\",\n    max_retries=5,\n)\n</code></pre>"},{"location":"user-guide/configuration/#_6","title":"\u7f13\u5b58\u914d\u7f6e","text":"<p>\u4f7f\u7528\u7f13\u5b58\u51cf\u5c11\u91cd\u590d\u8bf7\u6c42\uff1a</p> <pre><code>from llm_api_router import Client, ProviderConfig\nfrom llm_api_router.cache import Cache, MemoryCache, DiskCache\n\n# \u5185\u5b58\u7f13\u5b58\ncache = MemoryCache(\n    max_size=1000,           # \u6700\u5927\u7f13\u5b58\u6761\u76ee\u6570\n    ttl=3600,                # \u751f\u5b58\u65f6\u95f4\uff08\u79d2\uff09\n)\n\n# \u78c1\u76d8\u7f13\u5b58\ncache = DiskCache(\n    cache_dir=\".llm_cache\",  # \u7f13\u5b58\u76ee\u5f55\n    max_size_mb=500,         # \u6700\u5927\u7f13\u5b58\u5927\u5c0f\uff08MB\uff09\n    ttl=86400,               # \u751f\u5b58\u65f6\u95f4\uff08\u79d2\uff09\n)\n\n# \u4f7f\u7528\u7f13\u5b58\nconfig = ProviderConfig(...)\nwith Client(config, cache=cache) as client:\n    # \u76f8\u540c\u8bf7\u6c42\u4f1a\u4f7f\u7528\u7f13\u5b58\n    response = client.chat.completions.create(...)\n</code></pre>"},{"location":"user-guide/configuration/#_7","title":"\u901f\u7387\u9650\u5236\u914d\u7f6e","text":"<p>\u63a7\u5236\u8bf7\u6c42\u901f\u7387\uff1a</p> <pre><code>from llm_api_router import RateLimiter\n\n# \u521b\u5efa\u901f\u7387\u9650\u5236\u5668\nlimiter = RateLimiter(\n    requests_per_minute=60,      # \u6bcf\u5206\u949f\u8bf7\u6c42\u6570\n    tokens_per_minute=100000,    # \u6bcf\u5206\u949f token \u6570\n    max_concurrent=10,           # \u6700\u5927\u5e76\u53d1\u6570\n)\n\n# \u4f7f\u7528\u901f\u7387\u9650\u5236\u5668\nasync with limiter.acquire():\n    response = await client.chat.completions.create(...)\n</code></pre>"},{"location":"user-guide/configuration/#_8","title":"\u8d1f\u8f7d\u5747\u8861\u914d\u7f6e","text":"<p>\u591a\u63d0\u4f9b\u5546\u8d1f\u8f7d\u5747\u8861\uff1a</p> <pre><code>from llm_api_router import LoadBalancer, ProviderConfig\n\nconfigs = [\n    ProviderConfig(provider_type=\"openai\", api_key=\"sk-1\", default_model=\"gpt-4o\"),\n    ProviderConfig(provider_type=\"openai\", api_key=\"sk-2\", default_model=\"gpt-4o\"),\n]\n\n# \u8f6e\u8be2\u7b56\u7565\nlb = LoadBalancer(configs, strategy=\"round_robin\")\n\n# \u52a0\u6743\u7b56\u7565\nlb = LoadBalancer(\n    configs, \n    strategy=\"weighted\",\n    weights=[0.7, 0.3],  # \u7b2c\u4e00\u4e2a\u5904\u740670%\u8bf7\u6c42\n)\n\n# \u6700\u5c11\u8fde\u63a5\u7b56\u7565\nlb = LoadBalancer(configs, strategy=\"least_connections\")\n\n# \u968f\u673a\u7b56\u7565\nlb = LoadBalancer(configs, strategy=\"random\")\n</code></pre>"},{"location":"user-guide/configuration/#_9","title":"\u8d1f\u8f7d\u5747\u8861\u7b56\u7565","text":"\u7b56\u7565 \u8bf4\u660e \u9002\u7528\u573a\u666f <code>round_robin</code> \u8f6e\u8be2\u6bcf\u4e2a\u63d0\u4f9b\u5546 \u5747\u5300\u5206\u914d\u8d1f\u8f7d <code>weighted</code> \u6309\u6743\u91cd\u5206\u914d \u4e3b\u5907\u5207\u6362 <code>least_connections</code> \u9009\u62e9\u6700\u5c11\u8fde\u63a5\u7684 \u52a8\u6001\u8d1f\u8f7d <code>random</code> \u968f\u673a\u9009\u62e9 \u7b80\u5355\u573a\u666f"},{"location":"user-guide/configuration/#prompt","title":"Prompt \u6a21\u677f\u914d\u7f6e","text":"<pre><code>from llm_api_router import PromptTemplate\n\n# \u521b\u5efa\u6a21\u677f\ntemplate = PromptTemplate(\n    name=\"qa_template\",\n    template=\"\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e0a\u4e0b\u6587\u56de\u7b54\u95ee\u9898\uff1a\\n\\n\u4e0a\u4e0b\u6587\uff1a{context}\\n\\n\u95ee\u9898\uff1a{question}\",\n    variables=[\"context\", \"question\"],\n    default_values={\"context\": \"\u65e0\u4e0a\u4e0b\u6587\"},\n)\n\n# \u6e32\u67d3\u6a21\u677f\nprompt = template.render(\n    context=\"Python \u662f\u4e00\u79cd\u7f16\u7a0b\u8bed\u8a00\",\n    question=\"Python \u662f\u4ec0\u4e48\uff1f\"\n)\n</code></pre>"},{"location":"user-guide/configuration/#_10","title":"\u4f1a\u8bdd\u7ba1\u7406\u914d\u7f6e","text":"<pre><code>from llm_api_router import ConversationManager\n\n# \u521b\u5efa\u4f1a\u8bdd\u7ba1\u7406\u5668\nmanager = ConversationManager(\n    max_history=50,           # \u6700\u5927\u5386\u53f2\u6d88\u606f\u6570\n    max_tokens=4000,          # \u6700\u5927 token \u6570\n    system_message=\"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u52a9\u624b\",  # \u7cfb\u7edf\u6d88\u606f\n)\n\n# \u6dfb\u52a0\u6d88\u606f\nmanager.add_user_message(\"\u4f60\u597d\")\nmanager.add_assistant_message(\"\u4f60\u597d\uff01\u6709\u4ec0\u4e48\u6211\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7684\uff1f\")\n\n# \u83b7\u53d6\u5b8c\u6574\u6d88\u606f\u5217\u8868\nmessages = manager.get_messages()\n</code></pre>"},{"location":"user-guide/configuration/#_11","title":"\u65e5\u5fd7\u914d\u7f6e","text":"<pre><code>from llm_api_router import configure_logging\n\n# \u914d\u7f6e\u65e5\u5fd7\nconfigure_logging(\n    level=\"INFO\",              # \u65e5\u5fd7\u7ea7\u522b\n    format=\"json\",             # \u8f93\u51fa\u683c\u5f0f\uff1ajson \u6216 text\n    output=\"stderr\",           # \u8f93\u51fa\u4f4d\u7f6e\n    include_request=True,      # \u662f\u5426\u5305\u542b\u8bf7\u6c42\u8be6\u60c5\n    include_response=True,     # \u662f\u5426\u5305\u542b\u54cd\u5e94\u8be6\u60c5\n)\n</code></pre>"},{"location":"user-guide/configuration/#_12","title":"\u65e5\u5fd7\u7ea7\u522b","text":"<ul> <li><code>DEBUG</code> - \u8be6\u7ec6\u8c03\u8bd5\u4fe1\u606f</li> <li><code>INFO</code> - \u4e00\u822c\u4fe1\u606f</li> <li><code>WARNING</code> - \u8b66\u544a\u4fe1\u606f</li> <li><code>ERROR</code> - \u9519\u8bef\u4fe1\u606f</li> <li><code>CRITICAL</code> - \u4e25\u91cd\u9519\u8bef</li> </ul>"},{"location":"user-guide/configuration/#_13","title":"\u6307\u6807\u914d\u7f6e","text":"<pre><code>from llm_api_router import MetricsCollector\n\n# \u521b\u5efa\u6307\u6807\u6536\u96c6\u5668\nmetrics = MetricsCollector()\n\n# \u83b7\u53d6\u6307\u6807\nstats = metrics.get_stats()\nprint(f\"\u603b\u8bf7\u6c42\u6570: {stats['total_requests']}\")\nprint(f\"\u5e73\u5747\u5ef6\u8fdf: {stats['avg_latency_ms']}ms\")\nprint(f\"\u9519\u8bef\u7387: {stats['error_rate']}%\")\n\n# \u5bfc\u51fa Prometheus \u683c\u5f0f\nprometheus_metrics = metrics.export_prometheus()\n</code></pre>"},{"location":"user-guide/configuration/#_14","title":"\u73af\u5883\u53d8\u91cf","text":"<p>\u652f\u6301\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u914d\u7f6e\uff1a</p> <pre><code># API Keys\nexport OPENAI_API_KEY=\"sk-xxx\"\nexport ANTHROPIC_API_KEY=\"sk-ant-xxx\"\nexport GEMINI_API_KEY=\"AIza-xxx\"\n\n# \u53ef\u9009\u914d\u7f6e\nexport LLM_ROUTER_TIMEOUT=\"60\"\nexport LLM_ROUTER_MAX_RETRIES=\"5\"\nexport LLM_ROUTER_LOG_LEVEL=\"DEBUG\"\n</code></pre> <pre><code>import os\nfrom llm_api_router import ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    timeout=float(os.environ.get(\"LLM_ROUTER_TIMEOUT\", 30)),\n)\n</code></pre>"},{"location":"user-guide/configuration/#_15","title":"\u914d\u7f6e\u6587\u4ef6","text":"<p>\u652f\u6301 YAML \u914d\u7f6e\u6587\u4ef6\uff1a</p> <pre><code># config.yaml\nproviders:\n  openai:\n    provider_type: openai\n    api_key: ${OPENAI_API_KEY}\n    default_model: gpt-4o\n    timeout: 30\n    max_retries: 3\n\n  anthropic:\n    provider_type: anthropic\n    api_key: ${ANTHROPIC_API_KEY}\n    default_model: claude-3-5-sonnet-20241022\n\ncache:\n  type: disk\n  cache_dir: .llm_cache\n  max_size_mb: 500\n  ttl: 86400\n\nlogging:\n  level: INFO\n  format: json\n</code></pre> <p>\u4f7f\u7528 CLI \u9a8c\u8bc1\u914d\u7f6e\uff1a</p> <pre><code>llm-router validate config.yaml\n</code></pre>"},{"location":"user-guide/providers/","title":"\u63d0\u4f9b\u5546\u914d\u7f6e","text":"<p>LLM API Router \u652f\u6301\u591a\u4e2a LLM \u63d0\u4f9b\u5546\uff0c\u672c\u6587\u6863\u8be6\u7ec6\u4ecb\u7ecd\u5404\u63d0\u4f9b\u5546\u7684\u914d\u7f6e\u65b9\u5f0f\u3002</p>"},{"location":"user-guide/providers/#_2","title":"\u652f\u6301\u7684\u63d0\u4f9b\u5546","text":"\u63d0\u4f9b\u5546 Provider Type \u6a21\u578b\u793a\u4f8b \u7279\u70b9 OpenAI <code>openai</code> gpt-4o, gpt-3.5-turbo \u6700\u5e7f\u6cdb\u4f7f\u7528 Anthropic <code>anthropic</code> claude-3-5-sonnet \u957f\u4e0a\u4e0b\u6587\uff0c\u63a8\u7406\u80fd\u529b\u5f3a Google Gemini <code>gemini</code> gemini-1.5-flash \u591a\u6a21\u6001\uff0c\u514d\u8d39\u989d\u5ea6 DeepSeek <code>deepseek</code> deepseek-chat \u9ad8\u6027\u4ef7\u6bd4 \u963f\u91cc\u4e91\u767e\u70bc <code>aliyun</code> qwen-turbo \u56fd\u5185\u90e8\u7f72 \u667a\u8c31 AI <code>zhipu</code> glm-4 \u4e2d\u6587\u4f18\u5316 xAI <code>xai</code> grok-beta \u6700\u65b0\u63a8\u7406 OpenRouter <code>openrouter</code> \u591a\u79cd\u6a21\u578b \u7edf\u4e00\u8bbf\u95ee\u591a\u5bb6\u6a21\u578b Ollama <code>ollama</code> llama3.2 \u672c\u5730\u90e8\u7f72"},{"location":"user-guide/providers/#_3","title":"\u57fa\u7840\u914d\u7f6e","text":"<p>\u6240\u6709\u63d0\u4f9b\u5546\u4f7f\u7528\u7edf\u4e00\u7684 <code>ProviderConfig</code> \u914d\u7f6e\uff1a</p> <pre><code>from llm_api_router import ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",      # \u5fc5\u586b\uff1a\u63d0\u4f9b\u5546\u7c7b\u578b\n    api_key=\"sk-xxx\",            # \u5fc5\u586b\uff1aAPI \u5bc6\u94a5\n    default_model=\"gpt-3.5-turbo\", # \u53ef\u9009\uff1a\u9ed8\u8ba4\u6a21\u578b\n    base_url=None,               # \u53ef\u9009\uff1a\u81ea\u5b9a\u4e49 API \u5730\u5740\n    timeout=30.0,                # \u53ef\u9009\uff1a\u8bf7\u6c42\u8d85\u65f6\u65f6\u95f4\uff08\u79d2\uff09\n    max_retries=3,               # \u53ef\u9009\uff1a\u6700\u5927\u91cd\u8bd5\u6b21\u6570\n)\n</code></pre>"},{"location":"user-guide/providers/#openai","title":"OpenAI","text":"<pre><code>from llm_api_router import Client, ProviderConfig\n\nconfig = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"sk-xxx\",  # \u6216 os.environ[\"OPENAI_API_KEY\"]\n    default_model=\"gpt-4o\"\n)\n\nwith Client(config) as client:\n    response = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n</code></pre>"},{"location":"user-guide/providers/#_4","title":"\u53ef\u7528\u6a21\u578b","text":"<ul> <li><code>gpt-4o</code> - \u6700\u65b0\u591a\u6a21\u6001\u6a21\u578b</li> <li><code>gpt-4o-mini</code> - \u8f7b\u91cf\u7248</li> <li><code>gpt-4-turbo</code> - \u9ad8\u6027\u80fd</li> <li><code>gpt-3.5-turbo</code> - \u7ecf\u6d4e\u5b9e\u60e0</li> </ul>"},{"location":"user-guide/providers/#azure-openai","title":"Azure OpenAI","text":"<p>\u4f7f\u7528 Azure \u90e8\u7f72\u7684 OpenAI \u6a21\u578b\uff1a</p> <pre><code>config = ProviderConfig(\n    provider_type=\"openai\",\n    api_key=\"your-azure-key\",\n    base_url=\"https://your-resource.openai.azure.com/openai/deployments/your-deployment\",\n    default_model=\"gpt-4\"\n)\n</code></pre>"},{"location":"user-guide/providers/#anthropic","title":"Anthropic","text":"<pre><code>config = ProviderConfig(\n    provider_type=\"anthropic\",\n    api_key=\"sk-ant-xxx\",\n    default_model=\"claude-3-5-sonnet-20241022\"\n)\n</code></pre>"},{"location":"user-guide/providers/#_5","title":"\u53ef\u7528\u6a21\u578b","text":"<ul> <li><code>claude-3-5-sonnet-20241022</code> - \u6700\u65b0 Sonnet</li> <li><code>claude-3-opus-20240229</code> - \u6700\u5f3a\u5927</li> <li><code>claude-3-haiku-20240307</code> - \u6700\u5feb\u901f</li> </ul>"},{"location":"user-guide/providers/#google-gemini","title":"Google Gemini","text":"<pre><code>config = ProviderConfig(\n    provider_type=\"gemini\",\n    api_key=\"AIza-xxx\",\n    default_model=\"gemini-1.5-flash\"\n)\n</code></pre>"},{"location":"user-guide/providers/#_6","title":"\u53ef\u7528\u6a21\u578b","text":"<ul> <li><code>gemini-1.5-pro</code> - \u9ad8\u6027\u80fd</li> <li><code>gemini-1.5-flash</code> - \u5feb\u901f\u54cd\u5e94</li> <li><code>gemini-1.5-flash-8b</code> - \u8f7b\u91cf\u7248</li> </ul>"},{"location":"user-guide/providers/#deepseek","title":"DeepSeek","text":"<pre><code>config = ProviderConfig(\n    provider_type=\"deepseek\",\n    api_key=\"sk-xxx\",\n    default_model=\"deepseek-chat\"\n)\n</code></pre>"},{"location":"user-guide/providers/#_7","title":"\u53ef\u7528\u6a21\u578b","text":"<ul> <li><code>deepseek-chat</code> - \u5bf9\u8bdd\u6a21\u578b</li> <li><code>deepseek-coder</code> - \u4ee3\u7801\u6a21\u578b</li> </ul>"},{"location":"user-guide/providers/#_8","title":"\u963f\u91cc\u4e91\u767e\u70bc","text":"<pre><code>config = ProviderConfig(\n    provider_type=\"aliyun\",\n    api_key=\"sk-xxx\",\n    default_model=\"qwen-turbo\"\n)\n</code></pre>"},{"location":"user-guide/providers/#_9","title":"\u53ef\u7528\u6a21\u578b","text":"<ul> <li><code>qwen-turbo</code> - \u5feb\u901f\u7248</li> <li><code>qwen-plus</code> - \u589e\u5f3a\u7248</li> <li><code>qwen-max</code> - \u6700\u5f3a\u7248</li> </ul>"},{"location":"user-guide/providers/#ai","title":"\u667a\u8c31 AI","text":"<pre><code>config = ProviderConfig(\n    provider_type=\"zhipu\",\n    api_key=\"xxx.xxx\",\n    default_model=\"glm-4\"\n)\n</code></pre>"},{"location":"user-guide/providers/#_10","title":"\u53ef\u7528\u6a21\u578b","text":"<ul> <li><code>glm-4</code> - \u6700\u65b0\u7248</li> <li><code>glm-4-flash</code> - \u5feb\u901f\u7248</li> <li><code>glm-3-turbo</code> - \u7ecf\u6d4e\u7248</li> </ul>"},{"location":"user-guide/providers/#xai-grok","title":"xAI (Grok)","text":"<pre><code>config = ProviderConfig(\n    provider_type=\"xai\",\n    api_key=\"xai-xxx\",\n    default_model=\"grok-beta\"\n)\n</code></pre>"},{"location":"user-guide/providers/#openrouter","title":"OpenRouter","text":"<p>OpenRouter \u63d0\u4f9b\u7edf\u4e00\u8bbf\u95ee\u591a\u5bb6\u6a21\u578b\u7684\u80fd\u529b\uff1a</p> <pre><code>config = ProviderConfig(\n    provider_type=\"openrouter\",\n    api_key=\"sk-or-xxx\",\n    default_model=\"openai/gpt-4o\"\n)\n</code></pre>"},{"location":"user-guide/providers/#_11","title":"\u6a21\u578b\u683c\u5f0f","text":"<p>OpenRouter \u6a21\u578b\u4f7f\u7528 <code>provider/model</code> \u683c\u5f0f\uff1a</p> <ul> <li><code>openai/gpt-4o</code></li> <li><code>anthropic/claude-3-sonnet</code></li> <li><code>meta-llama/llama-3-70b-instruct</code></li> </ul>"},{"location":"user-guide/providers/#ollama","title":"Ollama (\u672c\u5730\u90e8\u7f72)","text":"<p>Ollama \u5141\u8bb8\u5728\u672c\u5730\u8fd0\u884c\u5f00\u6e90\u6a21\u578b\uff1a</p> <pre><code>config = ProviderConfig(\n    provider_type=\"ollama\",\n    api_key=\"not-required\",  # Ollama \u4e0d\u9700\u8981 API Key\n    base_url=\"http://localhost:11434\",\n    default_model=\"llama3.2\"\n)\n</code></pre>"},{"location":"user-guide/providers/#ollama_1","title":"\u542f\u52a8 Ollama","text":"<pre><code># \u5b89\u88c5 Ollama\uff08macOS\uff09\nbrew install ollama\n\n# \u542f\u52a8\u670d\u52a1\nollama serve\n\n# \u62c9\u53d6\u6a21\u578b\nollama pull llama3.2\nollama pull codellama\nollama pull mistral\n</code></pre>"},{"location":"user-guide/providers/#_12","title":"\u591a\u63d0\u4f9b\u5546\u4f7f\u7528","text":"<p>\u53ef\u4ee5\u4f7f\u7528 <code>create_client</code> \u5de5\u5382\u51fd\u6570\u5feb\u901f\u521b\u5efa\u5ba2\u6237\u7aef\uff1a</p> <pre><code>from llm_api_router import create_client, ProviderConfig\n\n# \u5b9a\u4e49\u591a\u4e2a\u63d0\u4f9b\u5546\u914d\u7f6e\nconfigs = {\n    \"openai\": ProviderConfig(\n        provider_type=\"openai\",\n        api_key=\"sk-xxx\",\n        default_model=\"gpt-4o\"\n    ),\n    \"anthropic\": ProviderConfig(\n        provider_type=\"anthropic\", \n        api_key=\"sk-ant-xxx\",\n        default_model=\"claude-3-5-sonnet-20241022\"\n    ),\n    \"local\": ProviderConfig(\n        provider_type=\"ollama\",\n        api_key=\"\",\n        base_url=\"http://localhost:11434\",\n        default_model=\"llama3.2\"\n    ),\n}\n\n# \u6839\u636e\u9700\u6c42\u9009\u62e9\u63d0\u4f9b\u5546\nprovider = \"openai\"  # \u53ef\u52a8\u6001\u5207\u6362\nwith create_client(configs[provider]) as client:\n    response = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n</code></pre>"},{"location":"user-guide/providers/#_13","title":"\u8d1f\u8f7d\u5747\u8861","text":"<p>\u4f7f\u7528 <code>LoadBalancer</code> \u5728\u591a\u4e2a\u63d0\u4f9b\u5546\u95f4\u5206\u914d\u8bf7\u6c42\uff1a</p> <pre><code>from llm_api_router import LoadBalancer, ProviderConfig\n\n# \u914d\u7f6e\u591a\u4e2a\u63d0\u4f9b\u5546\nconfigs = [\n    ProviderConfig(provider_type=\"openai\", api_key=\"sk-1\", default_model=\"gpt-4o\"),\n    ProviderConfig(provider_type=\"anthropic\", api_key=\"sk-ant-1\", default_model=\"claude-3-5-sonnet-20241022\"),\n]\n\n# \u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5668\nlb = LoadBalancer(configs, strategy=\"round_robin\")\n\n# \u53d1\u9001\u8bf7\u6c42\uff08\u81ea\u52a8\u9009\u62e9\u63d0\u4f9b\u5546\uff09\nresponse = lb.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <p>\u652f\u6301\u7684\u7b56\u7565\uff1a</p> <ul> <li><code>round_robin</code> - \u8f6e\u8be2</li> <li><code>random</code> - \u968f\u673a</li> <li><code>weighted</code> - \u52a0\u6743</li> <li><code>least_connections</code> - \u6700\u5c11\u8fde\u63a5</li> </ul> <p>\u53c2\u89c1 \u8d1f\u8f7d\u5747\u8861\u914d\u7f6e \u4e86\u89e3\u66f4\u591a\u3002</p>"}]}