# Phase 1 Features Documentation

This document describes the features implemented in Phase 1 of the LLM API Router project.

## 1. Testing Framework

We've established a comprehensive testing framework with:

- **68 unit and integration tests**
- **100% coverage** on core infrastructure modules
- pytest with asyncio, mock, and coverage support
- Organized test structure:
  - `tests/unit/` - Unit tests for individual components
  - `tests/integration/` - Integration tests for complete flows
  - `tests/fixtures/` - Shared test fixtures and mock data

### Running Tests

```bash
# Run all tests
pytest tests/

# Run with coverage report
pytest tests/ --cov=src/llm_api_router --cov-report=term-missing

# Run specific test file
pytest tests/unit/test_config.py -v
```

## 2. CI/CD Pipeline

A complete CI/CD pipeline has been configured using GitHub Actions:

### Features
- **Multi-version testing**: Tests run on Python 3.10, 3.11, and 3.12
- **Code quality checks**:
  - `ruff` for linting
  - `black` for code formatting
  - `mypy` for type checking
- **Security scanning**: `safety` checks for known vulnerabilities
- **Coverage reporting**: Automatic upload to Codecov
- **Automated releases**: Existing release workflow for PyPI publishing

### Workflows
- `.github/workflows/ci.yml` - Main CI pipeline
- `.github/workflows/release.yml` - Release automation

## 3. Configuration Management

Enhanced configuration management with multiple loading methods:

### Loading from Environment Variables

```python
from llm_api_router import ProviderConfig

# Default naming (uses OPENAI_API_KEY)
config = ProviderConfig.from_env(
    provider_type="openai",
    default_model="gpt-3.5-turbo"
)

# Custom environment variable names
config = ProviderConfig.from_env(
    provider_type="openai",
    api_key_env="MY_API_KEY",
    base_url_env="MY_BASE_URL"
)
```

### Loading from Dictionary

```python
config = ProviderConfig.from_dict({
    "provider_type": "openai",
    "api_key": "sk-test-key",
    "default_model": "gpt-4"
})
```

### Loading from Files (JSON/YAML)

```python
# From JSON file
config = ProviderConfig.from_file("config.json")

# From YAML file
config = ProviderConfig.from_file("config.yaml")
```

**Example JSON config:**
```json
{
  "provider_type": "openai",
  "api_key": "sk-test-key",
  "default_model": "gpt-4",
  "base_url": "https://api.openai.com/v1",
  "extra_headers": {
    "X-Organization": "my-org"
  }
}
```

**Example YAML config:**
```yaml
provider_type: anthropic
api_key: sk-ant-test-key
default_model: claude-3-5-sonnet-20240620
api_version: "2023-06-01"
```

### Merging Configurations

```python
base_config = ProviderConfig(
    provider_type="openai",
    api_key="sk-key",
    default_model="gpt-3.5-turbo"
)

# Override specific fields
new_config = base_config.merge({
    "default_model": "gpt-4",
    "base_url": "https://custom.api.com"
})
```

### Configuration Validation

```python
config = ProviderConfig(
    provider_type="openai",
    api_key="sk-test-key"
)

# Validate configuration
config.validate()  # Returns True or raises ValueError
```

## 4. Error Handling and Retry Logic

### Enhanced Exception Hierarchy

New exception types for better error handling:

- `TimeoutError` - Request timeout
- `InvalidRequestError` - Invalid request (HTTP 400)
- `NotFoundError` - Resource not found (HTTP 404)
- `PermissionError` - Permission denied (HTTP 403)
- `MaxRetriesExceededError` - Exceeded retry limit

### Retry Configuration

```python
from llm_api_router import RetryConfig

# Default retry config
retry_config = RetryConfig(
    max_retries=3,
    initial_delay=1.0,
    max_delay=60.0,
    exponential_base=2.0,
    jitter=True,
    retry_on_status_codes=(429, 500, 502, 503, 504)
)
```

### Retry Decorators

```python
from llm_api_router.retry import with_retry, with_retry_async

# For synchronous functions
@with_retry(retry_config)
def make_api_call():
    # Your API call here
    pass

# For async functions
@with_retry_async(retry_config)
async def make_api_call_async():
    # Your async API call here
    pass
```

### Exponential Backoff

The retry logic implements exponential backoff with optional jitter:
- First retry: 1 second
- Second retry: 2 seconds
- Third retry: 4 seconds
- ...up to max_delay

With jitter enabled, delays are randomized by Â±50% to prevent thundering herd.

## 5. Examples

Several examples are provided in the `examples/` directory:

- `config_management.py` - Configuration loading and validation
- `config_from_files.py` - Loading configs from JSON/YAML files
- `ollama_example.py` - Using with Ollama (local models)

## Testing Coverage

Current test coverage:

| Module | Coverage |
|--------|----------|
| `__init__.py` | 100% |
| `client.py` | 98% |
| `exceptions.py` | 100% |
| `types.py` | 100% |
| `retry.py` | 66% |
| `factory.py` | 83% |

Overall: Strong coverage on core infrastructure, with providers having baseline coverage.

## Next Steps (Phase 2+)

Future enhancements include:
- New provider support (Azure OpenAI, AWS Bedrock)
- Embeddings API
- Function calling support
- Performance monitoring
- Circuit breaker pattern
- Advanced caching
