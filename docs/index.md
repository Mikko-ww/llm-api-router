# LLM API Router

<p align="center">
  <strong>ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹ API è·¯ç”±åº“</strong>
</p>

<p align="center">
  <a href="https://github.com/Mikko-ww/llm-api-router/actions"><img src="https://github.com/Mikko-ww/llm-api-router/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
  <a href="https://pypi.org/project/llm-api-router/"><img src="https://img.shields.io/pypi/v/llm-api-router" alt="PyPI"></a>
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.10%2B-blue" alt="Python"></a>
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License"></a>
</p>

---

## æ¦‚è¿°

`llm-api-router` æ˜¯ä¸€ä¸ª Python åº“ï¼Œä¸ºå„ç§å¤§è¯­è¨€æ¨¡å‹æä¾›å•†ï¼ˆå¦‚ OpenAIã€Anthropicã€Google Gemini ç­‰ï¼‰æä¾›ç»Ÿä¸€ã€ä¸€è‡´ä¸”ç±»å‹å®‰å…¨çš„æ¥å£ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”„ **ç»Ÿä¸€æ¥å£** - ç±»ä¼¼ OpenAI SDK çš„ `client.chat.completions.create` é£æ ¼
- ğŸŒ **å¤šå‚å•†æ”¯æŒ** - OpenAIã€Anthropicã€Geminiã€DeepSeekã€æ™ºè°±ã€é˜¿é‡Œäº‘ç­‰
- âš¡ **é›¶ä»£ç åˆ‡æ¢** - ä»…éœ€ä¿®æ”¹é…ç½®å³å¯åˆ‡æ¢åº•å±‚æ¨¡å‹æä¾›å•†
- ğŸŒŠ **æµå¼æ”¯æŒ** - ç»Ÿä¸€çš„ SSE æµå¼å“åº”å¤„ç†
- ğŸ”§ **å¼‚æ­¥æ”¯æŒ** - åŸç”Ÿæ”¯æŒ `asyncio` å’Œ `await`
- ğŸ“Š **å¯è§‚æµ‹æ€§** - å†…ç½®æ—¥å¿—ã€æŒ‡æ ‡æ”¶é›†ã€ç¼“å­˜ç­‰åŠŸèƒ½

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install llm-api-router
```

### åŸºç¡€ç”¨æ³•

```python
from llm_api_router import Client, ProviderConfig

config = ProviderConfig(
    provider_type="openai",
    api_key="sk-...",
    default_model="gpt-3.5-turbo"
)

with Client(config) as client:
    response = client.chat.completions.create(
        messages=[{"role": "user", "content": "Hello!"}]
    )
    print(response.choices[0].message.content)
```

### åˆ‡æ¢æä¾›å•†

åªéœ€æ›´æ”¹é…ç½®ï¼Œä»£ç æ— éœ€ä¿®æ”¹ï¼š

```python
# ä½¿ç”¨ Anthropic
config = ProviderConfig(
    provider_type="anthropic",
    api_key="sk-ant-...",
    default_model="claude-3-haiku-20240307"
)

# ä½¿ç”¨æœ¬åœ° Ollama
config = ProviderConfig(
    provider_type="ollama",
    api_key="not-required",
    base_url="http://localhost:11434",
    default_model="llama3.2"
)
```

## æ”¯æŒçš„æä¾›å•†

| æä¾›å•† | Chat | Embeddings | Function Calling |
|--------|:----:|:----------:|:----------------:|
| OpenAI | âœ… | âœ… | âœ… |
| Anthropic | âœ… | - | âœ… |
| Google Gemini | âœ… | âœ… | - |
| DeepSeek | âœ… | - | - |
| æ™ºè°± AI | âœ… | âœ… | - |
| é˜¿é‡Œäº‘ | âœ… | âœ… | - |
| Ollama | âœ… | - | - |
| OpenRouter | âœ… | - | - |
| xAI | âœ… | - | - |

## é«˜çº§ç‰¹æ€§

- [å“åº”ç¼“å­˜](user-guide/caching.md) - å‡å°‘é‡å¤ API è°ƒç”¨
- [é…ç½®ç®¡ç†](user-guide/configuration.md) - è¯¦ç»†é…ç½®å‚è€ƒ
- [æä¾›å•†æ”¯æŒ](user-guide/providers.md) - å„æä¾›å•†è¯¦æƒ…

## ä¸‹ä¸€æ­¥

- ğŸ“– æŸ¥çœ‹ [å®‰è£…æŒ‡å—](getting-started/installation.md) äº†è§£è¯¦ç»†å®‰è£…æ­¥éª¤
- ğŸš€ é˜…è¯» [å¿«é€Ÿå¼€å§‹](getting-started/quickstart.md) å¼€å§‹ä½¿ç”¨
- ğŸ“š æµè§ˆ [API å‚è€ƒ](api-reference/client.md) äº†è§£å®Œæ•´ API
